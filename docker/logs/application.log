2025-07-10 02:16:48,592 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/generator.json
2025-07-10 02:16:48,594 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/embedder.json
2025-07-10 02:16:48,594 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/repo.json
2025-07-10 02:16:48,595 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/lang.json
2025-07-10 02:16:49,739 - INFO - faiss.loader - loader.py:125 - Loading faiss with AVX2 support.
2025-07-10 02:16:49,805 - INFO - faiss.loader - loader.py:127 - Successfully loaded faiss with AVX2 support.
2025-07-10 02:16:49,826 - INFO - faiss - __init__.py:174 - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-07-10 02:16:49,887 - INFO - __main__ - main.py:42 - Starting Streaming API on port 8001
2025-07-10 02:16:53,533 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:16:53,540 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:16:53,746 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:16:53,750 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:16:58,815 - INFO - api.api - api.py:179 - Fetching model configurations
2025-07-10 02:16:58,817 - INFO - api.api - api.py:476 - Attempting to retrieve wiki cache for zhlhaohao/json_repair (github), lang: zh
2025-07-10 02:17:06,534 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:17:06,536 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:17:06,573 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:17:06,581 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:17:09,972 - INFO - api.api - api.py:179 - Fetching model configurations
2025-07-10 02:17:09,990 - INFO - api.api - api.py:476 - Attempting to retrieve wiki cache for AsyncFuncAI/deepwiki-open (github), lang: zh
2025-07-10 02:17:42,939 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 25 tokens
2025-07-10 02:17:42,972 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:17:42,974 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:17:42,974 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:17:42,974 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:17:43,018 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:17:43,025 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['system_prompt', 'conversation_history', 'output_format_str', 'input_str', 'contexts']
2025-07-10 02:17:43,025 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 02:17:43,025 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/AsyncFuncAI/deepwiki-open...
2025-07-10 02:17:43,025 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: AsyncFuncAI_deepwiki-open
2025-07-10 02:17:43,026 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/AsyncFuncAI_deepwiki-open. Using existing repository.
2025-07-10 02:17:43,026 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/AsyncFuncAI_deepwiki-open', 'save_db_file': '/root/.adalflow/databases/AsyncFuncAI_deepwiki-open.pkl'}
2025-07-10 02:17:43,026 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 02:17:43,057 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 02:17:43,057 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:17:43,057 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:17:43,057 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:17:43,122 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:17:43,123 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:17:43,123 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:17:43,186 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 821 documents from existing database
2025-07-10 02:17:43,187 - INFO - api.rag - rag.py:421 - Loaded 821 documents for retrieval
2025-07-10 02:17:43,187 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 821 documents)
2025-07-10 02:17:43,188 - INFO - api.rag - rag.py:384 - Embedding validation complete: 821/821 documents have valid embeddings
2025-07-10 02:17:43,188 - INFO - api.rag - rag.py:429 - Using 821 documents with valid embeddings for retrieval
2025-07-10 02:17:43,189 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:17:43,199 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 821 chunks
2025-07-10 02:17:43,199 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 02:17:43,199 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/AsyncFuncAI/deepwiki-open
2025-07-10 02:17:43,199 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['将Ask模态框里面的内容窗口扩展到父窗口的90%']}
2025-07-10 02:17:43,938 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 02:17:43,941 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 02:17:43,941 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:17:43,941 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:17:43,941 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/AsyncFuncAI/deepwiki-open (deepwiki-open).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: src/components/Mermaid.tsx

 children: React.ReactNode;
}> = ({ isOpen, onClose, children }) => {
  const modalRef = useRef<HTMLDivElement>(null);
  const [zoom, setZoom] = useState(1);

  // Close on Escape key
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === 'Escape') {
        onClose();
      }
    };

    if (isOpen) {
      document.addEventListener('keydown', handleKeyDown);
    }

    return () => {
      document.removeEventListener('keydown', handleKeyDown);
    };
  }, [isOpen, onClose]);

  // Handle click outside to close
  useEffect(() => {
    const handleOutsideClick = (e: MouseEvent) => {
      if (modalRef.current && !modalRef.current.contains(e.target as Node)) {
        onClose();
      }
    };

    if (isOpen) {
      document.addEventListener('mousedown', handleOutsideClick);
    }

    return () => {
      document.removeEventListener('mousedown', handleOutsideClick);
    };
  }, [isOpen, onClose]);

  // Reset zoom when modal opens
  useEffect(() => {
    if (isOpen) {
      setZoom(1);
    }
  }, [isOpen]);

  if (!isOpen) return null;

  return (
    <div className="fixed inset-0 z-50 flex items-center justify-center bg-black bg-opacity-75 p-4">
      <div
        ref={modalRef}
        className="bg-[var(--card-bg)] rounded-lg shadow-custom max-w-5xl max-h-[90vh] w-full overflow-hidden flex flex-col card-japanese"
      >
        {/* Modal header with controls */}
        <div className="flex items-center justify-between p-4 border-b border-[var(--border-color)]">
          <div className="font-medium text-[var(--foreground)] font-serif">図表表示</div>
          <div className="flex items-center gap-4">
           

     <div
        ref={modalRef}
        className="bg-[var(--card-bg)] rounded-lg shadow-custom max-w-5xl max-h-[90vh] w-full overflow-hidden flex flex-col card-japanese"
      >
        {/* Modal header with controls */}
        <div className="flex items-center justify-between p-4 border-b border-[var(--border-color)]">
          <div className="font-medium text-[var(--foreground)] font-serif">図表表示</div>
          <div className="flex items-center gap-4">
            <div className="flex items-center gap-2">
              <button
                onClick={() => setZoom(Math.max(0.5, zoom - 0.1))}
                className="text-[var(--foreground)] hover:bg-[var(--accent-primary)]/10 p-2 rounded-md border border-[var(--border-color)] transition-colors"
                aria-label="Zoom out"
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <circle cx="11" cy="11" r="8"></circle>
                  <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                  <line x1="8" y1="11" x2="14" y2="11"></line>
                </svg>
              </button>
              <span className="text-sm text-[var(--muted)]">{Math.round(zoom * 100)}%</span>
              <button
   

         </button>
            </div>
            <button
              onClick={onClose}
              className="text-[var(--foreground)] hover:bg-[var(--accent-primary)]/10 p-2 rounded-md border border-[var(--border-color)] transition-colors"
              aria-label="Close"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                <line x1="18" y1="6" x2="6" y2="18"></line>
                <line x1="6" y1="6" x2="18" y2="18"></line>
              </svg>
            </button>
          </div>
        </div>

        {/* Modal content with zoom */}
        <div className="overflow-auto p-6 flex-1 flex items-center justify-center bg-[var(--background)]/50">
          <div
            style={{
              transform: `scale(${zoom})`,
              transformOrigin: 'center center',
              transition: 'transform 0.3s ease-out'
            }}
          >
            {children}
          </div>
     

     <div className="w-2 h-2 bg-[var(--accent-primary)]/70 rounded-full animate-pulse delay-150"></div>
          <span className="text-[var(--muted)] text-xs ml-2 font-serif">図表を描画中...</span>
        </div>
      </div>
    );
  }

  return (
    <>
      <div
        ref={containerRef}
        className={`w-full max-w-full ${zoomingEnabled ? "h-[600px] p-4" : ""}`}
      >
        <div
          className={`relative group ${zoomingEnabled ? "h-full rounded-lg border-2 border-black" : ""}`}
        >
          <div
            className={`flex justify-center overflow-auto text-center my-2 cursor-pointer hover:shadow-md transition-shadow duration-200 rounded-md ${className} ${zoomingEnabled ? "h-full" : ""}`}
            dangerouslySetInnerHTML={{ __html: svg }}
            onClick={zoomingEnabled ? undefined : handleDiagramClick}
            title={zoomingEnabled ? undefined : "Click to view fullscreen"}
          />

          {!zoomingEnabled && (
            <div className="absolute top-2 right-2 bg-gray-700/70 dark:bg-gray-900/70 text-white p-1.5 rounded-md opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex items-center gap-1.5 text-xs shadow-md pointer-events-none">
              <svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                <circle cx="11" cy="11" r="8"></circle>
                <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                <line x1="11" y1="8" x2="11" y2="14"></line>
    

## File Path: src/app/[owner]/[repo]/page.tsx

${isAskModalOpen ? 'opacity-100' : 'opacity-0 pointer-events-none'}`}>
        <div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[80vh] flex flex-col">
          <div className="flex items-center justify-end p-3 absolute top-0 right-0 z-10">
            <button
              onClick={() => {
                // Just close the modal without clearing the conversation
                setIsAskModalOpen(false);
              }}
              className="text-[var(--muted)] hover:text-[var(--foreground)] transition-colors bg-[var(--card-bg)]/80 rounded-full p-2"
              aria-label="Close"
            >
              <FaTimes className="text-xl" />
            </button>
          </div>
          <div className="flex-1 overflow-y-auto p-4">
            <Ask
              repoInfo={effectiveRepoInfo}
              provider={selectedProviderState}
              model={selectedModelState}
              isCustomModel={isCustomSelectedModelState}
              customModel={customSelectedModelState}
              language={language}
              onRef={(ref) => (askComponentRef.current = ref)}
            />
       

 </main>

      <footer className="max-w-[90%] xl:max-w-[1400px] mx-auto mt-8 flex flex-col gap-4 w-full">
        <div className="flex justify-between items-center gap-4 text-center text-[var(--muted)] text-sm h-fit w-full bg-[var(--card-bg)] rounded-lg p-3 shadow-sm border border-[var(--border-color)]">
          <p className="flex-1 font-serif">
            {messages.footer?.copyright || 'DeepWiki - Generate Wiki from GitHub/Gitlab/Bitbucket repositories'}
          </p>
          <ThemeToggle />
        </div>
      </footer>

      {/* Floating Chat Button */}
      {!isLoading && wikiStructure && (
        <button
          onClick={() => setIsAskModalOpen(true)}
          className="fixed bottom-6 right-6 w-14 h-14 rounded-full bg-[var(--accent-primary)] text-white shadow-lg flex items-center justify-center hover:bg-[var(--accent-primary)]/90 transition-all z-50"
          aria-label={messages.ask?.title || 'Ask about this repository'}
        >
          <FaComments className="text-xl" />
        </button>
      )}

      {/* Ask Modal - Always render but conditionally show/hide */}
      <div className={`fixed inset-0 bg-black/50 flex items-center justify-center z-50 p-4 transition-opacity duration-300 ${isAskModalOpen ? 'opacity-100' : 'opacity-0 pointer-events-none'}`}>
        <div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[80vh] flex flex-col">
          <div className="flex items-center justify-end p-3 absolute top-0 right-0 z-10">
            <button
              onClick={() => {
                // Just close the modal without clearing the conversation
         

     return filePath;
    }

    try {
      const url = new URL(repoUrl);
      const hostname = url.hostname;
      
      if (hostname === 'github.com' || hostname.includes('github')) {
        // GitHub URL format: https://github.com/owner/repo/blob/branch/path
        return `${repoUrl}/blob/${defaultBranch}/${filePath}`;
      } else if (hostname === 'gitlab.com' || hostname.includes('gitlab')) {
        // GitLab URL format: https://gitlab.com/owner/repo/-/blob/branch/path
        return `${repoUrl}/-/blob/${defaultBranch}/${filePath}`;
      } else if (hostname === 'bitbucket.org' || hostname.includes('bitbucket')) {
        // Bitbucket URL format: https://bitbucket.org/owner/repo/src/branch/path
        return `${repoUrl}/src/${defaultBranch}/${filePath}`;
      }
    } catch (error) {
      console.warn('Error generating file URL:', error);
    }

    // Fallback to just the file path
    return filePath;
  }, [effectiveRepoInfo, defaultBranch]);

  // Memoize repo info to avoid triggering updates in callbacks

  // Add useEffect to handle scroll reset
  useEffect(() => {
    // Scroll to top when currentPageId changes
    const wikiContent = document.getElementById('wiki-content');
    if (wikiContent) {
      wikiContent.scrollTo({ top: 0, behavior: 'smooth' });
    }
  }, [currentPageId]);

  // close the modal when escape is pressed
  useEffect(() => {
    const handleEsc = (event: KeyboardEvent) => {
      if (event.key === 'Escape') {
        setIsAskModalOpen(false);
      }
    };

    if (isAskModalOpen) {
      window.addEventListener('keydown', handleEsc);
    }

    // Cleanup on unmount or when modal closes
    return () => {
      window.removeEventListener('keydown', handleEsc);
    

}
  }, [currentPageId]);

  // close the modal when escape is pressed
  useEffect(() => {
    const handleEsc = (event: KeyboardEvent) => {
      if (event.key === 'Escape') {
        setIsAskModalOpen(false);
      }
    };

    if (isAskModalOpen) {
      window.addEventListener('keydown', handleEsc);
    }

    // Cleanup on unmount or when modal closes
    return () => {
      window.removeEventListener('keydown', handleEsc);
    };
  }, [isAskModalOpen]);

  // Fetch authentication status on component mount
  useEffect(() => {
    const fetchAuthStatus = async () => {
      try {
        setIsAuthLoading(true);
        const response = await fetch('/api/auth/status');
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        const data = await response.json();
        setAuthRequired(data.auth_required);
      } catch (err) {
        console.error("Failed to fetch auth status:", err);
        // Assuming auth is required if fetch fails to avoid blocking UI for safety
        setAuthRequired(true);
      } finally {
        setIsAuthLoading(false);
      }
    };

    fetchAuthStatus();
  }, []);

  // Generate content for a wiki page
  const generatePageContent = useCallback(async (page: WikiPage, owner: string, repo: string) => {
    return new Promise<void>(async (resolve) => {
      try {
        // Skip if content already exists
        if (generatedPages[page.id]?.content) {
 

## File Path: src/components/Ask.tsx

    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13 5l7 7-7 7M5 5l7 7-7 7" />
                  </svg>
                  <span>{messages.ask?.askButton || 'Ask'}</span>
                </>
              )}
            </button>
          </div>

          {/* Deep Research toggle */}
          <div className="flex items-center mt-2 justify-between">
            <div className="group relative">
              <label className="flex items-center cursor-pointer">
                <span className="text-xs text-gray-600 dark:text-gray-400 mr-2">Deep Research</span>
                <div className="relative">
                  <input
                    type="checkbox"
                    checked={deepResearch}
                    onChange={() => setDeepResearch(!deepResearch)}
                    className="sr-only"
                  />
                  <div className={`w-10 h-5 rounded-full transition-colors ${deepResearch ? 'bg-purple-600' : 'bg-gray-300 dark:bg-gray-600'}`}></div>
  

    type="submit"
              disabled={isLoading || !question.trim()}
              className={`absolute right-3 top-1/2 transform -translate-y-1/2 px-4 py-2 rounded-md font-medium text-sm ${
                isLoading || !question.trim()
                  ? 'bg-[var(--button-disabled-bg)] text-[var(--button-disabled-text)] cursor-not-allowed'
                  : 'bg-[var(--accent-primary)] text-white hover:bg-[var(--accent-primary)]/90 shadow-sm'
              } transition-all duration-200 flex items-center gap-1.5`}
            >
              {isLoading ? (
                <div className="w-4 h-4 rounded-full border-2 border-t-transparent border-white animate-spin" />
              ) : (
                <>
                  <svg className="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13 5l7 7-7 7M5 5l7 7-7 7" />
                  </svg>
                  <span>{messages.ask?.askButton || 'Ask'}</span>
                </>
              )}
            </button>
     

const [buttonWidth, setButtonWidth] = useState(0);
  const buttonRef = useRef<HTMLButtonElement>(null);

  // Measure button width and update state
  useEffect(() => {
    if (buttonRef.current) {
      const width = buttonRef.current.offsetWidth;
      setButtonWidth(width);
    }
  }, [messages.ask?.askButton, isLoading]);

  return (
    <div>
      <div className="p-4">
        <div className="flex items-center justify-end mb-4">
          {/* Model selection button */}
          <button
            type="button"
            onClick={() => setIsModelSelectionModalOpen(true)}
            className="text-xs px-2.5 py-1 rounded border border-[var(--border-color)]/40 bg-[var(--background)]/10 text-[var(--foreground)]/80 hover:bg-[var(--background)]/30 hover:text-[var(--foreground)] transition-colors flex items-center gap-1.5"
          >
            <span>{selectedProvider}/{isCustomSelectedModel ? customSelectedModel : selectedModel}</span>
            <svg className="h-3.5 w-3.5 text-[var(--accent-primary)]/70" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M11 5H6a2 2 0 00-2 2v11a2 2 0 002 2h11a2 2 0 002-2v-5m-1.414-9.414a2 2 0 112.828 2.828L11.828 15H9v-2.828l8.586-8.586z" />
            </svg>
          </button>
        </div>

        {/* Question input */}
        <form onSubmit={handleSubmit} className="mt-4">
          <div className="relative">
            <input
              ref={inputRef}
              type="text"
              

'use client';

import React, {useState, useRef, useEffect} from 'react';
import {FaChevronLeft, FaChevronRight } from 'react-icons/fa';
import Markdown from './Markdown';
import { useLanguage } from '@/contexts/LanguageContext';
import RepoInfo from '@/types/repoinfo';
import getRepoUrl from '@/utils/getRepoUrl';
import ModelSelectionModal from './ModelSelectionModal';
import { createChatWebSocket, closeWebSocket, ChatCompletionRequest } from '@/utils/websocketClient';

interface Model {
  id: string;
  name: string;
}

interface Provider {
  id: string;
  name: string;
  models: Model[];
  supportsCustomModel?: boolean;
}

interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

interface ResearchStage {
  title: string;
  content: string;
  iteration: number;
  type: 'plan' | 'update' | 'conclusion';
}

interface AskProps {
  repoInfo: RepoInfo;
  provider?: string;
  model?: string;
  isCustomModel?: boolean;
  customModel?: string;
  language?: string;
  onRef?: (ref: { clearConversation: () => void }) => void;
}

const Ask: React.FC<AskProps> = ({
  repoInfo,
  provider = '',
  model = '',
  isCustomModel = false,
  customModel = '',
  language = 'en',
  onRef
}) => {
  const [question, setQuestion] = useState('');
  const [response, setResponse] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [deepResearch, setDeepResearch] = useState(false);

  // Model selection state
  const [selectedProvider, setSelectedProvider] = useState(provider);
  const [selectedModel, setSelectedModel] = useState(model);
  const [isCustomSelectedModel, setIsCustomSelectedModel] = useState(isCustomModel);
  const [customSelectedModel, setCustomSelectedModel] = useState(customModel);
  const [isModelSelectionModalOpen, setIsModelSelectionModalOpen] = useState(false);
  const [isComprehensiveView, setIsComprehensiveView] = useState(true);

  // Get language context for translations
  const { messages } = useLanguage();

  // Research navigation state
  const [researchStages, setResearchStages] = useState<ResearchStage[]>([]);
  const [currentStageIndex, setCurrentStageIndex] = useState(0);
  const [conversationHistory, setConversationHistory] = useState<Message[]>([]);
  const [researchIteration, setResearchIteration] = useState(0);
  const [researchComplete, setResearchComplete] = useState(false);
  const inputRef = useRef<HTMLInputElement>(null);
  const responseRef = useRef<HTMLDivElement>(null);
  const providerRef = useRef(provider);
  const modelRef = useRef(model);

  // Focus input on component mount
  useEffect(() => {
    if (inputRef.current) {
      inputRef.current.focus();
    }
  }, []);

  // Expose clearConversation method to parent component
  useEffect(() => {
    if (onRef) {
      onRef({ clearConversation });
    }
  }, [onRef]);

  // Scroll 

<span>Synthesizing all research findings...</span>
                      </div>
                    </>
                  )}
                </div>
              </div>
            )}
          </div>
        )}
      </div>

      {/* Model Selection Modal */}
      <ModelSelectionModal
        isOpen={isModelSelectionModalOpen}
        onClose={() => setIsModelSelectionModalOpen(false)}
        provider={selectedProvider}
        setProvider={setSelectedProvider}
        model={selectedModel}
        setModel={setSelectedModel}
        isCustomModel={isCustomSelectedModel}
        setIsCustomModel={setIsCustomSelectedModel}
        customModel={customSelectedModel}
        setCustomModel={setCustomSelectedModel}
        isComprehensiveView={isComprehensiveView}
        setIsComprehensiveView={setIsComprehensiveView}
        showFileFilters={false}
        onApply={() => {
          console.log('Model selection applied:', selectedProvider, selectedModel);
        }}
        showWikiType={false}
        authRequired={false}
        isAuthLoading={false}
      />
    </div>
  );
};

export default Ask;


},
        // Close handler
        () => {
          // If deep research is enabled, check if we should continue
          if (deepResearch) {
            const isComplete = checkIfResearchComplete(fullResponse);
            setResearchComplete(isComplete);

            // If not complete, start the research process
            if (!isComplete) {
              setResearchIteration(1);
              // The continueResearch function will be triggered by the useEffect
            }
          }

          setIsLoading(false);
        }
      );
    } catch (error) {
      console.error('Error during API call:', error);
      setResponse(prev => prev + '\n\nError: Failed to get a response. Please try again.');
      setResearchComplete(true);
      setIsLoading(false);
    }
  };

  const [buttonWidth, setButtonWidth] = useState(0);
  const buttonRef = useRef<HTMLButtonElement>(null);

  // Measure button width and update state
  useEffect(() => {
    if (buttonRef.current) {
      const width = buttonRef.current.offsetWidth;
      setButtonWidth(width);
    }
  }, [messages.ask?.askButton, isLoading]);

  return (
    <div>
      <div className="p-4">
        <div className="flex items-center justify-end mb-4">
          {/* Model selection button */}
          

## File Path: src/components/ModelSelectionModal.tsx

   className="text-[var(--muted)] hover:text-[var(--foreground)] focus:outline-none transition-colors"
            >
              <svg className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
              </svg>
            </button>
          </div>

          {/* Modal body */}
          <div className="p-6">
            {/* Wiki Type Selector */}
            {
              showWikiType && <WikiTypeSelector
                    isComprehensiveView={localIsComprehensiveView}
                    setIsComprehensiveView={setLocalIsComprehensiveView}
                />
            }

            {/* Divider */}
            <div className="my-4 border-t border-[var(--border-color)]/30"></div>

            {/* Model Selector */}
            <UserSelector
              provider={localProvider}
              setProvider={setLocalProvider}
              model={localModel}
              setModel={setLocalModel}
 

## File Path: src/components/ConfigurationModal.tsx

</div>

            {/* Language selection */}
            <div className="mb-4">
              <label htmlFor="language-select" className="block text-sm font-medium text-[var(--foreground)] mb-2">
                {t.form?.wikiLanguage || 'Wiki Language'}
              </label>
              <select
                id="language-select"
                value={selectedLanguage}
                onChange={(e) => setSelectedLanguage(e.target.value)}
                className="input-japanese block w-full px-3 py-2 text-sm rounded-md bg-transparent text-[var(--foreground)] focus:outline-none focus:border-[var(--accent-primary)]"
              >
                {
                  Object.entries(supportedLanguages).map(([key, value])=> <option key={key} value={key}>{value}</option>)
                }
              </select>
            </div>

            {/* Wiki Type Selector - more compact version */}
            <div className="mb-4">
              <label className="block text-sm font-medium text-[var(--foreground)] mb-2">
                {t.form?.wikiType || 'Wiki Type'}
              

       : 'bg-[var(--background)]/50 border-[var(--border-color)] text-[var(--foreground)] hover:bg-[var(--background)]'
                  }`}
                >
                  <div className="flex items-center">
                    <div className="text-left">
                      <div className="font-medium text-sm">{t.form?.concise || 'Concise'}</div>
                      <div className="text-xs opacity-80">
                        {t.form?.conciseDescription || 'Simplified wiki with fewer pages'}
                      </div>
                    </div>
                  </div>
                  {!isComprehensiveView && (
                    <div className="ml-2 h-4 w-4 rounded-full bg-[var(--accent-primary)]/20 flex items-center justify-center">
                      <div className="h-2 w-2 rounded-full bg-[var(--accent-primary)]"></div>
                    </div>
                  )}
            

             : 'bg-[var(--background)]/50 border-[var(--border-color)] text-[var(--foreground)] hover:bg-[var(--background)]'
                  }`}
                >
                  <div className="flex items-center">
                    <div className="text-left">
                      <div className="font-medium text-sm">{t.form?.comprehensive || 'Comprehensive'}</div>
                      <div className="text-xs opacity-80">
                        {t.form?.comprehensiveDescription || 'Detailed wiki with structured sections'}
                      </div>
                    </div>
                  </div>
                  {isComprehensiveView && (
                    <div className="ml-2 h-4 w-4 rounded-full bg-[var(--accent-primary)]/20 flex items-center justify-center">
                      <div className="h-2 w-2 rounded-full bg-[var(--accent-primary)]"></div>
                    </div>
                  )}
      

## File Path: src/app/page.tsx

        placeholder={t('form.repoPlaceholder') || "owner/repo, GitHub/GitLab/BitBucket URL, or local folder path"}
                  className="input-japanese block w-full pl-10 pr-3 py-2.5 border-[var(--border-color)] rounded-lg bg-transparent text-[var(--foreground)] focus:outline-none focus:border-[var(--accent-primary)]"
                />
                {error && (
                  <div className="text-[var(--highlight)] text-xs mt-1">
                    {error}
                  </div>
                )}
              </div>
              <button
                type="submit"
                className="btn-japanese px-6 py-2.5 rounded-lg disabled:opacity-50 disabled:cursor-not-allowed"
                disabled={isSubmitting}
              >
                {isSubmitting ? t('common.processing') : t('common.generateWiki')}
              </button>
            </div>
          </form>

          {/* Configuration Modal */}
          <ConfigurationModal
            isOpen={isConfigModalOpen}
          

## File Path: src/messages/zh.json

  "useCustomModel": "使用自定义模型",
    "customModelPlaceholder": "输入自定义模型名称",
    "addTokens": "+ 添加私有仓库访问令牌",
    "hideTokens": "- 隐藏访问令牌",
    "accessToken": "私有仓库访问令牌",
    "selectPlatform": "选择平台",
    "personalAccessToken": "{platform}个人访问令牌",
    "tokenPlaceholder": "输入您的{platform}令牌",
    "tokenSecurityNote": "令牌仅存储在内存中，从不持久化。",
    "defaultFiltersInfo": "默认过滤器包括node_modules、.git和常见的构建文件。",
    "fileFilterTitle": "文件过滤配置",
    "advancedOptions": "高级选项",
    "viewDefaults": "查看默认过滤",
    "showFilters": "显示过滤器",
    "hideFilters": "隐藏过滤器",
    "excludedDirs": "要排除的目录",
    "excludedDirsHelp": "每行一个目录路径。以./开头表示相对于仓库根目录的路径。",
    "enterExcludedDirs": "输入要排除的目录，每行一个...",
    "excludedFiles": "要排除的文件",
    "excludedFilesHelp": "每行一个文件名。支持通配符(*)。",
    "enterExcludedFiles": "输入要排除的文件，每行一个...",
    "defaultFilters": "默认排除的文件和目录",
    "directories": "目录",
    "files": "文件",
    "scrollToViewMore": "可滑动查看更多",
    "changeModel": "修改模型",
    "defaultNote": "这些默认配置已经被应用。请在上方添加您的自定义排除项。",
    "hideDefault": "隐藏默认配置",
    "viewDefault": "查看默认配置",
    "authorizationCode": "授权码",
    "authorizationRequired": "生成wiki页面需要填写授权码"
  },
  "footer": {
    "copyright": "DeepWiki - 为代码仓库提供AI驱动的文档"
  },
  "ask": {
    "placeholder": "询问关于此仓库的问题...",
    "askButton": "提问",
    "deepResearch": "深度研究",
    "researchInProgress": "研究进行中...",
    "continueResearch": "继续研究",
    "viewPlan": "查看计划",
    "viewUpdates": "查看更新",
    "viewConclusion": "查看结论"
  },
  "repoPage": {
    "refreshWiki": "刷新Wiki",
    "confirmRefresh": "确认刷新",
    "cancel": "取消",
    "home": "首页",
    "errorTitle": "错误",
    "errorMessageDefault": "请检查您的仓库是否存在且为公开仓库。有效格式为\"owner/repo\", \"https://github.com/owner/repo\", \"https://gitlab.com/owner/repo\", \"https://bitbucket.org/owner/repo\", 或本地文件夹路径，如\"C:\\\\path\\\\to\\\\folder\"或\"/path/to/folder\"。",
    "embeddingErrorDefault": "这个错误与用于分析您的仓库的文件嵌入系统有关。请检查您的嵌入模型配置、API 密钥，并重试。如果问题持续存在，请考虑在模型设置中切换到不同的嵌入提供者。",
    "backToHome": "返回首页",
    "exportWiki": "导出Wiki",
    "exportAsMarkdown": "导出为Markdown",
    "exportAsJson": "导出为JSON",
    "pages": "页面",
    "relatedFiles": "相关文件：",
    "relatedPages": "相关页面：",
    "selectPagePrompt": "从导航中选择一个页面以查看其内容",
    "askAboutRepo": "询问关于此仓库的问题"
  },
  "nav": {
    "wikiProjects": "项目列表"
  },
  "projects": {
    "title": "已处理的Wiki项目",
    "searchPlaceholder": "按项目名称、所有者或仓库搜索...",
    "noProjects": "服务器缓存中未找到项目。缓存可能为空或服务器遇到问题。",
    "noSearchResults": "没有项目符合您的搜索条件。",
    "processedOn": "处理时间:",
    "loadingProjects": "正在加载项目...",
   
<END_OF_CONTEXT>

<query>
将Ask模态框里面的内容窗口扩展到父窗口的90%
</query>

Assistant: 
2025-07-10 02:17:43,942 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 02:17:43,972 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 02:17:45,539 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:17:52,188 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
在文件 `src/components/Ask.tsx` 中，`Ask` 组件渲染了一个模态框（Modal），其内容窗口的样式定义如下：

```tsx
<div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[80vh] flex flex-col">
```

要将模态框内容窗口扩展到父窗口的 90%，可以修改 `max-w-3xl` 为 `w-[90%]` 并调整其他相关样式以确保适配。修改后的代码如下：

```tsx
<div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-[90%] max-h-[80vh] flex flex-col">
```

此外，如果希望模态框在小屏幕上保持响应式，可以添加 Tailwind CSS 的响应式前缀，例如：

```tsx
<div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-[90%] sm:w-[80%] max-h-[80vh] flex flex-col">
```

这样可以确保在不同屏幕尺寸下，模态框宽度合理适应。
2025-07-10 02:18:15,752 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:18:15,754 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:18:16,030 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:18:16,032 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:18:31,061 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 38 tokens
2025-07-10 02:18:31,091 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:18:31,093 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:18:31,093 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:18:31,093 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:18:31,126 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:18:31,134 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['system_prompt', 'conversation_history', 'output_format_str', 'input_str', 'contexts']
2025-07-10 02:18:31,134 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 02:18:31,134 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/AsyncFuncAI/deepwiki-open...
2025-07-10 02:18:31,134 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: AsyncFuncAI_deepwiki-open
2025-07-10 02:18:31,135 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/AsyncFuncAI_deepwiki-open. Using existing repository.
2025-07-10 02:18:31,135 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/AsyncFuncAI_deepwiki-open', 'save_db_file': '/root/.adalflow/databases/AsyncFuncAI_deepwiki-open.pkl'}
2025-07-10 02:18:31,135 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 02:18:31,146 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 02:18:31,146 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:18:31,146 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:18:31,146 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:18:31,204 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:18:31,204 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:18:31,205 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:18:31,265 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 821 documents from existing database
2025-07-10 02:18:31,265 - INFO - api.rag - rag.py:421 - Loaded 821 documents for retrieval
2025-07-10 02:18:31,266 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 821 documents)
2025-07-10 02:18:31,266 - INFO - api.rag - rag.py:384 - Embedding validation complete: 821/821 documents have valid embeddings
2025-07-10 02:18:31,266 - INFO - api.rag - rag.py:429 - Using 821 documents with valid embeddings for retrieval
2025-07-10 02:18:31,266 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:18:31,278 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 821 chunks
2025-07-10 02:18:31,279 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 02:18:31,279 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/AsyncFuncAI/deepwiki-open
2025-07-10 02:18:31,282 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['将Ask模态框里面的内容窗口扩展到父窗口的高度的90%，请在ask.tsx的源码上修改']}
2025-07-10 02:18:31,512 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 02:18:31,515 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 02:18:31,515 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:18:31,516 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:18:31,516 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/AsyncFuncAI/deepwiki-open (deepwiki-open).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: src/app/[owner]/[repo]/page.tsx

${isAskModalOpen ? 'opacity-100' : 'opacity-0 pointer-events-none'}`}>
        <div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[80vh] flex flex-col">
          <div className="flex items-center justify-end p-3 absolute top-0 right-0 z-10">
            <button
              onClick={() => {
                // Just close the modal without clearing the conversation
                setIsAskModalOpen(false);
              }}
              className="text-[var(--muted)] hover:text-[var(--foreground)] transition-colors bg-[var(--card-bg)]/80 rounded-full p-2"
              aria-label="Close"
            >
              <FaTimes className="text-xl" />
            </button>
          </div>
          <div className="flex-1 overflow-y-auto p-4">
            <Ask
              repoInfo={effectiveRepoInfo}
              provider={selectedProviderState}
              model={selectedModelState}
              isCustomModel={isCustomSelectedModelState}
              customModel={customSelectedModelState}
              language={language}
              onRef={(ref) => (askComponentRef.current = ref)}
            />
       

 </main>

      <footer className="max-w-[90%] xl:max-w-[1400px] mx-auto mt-8 flex flex-col gap-4 w-full">
        <div className="flex justify-between items-center gap-4 text-center text-[var(--muted)] text-sm h-fit w-full bg-[var(--card-bg)] rounded-lg p-3 shadow-sm border border-[var(--border-color)]">
          <p className="flex-1 font-serif">
            {messages.footer?.copyright || 'DeepWiki - Generate Wiki from GitHub/Gitlab/Bitbucket repositories'}
          </p>
          <ThemeToggle />
        </div>
      </footer>

      {/* Floating Chat Button */}
      {!isLoading && wikiStructure && (
        <button
          onClick={() => setIsAskModalOpen(true)}
          className="fixed bottom-6 right-6 w-14 h-14 rounded-full bg-[var(--accent-primary)] text-white shadow-lg flex items-center justify-center hover:bg-[var(--accent-primary)]/90 transition-all z-50"
          aria-label={messages.ask?.title || 'Ask about this repository'}
        >
          <FaComments className="text-xl" />
        </button>
      )}

      {/* Ask Modal - Always render but conditionally show/hide */}
      <div className={`fixed inset-0 bg-black/50 flex items-center justify-center z-50 p-4 transition-opacity duration-300 ${isAskModalOpen ? 'opacity-100' : 'opacity-0 pointer-events-none'}`}>
        <div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[80vh] flex flex-col">
          <div className="flex items-center justify-end p-3 absolute top-0 right-0 z-10">
            <button
              onClick={() => {
                // Just close the modal without clearing the conversation
         

}
  }, [currentPageId]);

  // close the modal when escape is pressed
  useEffect(() => {
    const handleEsc = (event: KeyboardEvent) => {
      if (event.key === 'Escape') {
        setIsAskModalOpen(false);
      }
    };

    if (isAskModalOpen) {
      window.addEventListener('keydown', handleEsc);
    }

    // Cleanup on unmount or when modal closes
    return () => {
      window.removeEventListener('keydown', handleEsc);
    };
  }, [isAskModalOpen]);

  // Fetch authentication status on component mount
  useEffect(() => {
    const fetchAuthStatus = async () => {
      try {
        setIsAuthLoading(true);
        const response = await fetch('/api/auth/status');
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        const data = await response.json();
        setAuthRequired(data.auth_required);
      } catch (err) {
        console.error("Failed to fetch auth status:", err);
        // Assuming auth is required if fetch fails to avoid blocking UI for safety
        setAuthRequired(true);
      } finally {
        setIsAuthLoading(false);
      }
    };

    fetchAuthStatus();
  }, []);

  // Generate content for a wiki page
  const generatePageContent = useCallback(async (page: WikiPage, owner: string, repo: string) => {
    return new Promise<void>(async (resolve) => {
      try {
        // Skip if content already exists
        if (generatedPages[page.id]?.content) {
 

     return filePath;
    }

    try {
      const url = new URL(repoUrl);
      const hostname = url.hostname;
      
      if (hostname === 'github.com' || hostname.includes('github')) {
        // GitHub URL format: https://github.com/owner/repo/blob/branch/path
        return `${repoUrl}/blob/${defaultBranch}/${filePath}`;
      } else if (hostname === 'gitlab.com' || hostname.includes('gitlab')) {
        // GitLab URL format: https://gitlab.com/owner/repo/-/blob/branch/path
        return `${repoUrl}/-/blob/${defaultBranch}/${filePath}`;
      } else if (hostname === 'bitbucket.org' || hostname.includes('bitbucket')) {
        // Bitbucket URL format: https://bitbucket.org/owner/repo/src/branch/path
        return `${repoUrl}/src/${defaultBranch}/${filePath}`;
      }
    } catch (error) {
      console.warn('Error generating file URL:', error);
    }

    // Fallback to just the file path
    return filePath;
  }, [effectiveRepoInfo, defaultBranch]);

  // Memoize repo info to avoid triggering updates in callbacks

  // Add useEffect to handle scroll reset
  useEffect(() => {
    // Scroll to top when currentPageId changes
    const wikiContent = document.getElementById('wiki-content');
    if (wikiContent) {
      wikiContent.scrollTo({ top: 0, behavior: 'smooth' });
    }
  }, [currentPageId]);

  // close the modal when escape is pressed
  useEffect(() => {
    const handleEsc = (event: KeyboardEvent) => {
      if (event.key === 'Escape') {
        setIsAskModalOpen(false);
      }
    };

    if (isAskModalOpen) {
      window.addEventListener('keydown', handleEsc);
    }

    // Cleanup on unmount or when modal closes
    return () => {
      window.removeEventListener('keydown', handleEsc);
    

       provider={selectedProviderState}
              model={selectedModelState}
              isCustomModel={isCustomSelectedModelState}
              customModel={customSelectedModelState}
              language={language}
              onRef={(ref) => (askComponentRef.current = ref)}
            />
          </div>
        </div>
      </div>

      <ModelSelectionModal
        isOpen={isModelSelectionModalOpen}
        onClose={() => setIsModelSelectionModalOpen(false)}
        provider={selectedProviderState}
        setProvider={setSelectedProviderState}
        model={selectedModelState}
        setModel={setSelectedModelState}
        isCustomModel={isCustomSelectedModelState}
        setIsCustomModel={setIsCustomSelectedModelState}
        customModel={customSelectedModelState}
        setCustomModel={setCustomSelectedModelState}
        isComprehensiveView={isComprehensiveView}
        setIsComprehensiveView={setIsComprehensiveView}
        showFileFilters={true}
        excludedDirs={modelExcludedDirs}
        setExcludedDirs={setModelExcludedDirs}
        excludedFiles={modelExcludedFiles}
        setExcludedFiles={setModelExcludedFiles}
        onApply={confirmRefresh}
        showWikiType={true}
        showTokenInput={effectiveRepoInfo.type !== 'local' && !currentToken} // Show token input if not local and no current token
        repositoryType={effectiveRepoInfo.type as 'github' | 'gitlab' | 'bitbucket'}
        authRequired={authRequired}
        authCode={authCode}
        setAuthCode={setAuthCode}
        isAuthLoading={isAuthLoading}
    

## File Path: src/components/Ask.tsx

    type="submit"
              disabled={isLoading || !question.trim()}
              className={`absolute right-3 top-1/2 transform -translate-y-1/2 px-4 py-2 rounded-md font-medium text-sm ${
                isLoading || !question.trim()
                  ? 'bg-[var(--button-disabled-bg)] text-[var(--button-disabled-text)] cursor-not-allowed'
                  : 'bg-[var(--accent-primary)] text-white hover:bg-[var(--accent-primary)]/90 shadow-sm'
              } transition-all duration-200 flex items-center gap-1.5`}
            >
              {isLoading ? (
                <div className="w-4 h-4 rounded-full border-2 border-t-transparent border-white animate-spin" />
              ) : (
                <>
                  <svg className="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13 5l7 7-7 7M5 5l7 7-7 7" />
                  </svg>
                  <span>{messages.ask?.askButton || 'Ask'}</span>
                </>
              )}
            </button>
     

const [buttonWidth, setButtonWidth] = useState(0);
  const buttonRef = useRef<HTMLButtonElement>(null);

  // Measure button width and update state
  useEffect(() => {
    if (buttonRef.current) {
      const width = buttonRef.current.offsetWidth;
      setButtonWidth(width);
    }
  }, [messages.ask?.askButton, isLoading]);

  return (
    <div>
      <div className="p-4">
        <div className="flex items-center justify-end mb-4">
          {/* Model selection button */}
          <button
            type="button"
            onClick={() => setIsModelSelectionModalOpen(true)}
            className="text-xs px-2.5 py-1 rounded border border-[var(--border-color)]/40 bg-[var(--background)]/10 text-[var(--foreground)]/80 hover:bg-[var(--background)]/30 hover:text-[var(--foreground)] transition-colors flex items-center gap-1.5"
          >
            <span>{selectedProvider}/{isCustomSelectedModel ? customSelectedModel : selectedModel}</span>
            <svg className="h-3.5 w-3.5 text-[var(--accent-primary)]/70" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M11 5H6a2 2 0 00-2 2v11a2 2 0 002 2h11a2 2 0 002-2v-5m-1.414-9.414a2 2 0 112.828 2.828L11.828 15H9v-2.828l8.586-8.586z" />
            </svg>
          </button>
        </div>

        {/* Question input */}
        <form onSubmit={handleSubmit} className="mt-4">
          <div className="relative">
            <input
              ref={inputRef}
              type="text"
              

    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M13 5l7 7-7 7M5 5l7 7-7 7" />
                  </svg>
                  <span>{messages.ask?.askButton || 'Ask'}</span>
                </>
              )}
            </button>
          </div>

          {/* Deep Research toggle */}
          <div className="flex items-center mt-2 justify-between">
            <div className="group relative">
              <label className="flex items-center cursor-pointer">
                <span className="text-xs text-gray-600 dark:text-gray-400 mr-2">Deep Research</span>
                <div className="relative">
                  <input
                    type="checkbox"
                    checked={deepResearch}
                    onChange={() => setDeepResearch(!deepResearch)}
                    className="sr-only"
                  />
                  <div className={`w-10 h-5 rounded-full transition-colors ${deepResearch ? 'bg-purple-600' : 'bg-gray-300 dark:bg-gray-600'}`}></div>
  

      </button>
        </div>

        {/* Question input */}
        <form onSubmit={handleSubmit} className="mt-4">
          <div className="relative">
            <input
              ref={inputRef}
              type="text"
              value={question}
              onChange={(e) => setQuestion(e.target.value)}
              placeholder={messages.ask?.placeholder || 'What would you like to know about this codebase?'}
              className="block w-full rounded-md border border-[var(--border-color)] bg-[var(--input-bg)] text-[var(--foreground)] px-5 py-3.5 text-base shadow-sm focus:border-[var(--accent-primary)] focus:ring-2 focus:ring-[var(--accent-primary)]/30 focus:outline-none transition-all"
              style={{ paddingRight: `${buttonWidth + 24}px` }}
              disabled={isLoading}
            />
            <button
              ref={buttonRef}
              type="submit"
              disabled={isLoading || !question.trim()}
              className={`absolute right-3 top-1/2 transform -translate-y-1/2 px-4 py-2 rounded-md font-medium text-sm ${
                isLoading || !question.trim()
                  ? 'bg-[var(--button-disabled-bg)] text-[var(--button-disabled-text)] cursor-not-allowed'
                 

'use client';

import React, {useState, useRef, useEffect} from 'react';
import {FaChevronLeft, FaChevronRight } from 'react-icons/fa';
import Markdown from './Markdown';
import { useLanguage } from '@/contexts/LanguageContext';
import RepoInfo from '@/types/repoinfo';
import getRepoUrl from '@/utils/getRepoUrl';
import ModelSelectionModal from './ModelSelectionModal';
import { createChatWebSocket, closeWebSocket, ChatCompletionRequest } from '@/utils/websocketClient';

interface Model {
  id: string;
  name: string;
}

interface Provider {
  id: string;
  name: string;
  models: Model[];
  supportsCustomModel?: boolean;
}

interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

interface ResearchStage {
  title: string;
  content: string;
  iteration: number;
  type: 'plan' | 'update' | 'conclusion';
}

interface AskProps {
  repoInfo: RepoInfo;
  provider?: string;
  model?: string;
  isCustomModel?: boolean;
  customModel?: string;
  language?: string;
  onRef?: (ref: { clearConversation: () => void }) => void;
}

const Ask: React.FC<AskProps> = ({
  repoInfo,
  provider = '',
  model = '',
  isCustomModel = false,
  customModel = '',
  language = 'en',
  onRef
}) => {
  const [question, setQuestion] = useState('');
  const [response, setResponse] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [deepResearch, setDeepResearch] = useState(false);

  // Model selection state
  const [selectedProvider, setSelectedProvider] = useState(provider);
  const [selectedModel, setSelectedModel] = useState(model);
  const [isCustomSelectedModel, setIsCustomSelectedModel] = useState(isCustomModel);
  const [customSelectedModel, setCustomSelectedModel] = useState(customModel);
  const [isModelSelectionModalOpen, setIsModelSelectionModalOpen] = useState(false);
  const [isComprehensiveView, setIsComprehensiveView] = useState(true);

  // Get language context for translations
  const { messages } = useLanguage();

  // Research navigation state
  const [researchStages, setResearchStages] = useState<ResearchStage[]>([]);
  const [currentStageIndex, setCurrentStageIndex] = useState(0);
  const [conversationHistory, setConversationHistory] = useState<Message[]>([]);
  const [researchIteration, setResearchIteration] = useState(0);
  const [researchComplete, setResearchComplete] = useState(false);
  const inputRef = useRef<HTMLInputElement>(null);
  const responseRef = useRef<HTMLDivElement>(null);
  const providerRef = useRef(provider);
  const modelRef = useRef(model);

  // Focus input on component mount
  useEffect(() => {
    if (inputRef.current) {
      inputRef.current.focus();
    }
  }, []);

  // Expose clearConversation method to parent component
  useEffect(() => {
    if (onRef) {
      onRef({ clearConversation });
    }
  }, [onRef]);

  // Scroll 

},
        // Close handler
        () => {
          // If deep research is enabled, check if we should continue
          if (deepResearch) {
            const isComplete = checkIfResearchComplete(fullResponse);
            setResearchComplete(isComplete);

            // If not complete, start the research process
            if (!isComplete) {
              setResearchIteration(1);
              // The continueResearch function will be triggered by the useEffect
            }
          }

          setIsLoading(false);
        }
      );
    } catch (error) {
      console.error('Error during API call:', error);
      setResponse(prev => prev + '\n\nError: Failed to get a response. Please try again.');
      setResearchComplete(true);
      setIsLoading(false);
    }
  };

  const [buttonWidth, setButtonWidth] = useState(0);
  const buttonRef = useRef<HTMLButtonElement>(null);

  // Measure button width and update state
  useEffect(() => {
    if (buttonRef.current) {
      const width = buttonRef.current.offsetWidth;
      setButtonWidth(width);
    }
  }, [messages.ask?.askButton, isLoading]);

  return (
    <div>
      <div className="p-4">
        <div className="flex items-center justify-end mb-4">
          {/* Model selection button */}
          

<span>Synthesizing all research findings...</span>
                      </div>
                    </>
                  )}
                </div>
              </div>
            )}
          </div>
        )}
      </div>

      {/* Model Selection Modal */}
      <ModelSelectionModal
        isOpen={isModelSelectionModalOpen}
        onClose={() => setIsModelSelectionModalOpen(false)}
        provider={selectedProvider}
        setProvider={setSelectedProvider}
        model={selectedModel}
        setModel={setSelectedModel}
        isCustomModel={isCustomSelectedModel}
        setIsCustomModel={setIsCustomSelectedModel}
        customModel={customSelectedModel}
        setCustomModel={setCustomSelectedModel}
        isComprehensiveView={isComprehensiveView}
        setIsComprehensiveView={setIsComprehensiveView}
        showFileFilters={false}
        onApply={() => {
          console.log('Model selection applied:', selectedProvider, selectedModel);
        }}
        showWikiType={false}
        authRequired={false}
        isAuthLoading={false}
      />
    </div>
  );
};

export default Ask;


## File Path: src/components/Mermaid.tsx

 children: React.ReactNode;
}> = ({ isOpen, onClose, children }) => {
  const modalRef = useRef<HTMLDivElement>(null);
  const [zoom, setZoom] = useState(1);

  // Close on Escape key
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === 'Escape') {
        onClose();
      }
    };

    if (isOpen) {
      document.addEventListener('keydown', handleKeyDown);
    }

    return () => {
      document.removeEventListener('keydown', handleKeyDown);
    };
  }, [isOpen, onClose]);

  // Handle click outside to close
  useEffect(() => {
    const handleOutsideClick = (e: MouseEvent) => {
      if (modalRef.current && !modalRef.current.contains(e.target as Node)) {
        onClose();
      }
    };

    if (isOpen) {
      document.addEventListener('mousedown', handleOutsideClick);
    }

    return () => {
      document.removeEventListener('mousedown', handleOutsideClick);
    };
  }, [isOpen, onClose]);

  // Reset zoom when modal opens
  useEffect(() => {
    if (isOpen) {
      setZoom(1);
    }
  }, [isOpen]);

  if (!isOpen) return null;

  return (
    <div className="fixed inset-0 z-50 flex items-center justify-center bg-black bg-opacity-75 p-4">
      <div
        ref={modalRef}
        className="bg-[var(--card-bg)] rounded-lg shadow-custom max-w-5xl max-h-[90vh] w-full overflow-hidden flex flex-col card-japanese"
      >
        {/* Modal header with controls */}
        <div className="flex items-center justify-between p-4 border-b border-[var(--border-color)]">
          <div className="font-medium text-[var(--foreground)] font-serif">図表表示</div>
          <div className="flex items-center gap-4">
           

         </button>
            </div>
            <button
              onClick={onClose}
              className="text-[var(--foreground)] hover:bg-[var(--accent-primary)]/10 p-2 rounded-md border border-[var(--border-color)] transition-colors"
              aria-label="Close"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                <line x1="18" y1="6" x2="6" y2="18"></line>
                <line x1="6" y1="6" x2="18" y2="18"></line>
              </svg>
            </button>
          </div>
        </div>

        {/* Modal content with zoom */}
        <div className="overflow-auto p-6 flex-1 flex items-center justify-center bg-[var(--background)]/50">
          <div
            style={{
              transform: `scale(${zoom})`,
              transformOrigin: 'center center',
              transition: 'transform 0.3s ease-out'
            }}
          >
            {children}
          </div>
     

     <div
        ref={modalRef}
        className="bg-[var(--card-bg)] rounded-lg shadow-custom max-w-5xl max-h-[90vh] w-full overflow-hidden flex flex-col card-japanese"
      >
        {/* Modal header with controls */}
        <div className="flex items-center justify-between p-4 border-b border-[var(--border-color)]">
          <div className="font-medium text-[var(--foreground)] font-serif">図表表示</div>
          <div className="flex items-center gap-4">
            <div className="flex items-center gap-2">
              <button
                onClick={() => setZoom(Math.max(0.5, zoom - 0.1))}
                className="text-[var(--foreground)] hover:bg-[var(--accent-primary)]/10 p-2 rounded-md border border-[var(--border-color)] transition-colors"
                aria-label="Zoom out"
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <circle cx="11" cy="11" r="8"></circle>
                  <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                  <line x1="8" y1="11" x2="14" y2="11"></line>
                </svg>
              </button>
              <span className="text-sm text-[var(--muted)]">{Math.round(zoom * 100)}%</span>
              <button
   

     <div className="w-2 h-2 bg-[var(--accent-primary)]/70 rounded-full animate-pulse delay-150"></div>
          <span className="text-[var(--muted)] text-xs ml-2 font-serif">図表を描画中...</span>
        </div>
      </div>
    );
  }

  return (
    <>
      <div
        ref={containerRef}
        className={`w-full max-w-full ${zoomingEnabled ? "h-[600px] p-4" : ""}`}
      >
        <div
          className={`relative group ${zoomingEnabled ? "h-full rounded-lg border-2 border-black" : ""}`}
        >
          <div
            className={`flex justify-center overflow-auto text-center my-2 cursor-pointer hover:shadow-md transition-shadow duration-200 rounded-md ${className} ${zoomingEnabled ? "h-full" : ""}`}
            dangerouslySetInnerHTML={{ __html: svg }}
            onClick={zoomingEnabled ? undefined : handleDiagramClick}
            title={zoomingEnabled ? undefined : "Click to view fullscreen"}
          />

          {!zoomingEnabled && (
            <div className="absolute top-2 right-2 bg-gray-700/70 dark:bg-gray-900/70 text-white p-1.5 rounded-md opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex items-center gap-1.5 text-xs shadow-md pointer-events-none">
              <svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                <circle cx="11" cy="11" r="8"></circle>
                <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                <line x1="11" y1="8" x2="11" y2="14"></line>
    

import React, { useEffect, useRef, useState } from 'react';
import mermaid from 'mermaid';
// We'll use dynamic import for svg-pan-zoom

// Initialize mermaid with defaults - Japanese aesthetic
mermaid.initialize({
  startOnLoad: true,
  theme: 'neutral',
  securityLevel: 'loose',
  suppressErrorRendering: true,
  logLevel: 'error',
  maxTextSize: 100000, // Increase text size limit
  htmlLabels: true,
  flowchart: {
    htmlLabels: true,
    curve: 'basis',
    nodeSpacing: 60,
    rankSpacing: 60,
    padding: 20,
  },
  themeCSS: `
    /* Japanese aesthetic styles for all diagrams */
    .node rect, .node circle, .node ellipse, .node polygon, .node path {
      fill: #f8f4e6;
      stroke: #d7c4bb;
      stroke-width: 1px;
    }
    .edgePath .path {
      stroke: #9b7cb9;
      stroke-width: 1.5px;
    }
    .edgeLabel {
      background-color: transparent;
      color: #333333;
      p {
        background-color: transparent !important;
      }
    }
    .label {
      color: #333333;
    }
    .cluster rect {
      fill: #f8f4e6;
      stroke: #d7c4bb;
      stroke-width: 1px;
    }

    /* Sequence diagram specific styles */
    .actor {
      fill: #f8f4e6;
      stroke: #d7c4bb;
      stroke-width: 1px;
    }
    text.actor {
      fill: #333333;
      stroke: none;
    }
    .messageText {
      fill: #333333;
      stroke: none;
    }
    .messageLine0, .messageLine1 {
      stroke: #9b7cb9;
  

  <div className={`border border-[var(--highlight)]/30 rounded-md p-4 bg-[var(--highlight)]/5 ${className}`}>
        <div className="flex items-center mb-3">
          <div className="text-[var(--highlight)] text-xs font-medium flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" className="h-4 w-4 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z" />
            </svg>
            図表レンダリングエラー
          </div>
        </div>
        <div ref={mermaidRef} className="text-xs overflow-auto"></div>
        <div className="mt-3 text-xs text-[var(--muted)] font-serif">
          図表に構文エラーがあり、レンダリングできません。
        </div>
      </div>
    );
  }

  if (!svg) {
    return (
      <div className={`flex justify-center items-center p-4 ${className}`}>
        <div className="flex items-center space-x-2">
          <div className="w-2 h-2 bg-[var(--accent-primary)]/70 rounded-full animate-pulse"></div>
          <div className="w-2 h-2 bg-[var(--accent-primary)]/70 rounded-full animate-pulse delay-75"></div>
          <div className="w-2 h-2 bg-[var(--accent-primary)]/70 rounded-full animate-pulse delay-150"></div>
          <span className="text-[var(--muted)] text-xs ml-2 font-serif">図表を描画中...</span>
        </div>
      </div>
    );
  }

  return (
    <>
      <div
        ref={containerRef}
        className={`w-full max-w-full ${zoomingEnabled ? "h-[600px] p-4" : ""}`}
      >
        <div
     

## File Path: src/components/ModelSelectionModal.tsx

   className="text-[var(--muted)] hover:text-[var(--foreground)] focus:outline-none transition-colors"
            >
              <svg className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
              </svg>
            </button>
          </div>

          {/* Modal body */}
          <div className="p-6">
            {/* Wiki Type Selector */}
            {
              showWikiType && <WikiTypeSelector
                    isComprehensiveView={localIsComprehensiveView}
                    setIsComprehensiveView={setLocalIsComprehensiveView}
                />
            }

            {/* Divider */}
            <div className="my-4 border-t border-[var(--border-color)]/30"></div>

            {/* Model Selector */}
            <UserSelector
              provider={localProvider}
              setProvider={setLocalProvider}
              model={localModel}
              setModel={setLocalModel}
 

## File Path: src/app/page.tsx

mb-3 font-serif">{t('home.sequenceDiagram')}</h4>
                <Mermaid chart={DEMO_SEQUENCE_CHART} />
              </div>
            </div>
          </div>
            </>
          )}
        </div>
      </main>

      <footer className="max-w-6xl mx-auto mt-8 flex flex-col gap-4 w-full">
        <div
          className="flex flex-col sm:flex-row justify-between items-center gap-4 bg-[var(--card-bg)] rounded-lg p-4 border border-[var(--border-color)] shadow-custom">
          <p className="text-[var(--muted)] text-sm font-serif">{t('footer.copyright')}</p>

          <div className="flex items-center gap-6">
            <div className="flex items-center space-x-5">
              <a href="https://github.com/AsyncFuncAI/deepwiki-open" target="_blank" rel="noopener noreferrer"
                className="text-[var(--muted)] hover:text-[var(--accent-primary)] transition-colors">
                <FaGithub className="text-xl" />
              </a>
              <a href="https://buymeacoffee.com/sheing" target="_blank" rel="noopener noreferrer"
                className="text-[var(--muted)] hover:text-[var(--accent-primary)] transition-colors">
                <FaCoffee className="text-xl" />
              </a>
              <a href="https://x.com/sashimikun_void" target="_blank" rel="noopener noreferrer"
                className="text-[var(--muted)] hover:text-[var(--accent-primary)] transition-colors">
    
<END_OF_CONTEXT>

<query>
将Ask模态框里面的内容窗口扩展到父窗口的高度的90%，请在ask.tsx的源码上修改
</query>

Assistant: 
2025-07-10 02:18:31,516 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 02:18:31,545 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 02:18:33,177 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:18:38,035 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
在 `src/components/Ask.tsx` 中，将渲染模态框内容的 `div` 的 `max-h-[80vh]` 修改为 `max-h-[90vh]`。具体修改如下：

```tsx
// 修改前
<div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[80vh] flex flex-col">

// 修改后
<div className="bg-[var(--card-bg)] rounded-lg shadow-xl w-full max-w-3xl max-h-[90vh] flex flex-col">
```

这样可以将模态框内容窗口的高度扩展到父窗口的 90%。
2025-07-10 02:19:38,389 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 31 tokens
2025-07-10 02:19:38,418 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:19:38,419 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:19:38,419 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:19:38,419 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:19:38,450 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:19:38,457 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['system_prompt', 'conversation_history', 'output_format_str', 'input_str', 'contexts']
2025-07-10 02:19:38,457 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 02:19:38,457 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/AsyncFuncAI/deepwiki-open...
2025-07-10 02:19:38,457 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: AsyncFuncAI_deepwiki-open
2025-07-10 02:19:38,457 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/AsyncFuncAI_deepwiki-open. Using existing repository.
2025-07-10 02:19:38,457 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/AsyncFuncAI_deepwiki-open', 'save_db_file': '/root/.adalflow/databases/AsyncFuncAI_deepwiki-open.pkl'}
2025-07-10 02:19:38,457 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 02:19:38,475 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 02:19:38,475 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:19:38,475 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:19:38,475 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:19:38,535 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:19:38,535 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:19:38,535 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7f589feca0c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:19:38,591 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 821 documents from existing database
2025-07-10 02:19:38,591 - INFO - api.rag - rag.py:421 - Loaded 821 documents for retrieval
2025-07-10 02:19:38,591 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 821 documents)
2025-07-10 02:19:38,592 - INFO - api.rag - rag.py:384 - Embedding validation complete: 821/821 documents have valid embeddings
2025-07-10 02:19:38,592 - INFO - api.rag - rag.py:429 - Using 821 documents with valid embeddings for retrieval
2025-07-10 02:19:38,592 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:19:38,601 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 821 chunks
2025-07-10 02:19:38,601 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 02:19:38,601 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/AsyncFuncAI/deepwiki-open
2025-07-10 02:19:38,602 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['分析这个项目的向量嵌入的实现方案和工作流程，涉及到的相关源文件']}
2025-07-10 02:19:39,053 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 02:19:39,056 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 02:19:39,056 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:19:39,057 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:19:39,057 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/AsyncFuncAI/deepwiki-open (deepwiki-open).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: src/app/[owner]/[repo]/page.tsx

 *   Ensure snippets are well-formatted within Markdown code blocks with appropriate language identifiers.

6.  **Source Citations (EXTREMELY IMPORTANT):**
    *   For EVERY piece of significant information, explanation, diagram, table entry, or code snippet, you MUST cite the specific source file(s) and relevant line numbers from which the information was derived.
    *   Place citations at the end of the paragraph, under the diagram/table, or after the code snippet.
    *   Use the exact format: \`Sources: [filename.ext:start_line-end_line]()\` for a range, or \`Sources: [filename.ext:line_number]()\` for a single line. Multiple files can be cited: \`Sources: [file1.ext:1-10](), [file2.ext:5](), [dir/file3.ext]()\` (if the whole file is relevant and line numbers are not applicable or too broad).
    *   If an entire section is overwhelmingly based on one or two files, you can cite them under the section heading in addition to more specific citations within the section.
    *   IMPORTANT: You MUST cite AT LEAST 5 different source files throughout the wiki page to ensure comprehensive coverage.

7.  **Technical Accuracy:** All information must be derived SOLELY from the \`[RELEVANT_SOURCE_FILES]\`. Do not infer, invent, or use external knowledge about similar systems or common practices unless it's directly supported by the provided code. If information is not present in the provided files, do not include it or explicitly state its absence if crucial to the topic.

8.  **Clarity and Conciseness:** Use clear, professional, and concise technical language suitable for other developers working on or learning about the project. Avoid unnecessary jargon, but use correct technical terms where appropriate.

9.  **Conclusion/Summary:** End with a brief summary paragraph if appropriate for "${page.title}", reiterating the key aspects covered and their significance within the project.

IMPORTANT: Generate the content in ${language === 'en' ? 'English' :
            language === 'ja' ? 'Japanese (日本語)' :
            language === 'zh' ? 'Mandarin Chinese (中文)' :
        

     // Create the prompt content - simplified to avoid message dialogs
 const promptContent =
`You are an expert technical writer and software architect.
Your task is to generate a comprehensive and accurate technical wiki page in Markdown format about a specific feature, system, or module within a given software project.

You will be given:
1. The "[WIKI_PAGE_TOPIC]" for the page you need to create.
2. A list of "[RELEVANT_SOURCE_FILES]" from the project that you MUST use as the sole basis for the content. You have access to the full content of these files. You MUST use AT LEAST 5 relevant source files for comprehensive coverage - if fewer are provided, search for additional related files in the codebase.

CRITICAL STARTING INSTRUCTION:
The very first thing on the page MUST be a \`<details>\` block listing ALL the \`[RELEVANT_SOURCE_FILES]\` you used to generate the content. There MUST be AT LEAST 5 source files listed - if fewer were provided, you MUST find additional related files to include.
Format it exactly like this:
<details>
<summary>Relevant source files</summary>

Remember, do not provide any acknowledgements, disclaimers, apologies, or any other preface before the \`<details>\` block. JUST START with the \`<details>\` block.
The following files were used as context for generating this wiki page:

${filePaths.map(path => `- [${path}](${generateFileUrl(path)})`).join('\n')}
<!-- Add additional relevant files if fewer than 5 were provided -->
</details>

Immediately after the \`<details>\` block, the main title of the page should be a H1 Markdown heading: \`# ${page.title}\`.

Based ONLY on the content of the \`[RELEVANT_SOURCE_FILES]\`:

1.  **Introduction:** Start with a concise introduction (1-2 paragraphs) explaining the purpose, scope, and high-level overview of "${page.title}" within the context of the overall project. If relevant, and if information is available in the provided files, link to other potential wiki pages using the format \`[Link Text](#page-anchor-or-id)\`.

2.  **Detailed Sections:** Break down "${page.title}" into logical sections using H2 (\`##\`) and H3 (\`###\`) Markdown headings. For each section:
    *   Explain the architecture, components, data flow, or logic relevant to the section's focus, as evidenced in the source files.
    *   Identify key functions, classes, data structures, API endpoints, or configuration elements pertinent 

## File Path: README.ja.md

E4[Ollamaで生成]

    E1 --> E[ドキュメントを生成]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[視覚的な図を作成]
    E --> G[Wikiとして整理]
    F --> G
    G --> H[インタラクティブなDeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ プロジェクト構造

```
deepwiki/
├── api/                  # バックエンドAPIサーバー
│   ├── main.py           # APIエントリーポイント
│   ├── api.py            # FastAPI実装
│   ├── rag.py            # 検索拡張生成
│   ├── data_pipeline.py  # データ処理ユーティリティ
│   └── requirements.txt  # Python依存関係
│
├── src/                  # フロントエンドNext.jsアプリ
│   ├── app/              # Next.jsアプリディレクトリ
│   │   └── page.tsx      # メインアプリケーションページ
│   └── components/       # Reactコンポーネント
│       └── Mermaid.tsx   # Mermaid図レンダラー
│
├── public/               # 静的アセット
├── package.json          # JavaScript依存関係
└── .env                  # 環境変数（作成する必要あり）
```

## 🛠️ 高度な設定

### 環境変数

| 変数                          | 説明           

          | ✗    | API とフロントエンドを同じマシンでホストする場合、`NEXT_PUBLIC_SERVER_BASE_URL`のポートを適宜変更してください |
| `SERVER_BASE_URL`             | API サーバーのベース URL（デフォルト：`http://localhost:8001`） | ✗    |                                                                                                               |

### 設定ファイル

DeepWikiはシステムの様々な側面を管理するためにJSON設定ファイルを使用しています：

1. **`generator.json`**: テキスト生成モデルの設定
   - 利用可能なモデルプロバイダー（Google、OpenAI、OpenRouter、Ollama）を定義
   - 各プロバイダーのデフォルトおよび利用可能なモデルを指定
   - temperatureやtop_pなどのモデル固有のパラメータを含む

2. **`embedder.json`**: 埋め込みモデルとテキスト処理の設定
   - ベクトルストレージ用の埋め込みモデルを定義
   - RAG用の検索設定を含む
   - ドキュメントチャンク分割のためのテキスト分割設定を指定

3. **`repo.json`**: リポジトリ処理の設定
   - 特定のファイルやディレクトリを除外するファイルフィルターを含む
   - リポジトリサイズ制限と処理ルールを定義

デフォルトでは、これらのファイルは`api/config/`ディレクトリにあります。`DEEPWIKI_CONFIG_DIR`環境変数を使用して、その場所をカスタマイズできます。

### Docker セットアップ

Docker を使用して DeepWiki を実行できます：

```bash
# GitHub Container Registryからイメージをプル
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# 環境変数を設定してコンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

このコマンドは、ホスト上の ⁠~/.adalflow をコンテナ内の ⁠/root/.adalflow にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (⁠~/.adalflow/repos/)
- それらのエンベディングとインデックス (⁠~/.adalflow/databases/)
- 生成された Wiki のキャッシュ (⁠~/.adalflow/wikicache/)

これにより、コンテナが停止または削除されてもデータが永続化されます。
または、提供されている ⁠docker-compose.yml ファイルを使用します。

```bash
# まず.envファイルをAPIキーで編集
docker-compose up
```

（⁠docker-compose.yml ファイルは、上記の ⁠docker run コマンドと同様に、データ永続化のために ⁠~/.adalflow をマウントするように事前設定されています。）

#### Docker で.env ファイルを使用する

.env ファイルをコンテナにマウントすることもできます：

```bash
# APIキーを含む.envファイルを作成
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# .envファイルをマウントしてコンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

このコマンドは、ホスト上の ⁠~/.adalflow をコンテナ内の ⁠/root/.adalflow にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (⁠~/.adalflow/repos/)
- それらのエンベディングとインデックス (⁠~/.adalflow/databases/)
- 生成された Wiki のキャッシュ (⁠~/.adalflow/wikicache/)

これにより、コンテナが停止または削除されてもデータが永続化されます。

#### Docker イメージをローカルでビルドする

Docker イメージをローカルでビルドしたい場合：

```bash
# リポジトリをクローン
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Dockerイメージをビルド
docker build -t deepwiki-open .

# コンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  deepwiki-open
```

# API サーバー詳細

API サーバーは以下を提供します：

- リポジトリのクローンとインデックス作成
- RAG（Retrieval Augmented Generation：検索拡張生成）
- ストリーミングチャット補完

詳細については、API README を参照してください。

## 🤖 プロバイダーベースのモデル選択システム

DeepWikiでは、複数のLLMプロバイダーをサポートする柔軟なプロバイダーベースのモデル選択システムを実装しています：

### サポートされているプロバイダーとモデル

- **Google**: デフォルトは `gemini-2.0-flash`、また `gemini-1.5-flash`、`gemini-1.0-pro` などもサポート
- **OpenAI**: デフォルトは `gpt-4o`、また `o4-mini` などもサポート
- **OpenRouter**: Claude、Llama、Mistralなど、統一APIを通じて複数のモデルにアクセス
- 

で.env ファイルを使用する

.env ファイルをコンテナにマウントすることもできます：

```bash
# APIキーを含む.envファイルを作成
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# .envファイルをマウントしてコンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

このコマンドは、ホスト上の ⁠~/.adalflow をコンテナ内の ⁠/root/.adalflow にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (⁠~/.adalflow/repos/)
- それらのエンベディングとインデックス (⁠~/.adalflow/databases/)
- 生成された Wiki のキャッシュ (⁠~/.adalflow/wikicache/)

これにより、コンテナが停止または削除されてもデータが永続化されます。

#### Docker イメージをローカルでビルドする

Docker イメージをローカルでビルドしたい場合：

```bash
# リポジトリをクローン
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Dockerイメージをビルド
docker build -t deepwiki-open .

# コンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  deepwiki-open
```

# API サーバー詳細

API サーバーは以下を提供します：

- リポジトリのクローンとインデックス作成
- RAG（Retrieval Augmented Generation：検索拡張生成）
- ストリーミングチャット補完

詳細については、API README を参照してください。

## 🤖 プロバイダーベースのモデル選択システム

DeepWikiでは、複数のLLMプロバイダーをサポートする柔軟なプロバイダーベースのモデル選択システムを実装しています：

### サポートされているプロバイダーとモデル

- **Google**: デフォルトは `gemini-2.0-flash`、また `gemini-1.5-flash`、`gemini-1.0-pro` などもサポート
- **OpenAI**: デフォルトは `gpt-4o`、また `o4-mini` などもサポート
- **OpenRouter**: Claude、Llama、Mistralなど、統一APIを通じて複数のモデルにアクセス
- **Ollama**: `llama3` などのローカルで実行するオープンソースモデルをサポート

### 環境変数

各プロバイダーには、対応するAPI鍵の環境変数が必要です：

```
# API鍵
GOOGLE_API_KEY=あなたのGoogle API鍵        # Google Geminiモデルに必要
OPENAI_API_KEY=あなたのOpenAI鍵            # OpenAIモデルに必要
OPENROUTER_API_KEY=あなたのOpenRouter鍵    # OpenRouterモデルに必要

# OpenAI APIベースURL設定
OPENAI_BASE_URL=https://カスタムAPIエンドポイント.com/v1  # オプション、カスタムOpenAI APIエンドポイント用
```

### サービスプロバイダー向けのカスタムモデル選択

カスタムモデル選択機能は、あなたの組織のユーザーに様々なAIモデルの選択肢を提供するために特別に設計されています：

- あなたは組織内のユーザーに様々なAIモデルの選択肢を提供できます
- あなたはコード変更なしで急速に進化するLLM環境に迅速に適応できます
- あなたは事前定義リストにない専門的またはファインチューニングされたモデルをサポートできます

サービスプロバイダーは、事前定義されたオプションから選択するか、フロントエンドインターフェースでカスタムモデル識別子を入力することで、モデル提供を実装できます。

### エンタープライズプライベートチャネル向けのベースURL設定

OpenAIクライアントのbase_url設定は、主にプライベートAPIチャネルを持つエンタープライズユーザー向けに設計されています。この機能は：

- プライベートまたは企業固有のAPIエンドポイントへの接続を可能に
- 組織が自己ホスト型または独自にデプロイされたLLMサービスを使用可能に
- サードパーティのOpenAI API互換サービスとの統合をサポート

**近日公開**: 将来のアップデートでは、ユーザーがリクエストで自分のAPI鍵を提供する必要があるモードをDeepWikiがサポートする予定です。これにより、プライベートチャネルを持つエンタープライズ顧客は、DeepWikiデプロイメントと認証情報を共有することなく、既存のAPI設定を使用できるようになります。

## 🔌 OpenRouter 連携

DeepWiki は、モデルプロバイダーとして OpenRouter をサポートするようになり、単一の API を通じて数百の AI モデルにアクセスできるようになりました。

- 複数のモデルオプション: OpenAI、Anthropic、Google、Meta、Mistralなど、統一APIを通じて複数のモデルにアクセス
- 簡単な設定: OpenRouter API キーを追加し、使用したいモデルを選択するだけ
- コスト効率: 予算とパフォーマンスのニーズに合ったモデルを選択
- 簡単な切り替え: コードを変更することなく、異なるモデル間を切り替え可能

### DeepWiki で OpenRouter を使用する方法

1. API キーを取得: OpenRouter でサインアップし、API キーを取得します
2. 環境に追加: ⁠.env ファイルに ⁠OPENROUTER_API_KEY=your_key を追加します
3. UI で有効化: ホームページの「OpenRouter API を使用」オプションをチェックします
4. モデルを選択: GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 などの人気モデルから選択します

OpenRouter は特に以下のような場合に便利です：

- 複数のサービスにサインアップせずに異なるモデルを試したい
- お住まいの地域で制限されている可能性のあるモデルにアクセスしたい
- 異なるモデルプロバイダー間でパフォーマンスを比較したい
- ニーズに基づいてコストとパフォーマンスを最適化したい

## 🤖 質問と詳細調査機能

### 質問機能

質問機能を使用すると、検索拡張生成（RAG）を使用してリポジトリとチャットできます：

- **コンテキスト対応の回答**: リポジトリの実際のコードに基づいた正確な回答を取得
- **RAG 搭載**: システムは関連するコードスニペットを取得して根拠のある回答を提供
- **リアルタイムストリーミング**: よりインタラクティブな体験のために、生成されるレスポンスをリアルタイムで確認
- **会話履歴**: システムは質問間のコンテキストを維持し、より一貫性のあるインタラクションを実現

### 詳細調査機能

詳細調査は、複数ターンの研究プロセスでリポジトリ分析を次のレベルに引き上げます：

- **詳細な調査**: 複数の研究反復を通じて複雑なトピックを徹底的に探索
- **構造化されたプロセス**: 明確な研究計画、更新、包括的な結論を含む
- **自動継続**: AI は結論に達するまで自動的に研究を継続（最大 5 回の反復）
- **研究段階**:
  1. **研究計画**: アプローチと初期調査結果の概要
  2. **研究更新**: 新しい洞察を加えて前の反復を発展
  3. **最終結論**: すべての反復に基づく包括的な回答を提供

詳細調査を使用するには、質問を送信する前に質問インターフェースの「詳細調査」スイッチをオンにするだけです。

## 📱 スクリーンショット

![DeepWikiメインインターフェース](screenshots/Interface.png)
_DeepWiki のメインインターフェース_

![プライベートリポジトリサポート](screenshots/privaterepo.png)
_個人アクセストークンを使用したプライベートリポジトリへのアクセス_

![詳細調査機能](screenshots/DeepResearch.png)
_詳細調査は複雑なトピックに対して多段階の調査を実施_

### デモビデオ

[![DeepWikiデモビデオ](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

_DeepWiki の動作を見る！_

## ❓ トラブルシューティング

### API キーの問題

- **「環境変数が見つかりません」**: `.env`ファイルがプロジェクトのルートにあり、必要な API キーが含まれていることを確認
- **「API キーが無効です」**: キー全体が余分なスペースなしで正しくコピーされていることを確認
- **「OpenRouter API エラー」**: OpenRouter API キーが有効で、十分なクレジットがあることを確認

### 接続の問題

- **「API サーバーに接続できません」**: API サーバーがポート 8001 で実行されていることを確認
- **「CORS エラー」**: API はすべてのオリジンを許可するように設定されていますが、問題がある場合は、フロントエンドとバックエンドを同じマシンで実行してみてください

### 生成の問題

- **「Wiki の生成中にエラーが発生しました」**: 非常に大きなリポジトリの場合は、まず小さいものから試してみてください
- **「無効なリポジトリ形式」**: 有効な GitHub、GitLab、または Bitbucket URL の形式を使用していることを確認
- **「リポジトリ構造を取得できませんでした」**: プライベートリポジトリの場合、適切な権限を持つ有効な個人アクセストークンを入力したことを確認
- **「図のレンダリングエラー」**: アプリは自動的に壊れた図を修正しようとします

### 一般的な解決策

1. **両方のサーバーを再起動**: 単純な再起動でほとんどの問題が解決することがあります
2. **コンソールログを確認**: ブラウザの開発者ツールを開いて JavaScript エラーを確認
3. **API ログを確認**: API が実行されているターミナルで Python エラーを確認

## 🤝 貢献

貢献は歓迎します！以下のことを自由に行ってください：

- バグや機能リクエストの問題を開く
- コードを改善するためのプルリクエストを提出
- フィードバックやアイデアを共有

## 📄 ライセンス

このプロジェクトは MIT ライセンスの下でライセンスされています - 詳細は[LICENSE](LICENSE)ファイルを参照してください。

## ⭐ スター履歴

[![スター履歴チャート](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)



# DeepWiki-Open

![DeepWiki バナー](screenshots/Deepwiki.png)

**DeepWiki**は、GitHub、GitLab、または Bitbucket リポジトリのための美しくインタラクティブな Wiki を自動的に作成します！リポジトリ名を入力するだけで、DeepWiki は以下を行います：

1. コード構造を分析
2. 包括的なドキュメントを生成
3. すべての仕組みを説明する視覚的な図を作成
4. すべてを簡単に閲覧できる Wiki に整理

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ 特徴

- **即時ドキュメント生成**: あらゆる GitHub、GitLab、または Bitbucket リポジトリを数秒で Wiki に変換
- **プライベートリポジトリ対応**: 個人アクセストークンを使用してプライベートリポジトリに安全にアクセス
- **スマート分析**: AI を活用したコード構造と関係の理解
- **美しい図表**: アーキテクチャとデータフローを視覚化する自動 Mermaid 図
- **簡単なナビゲーション**: Wiki を探索するためのシンプルで直感的なインターフェース
- **質問機能**: RAG 搭載 AI を使用してリポジトリとチャットし、正確な回答を得る
- **詳細調査**: 複雑なトピックを徹底的に調査する多段階研究プロセス
- **複数のモデルプロバイダー**: Google Gemini、OpenAI、OpenRouter、およびローカル Ollama モデルのサポート

## 🚀 クイックスタート（超簡単！）

### オプション 1: Docker を使用

```bash
# リポジトリをクローン
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# APIキーを含む.envファイルを作成
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# オプション: OpenRouterモデルを使用する場合はOpenRouter APIキーを追加
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# Docker Composeで実行
docker-compose up
```

(上記の Docker コマンドおよび`docker-compose.yml`の設定では、ホスト上の`~/.adalflow`ディレクトリをコンテナ内の`/root/.adalflow`にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (`~/.adalflow/repos/`)
- それらのエンベディングとインデックス (`~/.adalflow/databases/`)
- 生成された Wiki のキャッシュ (`~/.adalflow/wikicache/`)

これにより、コンテナが停止または削除されてもデータが永続化されます。)

> 💡 **これらのキーの入手先:**
>
> - Google API キーは[Google AI Studio](https://makersuite.google.com/app/apikey)から取得
> - OpenAI API キーは[OpenAI Platform](https://platform.openai.com/api-keys)から取得

### オプション 2: 手動セットアップ（推奨）

#### ステップ 1: API キーの設定

プロジェクトのルートに`.env`ファイルを作成し、以下のキーを追加します：

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# オプション: OpenRouterモデルを使用する場合は追加
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### ステップ 2: バックエンドの起動

```bash
# Pythonの依存関係をインストール
pip install -r api/requirements.txt

# APIサーバーを起動
python -m api.main
```

#### ステップ 3: フロントエンドの起動

```bash
# JavaScript依存関係をインストール
npm install
# または
yarn install

# Webアプリを起動
npm run dev
# または
yarn dev
```

#### ステップ 4: DeepWiki を使用！

1. ブラウザで[http://localhost:3000](http://localhost:3000)を開く
2. GitHub、GitLab、または Bitbucket リポジトリを入力（例：`https://github.com/openai/codex`、`https://github.com/microsoft/autogen`、`https://gitlab.com/gitlab-org/gitlab`、または`https://bitbucket.org/redradish/atlassian_app_versions`）
3. プライベートリポジトリの場合は、「+ アクセストークンを追加」をクリックして GitHub または GitLab の個人アクセストークンを入力
4. 「Wiki を生成」をクリックして、魔法が起こるのを見守りましょう！

## 🔍 仕組み

DeepWiki は AI を使用して：

1. GitHub、GitLab、または Bitbucket リポジトリをクローンして分析（トークン認証によるプライベートリポジトリを含む）
2. スマート検索のためのコードの埋め込みを作成
3. コンテキスト対応 AI でドキュメントを生成（Google Gemini、OpenAI、OpenRouter、またはローカル Ollama モデルを使用）
4. コードの関係を説明する視覚的な図を作成
5. すべてを構造化された Wiki に整理
6. 質問機能を通じてリポジトリとのインテリジェントな Q&A を可能に
7. 詳細調査機能で深い研究能力を提供

```mermaid
graph TD
    A[ユーザーがGitHub/GitLab/Bitbucketリポジトリを入力] --> AA{プライベートリポジトリ?}
    AA -->|はい| AB[アクセストークンを追加]
    AA -->|いいえ| B[リポジトリをクローン]
    AB --> B
    B --> C[コード構造を分析]
    C --> D[コード埋め込みを作成]

    D --> M{モデルプロバイダーを選択}
    M -->|Google Gemini| E1[Geminiで生成]
    M -->|OpenAI| E2[OpenAIで生成]
    M -->|OpenRouter| E3[OpenRouterで生成]
    M -->|ローカルOllama| E4[Ollamaで生成]

    E1 --> E[ドキュメントを生成]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[視覚的な図を作成]
    E --> G[Wikiとして整理]
    F --> G
    G --> H[インタラクティブなDeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ プロジェクト構造

```
deepwiki/
├── api/ 

## File Path: README.kr.md

또는
yarn install

# 웹 앱 실행
npm run dev
# 또는
yarn dev
```

#### 4단계: DeepWiki 사용하기!

1. 브라우저에서 [http://localhost:3000](http://localhost:3000) 열기
2. GitHub, GitLab 또는 Bitbucket 저장소 입력 (예: `https://github.com/openai/codex`, `https://github.com/microsoft/autogen`, `https://gitlab.com/gitlab-org/gitlab`, `https://bitbucket.org/redradish/atlassian_app_versions`)
3. 비공개 저장소인 경우 "+ 액세스 토큰 추가" 클릭 후 GitHub 또는 GitLab 개인 액세스 토큰 입력
4. "Generate Wiki" 클릭 후 마법을 지켜보기!

## 🔍 작동 방식

DeepWiki는 AI를 사용하여 다음을 수행합니다:

1. GitHub, GitLab 또는 Bitbucket 저장소 복제 및 분석 (토큰 인증이 필요한 비공개 저장소 포함)
2. 스마트 검색을 위한 코드 임베딩 생성
3. 문맥 인지 AI로 문서 생성 (Google Gemini, OpenAI, OpenRouter 또는 로컬 Ollama 모델 사용)
4. 코드 관계를 설명하는 시각적 다이어그램 생성
5. 모든 것을 구조화된 위키로 정리
6. Ask 기능을 통한 저장소와의 지능형 Q&A 지원
7. DeepResearch로 심층 연구 기능 제공

```mermaid
graph TD
    A[사용자가 GitHub/GitLab/Bitbucket 저장소 입력] --> AA{비공개 저장소인가?}
    AA -->|예| AB[액세스 토큰 추가]
    AA -->|아니오| B[저장소 복제]
    AB --> B
    B --> C[코드 구조 분석]
    C --> D[코드 임베딩 생성]

    D --> M{모델 제공자 선택}
    M -->|Google Gemini| E1[Gemini로 생성]
    M -->|OpenAI| E2[OpenAI로 생성]
    M -->|OpenRouter| E3[OpenRouter로 생성]
    M -->|로컬 Ollama| E4[Ollama로 생성]

    E1 --> E[문서 생성]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[시각적 다이어그램 생성]
    E --> G[위키로 정리]
    F --> G
    G --> H[대화형 DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ 프로젝트 구조

```
deepwiki/
├── api/                  # 백엔드 API 서버
│   ├── main.py           # API 진입점
│   ├── api.py      

Interface](screenshots/Interface.png)
*DeepWiki의 메인 인터페이스*

![Private Repository Support](screenshots/privaterepo.png)
*개인 액세스 토큰으로 비공개 저장소 접근*

![DeepResearch Feature](screenshots/DeepResearch.png)
*DeepResearch는 복잡한 주제에 대해 다중 턴 조사를 수행*

### 데모 영상

[![DeepWiki Demo Video](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*DeepWiki 작동 영상 보기!*

## ❓ 문제 해결

### API 키 문제
- **"환경 변수 누락"**: `.env` 파일이 프로젝트 루트에 있고 필요한 API 키가 포함되어 있는지 확인
- **"API 키가 유효하지 않음"**: 키를 정확히 복사했는지, 공백이 없는지 확인
- **"OpenRouter API 오류"**: OpenRouter API 키가 유효하고 충분한 크레딧이 있는지 확인

### 연결 문제
- **"API 서버에 연결할 수 없음"**: API 서버가 포트 8001에서 실행 중인지 확인
- **"CORS 오류"**: API가 모든 출처를 허용하도록 설정되어 있지만 문제가 있으면 프론트엔드와 백엔드를 같은 머신에서 실행해 보세요

### 생성 문제
- **"위키 생성 오류"**: 아주 큰 저장소는 먼저 작은 저장소로 시도해 보세요
- **"잘못된 저장소 형식"**: 유효한 GitHub, GitLab 또는 Bitbucket URL 형식인지 확인
- **"저장소 구조를 가져올 수 없음"**: 비공개 저장소라면 적절한 권한의 개인 액세스 토큰을 입력했는지 확인
- **"다이어그램 렌더링 오류"**: 앱이 자동으로 다이어그램 오류를 수정하려 시도합니다

### 일반적인 해결법
1. **서버 둘 다 재시작**: 간단한 재시작으로 대부분 문제 해결
2. **콘솔 로그 확인**: 브라우저 개발자 도구에서 자바스크립트 오류 확인
3. **API 로그 확인**: API 실행 터미널에서 Python 오류 확인

## 🤝 기여

기여를 환영합니다! 다음을 자유롭게 해주세요:
- 버그나 기능 요청을 위한 이슈 열기
- 코드 개선을 위한 풀 리퀘스트 제출
- 피드백과 아이디어 공유

## 📄 라이선스

이 프로젝트는 MIT 라이선스 하에 있습니다 - 자세한 내용은 [LICENSE](LICENSE) 파일 참고.

## ⭐ 스타 히스토리

[![Star History Chart](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)



         # JavaScript 의존성
└── .env                  # 환경 변수 (직접 생성)
```

## 🛠️ 고급 설정

### 환경 변수

| 변수명 | 설명 | 필수 | 비고 |
|----------|-------------|----------|------|
| `GOOGLE_API_KEY` | AI 생성용 Google Gemini API 키 | 예 |
| `OPENAI_API_KEY` | 임베딩용 OpenAI API 키 | 예 |
| `OPENROUTER_API_KEY` | 대체 모델용 OpenRouter API 키 | 아니오 | OpenRouter 모델 사용 시 필요 |
| `PORT` | API 서버 포트 (기본값: 8001) | 아니오 | API와 프론트엔드를 같은 머신에서 호스팅 시 `SERVER_BASE_URL`의 포트도 변경 필요 |
| `SERVER_BASE_URL` | API 서버 기본 URL (기본값: http://localhost:8001) | 아니오 |

### 설정 파일

DeepWiki는 시스템의 다양한 측면을 관리하기 위해 JSON 설정 파일을 사용합니다:

1. **`generator.json`**: 텍스트 생성 모델 설정
   - 사용 가능한 모델 제공자(Google, OpenAI, OpenRouter, Ollama) 정의
   - 각 제공자의 기본 및 사용 가능한 모델 지정
   - temperature와 top_p 같은 모델별 매개변수 포함

2. **`embedder.json`**: 임베딩 모델 및 텍스트 처리 설정
   - 벡터 저장소용 임베딩 모델 정의
   - RAG를 위한 검색기 설정 포함
   - 문서 청킹을 위한 텍스트 분할기 설정 지정

3. **`repo.json`**: 저장소 처리 설정
   - 특정 파일 및 디렉토리를 제외하는 파일 필터 포함
   - 저장소 크기 제한 및 처리 규칙 정의

기본적으로 이러한 파일은 `api/config/` 디렉토리에 위치합니다. `DEEPWIKI_CONFIG_DIR` 환경 변수를 사용하여 위치를 사용자 정의할 수 있습니다.

### Docker 설정

Docker를 사용하여 DeepWiki를 실행할 수 있습니다:

```bash
# GitHub 컨테이너 레지스트리에서 이미지 가져오기
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# 환경 변수와 함께 컨테이너 실행
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

이 명령어는 또한 호스트의 `~/.adalflow`를 컨테이너의 `/root/.adalflow`에 마운트합니다. 이 경로는 다음을 저장하는 데 사용됩니다:
- 복제된 저장소 (`~/.adalflow/repos/`)
- 해당 저장소의 임베딩 및 인덱스 (`~/.adalflow/databases/`)
- 생성된 위키의 캐시 (`~/.adalflow/wikicache/`)

이를 통해 컨테이너가 중지되거나 제거되어도 데이터가 유지됩니다.

또는 제공된 `docker-compose.yml` 파일을 사용하세요:

```bash
# API 키가 포함된 .env 파일을 먼저 편집
docker-compose up
```

(`docker-compose.yml` 파일은 위의 `docker run` 명령어와 유사하게 데이터 지속성을 위해 `~/.adalflow`를 마운트하도록 미리 구성되어 있습니다.)

#### Docker에서 .env 파일 사용하기

.env 파일을 컨테이너에 마운트할 수도 있습니다:

```bash
# API 키가 포함된 .env 파일 

## File Path: api/data_pipeline.py

                 "file_path": relative_path,
                            "type": ext[1:],
                            "is_code": False,
                            "is_implementation": False,
                            "title": relative_path,
                            "token_count": token_count,
                        },
                    )
                    documents.append(doc)
            except Exception as e:
                logger.error(f"Error reading {file_path}: {e}")

    logger.info(f"Found {len(documents)} documents")
    return documents

def prepare_data_pipeline(is_ollama_embedder: bool = None):
    """
    Creates and returns the data transformation pipeline.

    Args:
        is_ollama_embedder (bool, optional): Whether to use Ollama for embedding.
                                        

## File Path: test/__init__.py

# Test package for deepwiki-open data pipeline


## File Path: README.es.md

las iteraciones

Para usar Investigación Profunda, simplemente activa el interruptor "Investigación Profunda" en la interfaz de Preguntas antes de enviar tu pregunta.

## 📱 Capturas de Pantalla

![Interfaz Principal de DeepWiki](screenshots/Interface.png)
*La interfaz principal de DeepWiki*

![Soporte para Repositorios Privados](screenshots/privaterepo.png)
*Acceso a repositorios privados con tokens de acceso personal*

![Función de Investigación Profunda](screenshots/DeepResearch.png)
*Investigación Profunda realiza investigaciones de múltiples turnos para temas complejos*

### Video de Demostración

[![Video de Demostración de DeepWiki](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*¡Mira DeepWiki en acción!*

## ❓ Solución de Problemas

### Problemas con Claves API
- **"Faltan variables de entorno"**: Asegúrate de que tu archivo `.env` esté en la raíz del proyecto y contenga las claves API requeridas
- **"Clave API no válida"**: Verifica que hayas copiado la clave completa correctamente sin espacios adicionales
- **"Error de API OpenRouter"**: Verifica que tu clave API de OpenRouter sea válida y tenga créditos suficientes

### Problemas de Conexión
- **"No se puede conectar al servidor API"**: Asegúrate de que el servidor API esté ejecutándose en el puerto 8001
- **"Error CORS"**: La API está configurada para permitir todos los orígenes, pero si tienes problemas, intenta ejecutar tanto el frontend como el backend en la misma máquina

### Problemas de Generación
- **"Error al generar wiki"**: Para repositorios muy grandes, prueba primero con uno más pequeño
- **"Formato de repositorio no válido"**: Asegúrate de usar un formato de URL válido para GitHub, GitLab o Bitbucket
- **"No se pudo obtener la estructura del repositorio"**: Para repositorios privados, asegúrate de haber ingresado un token de acceso personal válido con los permisos apropiados
- **"Error de renderizado de diagrama"**: La aplicación intentará automáticamente arreglar los diagramas rotos

### Soluciones Comunes
1. **Reiniciar ambos servidores**: A veces un simple reinicio soluciona la mayoría de los problemas
2. **Revisar los registros de la consola**: Abre las herramientas de desarrollo del navegador para ver cualquier error de JavaScript
3. **Revisar los registros de la API**: Mira la terminal donde se ejecuta la API para ver errores de Python

## 🤝 Contribuir

¡Las contribuciones son bienvenidas! Siéntete libre de:
- Abrir issues para bugs o solicitudes de funciones
- Enviar pull requests para mejorar el código
- Compartir tus comentarios e ideas

## 📄 Licencia

Este proyecto está licenciado bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para más detalles.

## ⭐ Historial 

el de `api/config/embedder_openai_compatible.json`.
2. En el archivo `.env` de la raíz del proyecto, configura las variables de entorno necesarias, por ejemplo:
   ```
   OPENAI_API_KEY=tu_api_key
   OPENAI_BASE_URL=tu_endpoint_compatible_openai
   ```
3. El programa sustituirá automáticamente los placeholders de embedder.json por los valores de tus variables de entorno.

Así puedes cambiar fácilmente a cualquier servicio de embedding compatible con OpenAI sin modificar el código.

## 🤖 Funciones de Preguntas e Investigación Profunda

### Función de Preguntas

La función de Preguntas te permite chatear con tu repositorio usando Generación Aumentada por Recuperación (RAG):

- **Respuestas Conscientes del Contexto**: Obtén respuestas precisas basadas en el código real de tu repositorio
- **Potenciada por RAG**: El sistema recupera fragmentos de código relevantes para proporcionar respuestas fundamentadas
- **Transmisión en Tiempo Real**: Ve las respuestas mientras se generan para una experiencia más interactiva
- **Historial de Conversación**: El sistema mantiene el contexto entre preguntas para interacciones más coherentes

### Función de Investigación Profunda

Investigación Profunda lleva el análisis de repositorios al siguiente nivel con un proceso de investigación de múltiples turnos:

- **Investigación en Profundidad**: Explora a fondo temas complejos a través de múltiples iteraciones de investigación
- **Proceso Estructurado**: Sigue un plan de investigación claro con actualizaciones y una conclusión completa
- **Continuación Automática**: La IA continúa automáticamente la investigación hasta llegar a una conclusión (hasta 5 iteraciones)
- **Etapas de Investigación**:
  1. **Plan de Investigación**: Describe el enfoque y los hallazgos iniciales
  2. **Actualizaciones de Investigación**: Desarrolla las iteraciones anteriores con nuevas perspectivas
  3. **Conclusión Final**: Proporciona una respuesta completa basada en todas las iteraciones

Para usar Investigación Profunda, simplemente activa el interruptor "Investigación Profunda" en la interfaz de Preguntas antes de enviar tu pregunta.

## 📱 Capturas de Pantalla

![Interfaz Principal de DeepWiki](screenshots/Interface.png)
*La interfaz principal de DeepWiki*

![Soporte para Repositorios Privados](screenshots/privaterepo.png)
*Acceso a repositorios privados con tokens de acceso personal*

![Función de Investigación Profunda](screenshots/DeepResearch.png)
*Investigación Profunda realiza investigaciones de múltiples turnos para temas complejos*

### Video de Demostración

[![Video de Demostración de DeepWiki](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*¡Mira DeepWiki en acción!*

## ❓ Solución de Problemas

### Problemas con Claves API
- **"Faltan variables de entorno"**: Asegúrate de que tu archivo `.env` esté en la raíz del proyecto y contenga las claves API requeridas
- **"Clave API no válida"**: Verifica 

Modelos Soportados

- **Google**: Predeterminado `gemini-2.0-flash`, también soporta `gemini-1.5-flash`, `gemini-1.0-pro`, etc.
- **OpenAI**: Predeterminado `gpt-4o`, también soporta `o4-mini`, etc.
- **OpenRouter**: Acceso a múltiples modelos a través de una API unificada, incluyendo Claude, Llama, Mistral, etc.
- **Ollama**: Soporte para modelos de código abierto ejecutados localmente como `llama3`

### Variables de Entorno

Cada proveedor requiere sus correspondientes variables de entorno para las claves API:

```
# Claves API
GOOGLE_API_KEY=tu_clave_api_google        # Requerida para modelos Google Gemini
OPENAI_API_KEY=tu_clave_api_openai        # Requerida para modelos OpenAI
OPENROUTER_API_KEY=tu_clave_api_openrouter # Requerida para modelos OpenRouter

# Configuración de URL Base de OpenAI API
OPENAI_BASE_URL=https://punto-final-personalizado.com/v1  # Opcional, para endpoints personalizados de OpenAI API

# Directorio de Configuración
DEEPWIKI_CONFIG_DIR=/ruta/a/directorio/config/personalizado  # Opcional, para ubicación personalizada de archivos de configuración
```

### Archivos de Configuración

DeepWiki utiliza archivos de configuración JSON para gestionar varios aspectos del sistema:

1. **`generator.json`**: Configuración para modelos de generación de texto
   - Define los proveedores de modelos disponibles (Google, OpenAI, OpenRouter, Ollama)
   - Especifica los modelos predeterminados y disponibles para cada proveedor
   - Contiene parámetros específicos de los modelos como temperatura y top_p

2. **`embedder.json`**: Configuración para modelos de embeddings y procesamiento de texto
   - Define modelos de embeddings para almacenamiento vectorial
   - Contiene configuración del recuperador para RAG
   - Especifica ajustes del divisor de texto para fragmentación de documentos

3. **`repo.json`**: Configuración para manejo de repositorios
   - Contiene filtros de archivos para excluir ciertos archivos y directorios
   - Define límites de tamaño de repositorio y reglas de procesamiento

Por defecto, estos archivos se encuentran en el directorio `api/config/`. Puedes personalizar su ubicación usando la variable de entorno `DEEPWIKI_CONFIG_DIR`.

### Selección de Modelos Personalizados para Proveedores de Servicios

La función de selección de modelos personalizados está diseñada específicamente para proveedores de servicios que necesitan:

- Puede ofrecer a los usuarios dentro de su organización una selección de diferentes modelos de IA
- Puede adaptarse rápidamente al panorama de LLM en rápida evolución sin cambios de código
- Puede soportar modelos especializados o ajustados que no están en la lista predefinida

Usted puede implementar sus ofertas de modelos seleccionando entre las opciones predefinidas o 

## File Path: README.zh.md

**RAG驱动**：系统检索相关代码片段，提供有根据的响应
- **实时流式传输**：实时查看生成的响应，获得更交互式的体验
- **对话历史**：系统在问题之间保持上下文，实现更连贯的交互

### 深度研究功能

深度研究通过多轮研究过程将仓库分析提升到新水平：

- **深入调查**：通过多次研究迭代彻底探索复杂主题
- **结构化过程**：遵循清晰的研究计划，包含更新和全面结论
- **自动继续**：AI自动继续研究直到达成结论（最多5次迭代）
- **研究阶段**：
  1. **研究计划**：概述方法和初步发现
  2. **研究更新**：在前一轮迭代基础上增加新见解
  3. **最终结论**：基于所有迭代提供全面答案

要使用深度研究，只需在提交问题前在提问界面中切换"深度研究"开关。

## 📱 截图

![DeepWiki主界面](screenshots/Interface.png)
*DeepWiki的主界面*

![私有仓库支持](screenshots/privaterepo.png)
*使用个人访问令牌访问私有仓库*

![深度研究功能](screenshots/DeepResearch.png)
*深度研究为复杂主题进行多轮调查*

### 演示视频

[![DeepWiki演示视频](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*观看DeepWiki实际操作！*

## ❓ 故障排除

### API密钥问题
- **"缺少环境变量"**：确保您的`.env`文件位于项目根目录并包含所需的API密钥
- **"API密钥无效"**：检查您是否正确复制了完整密钥，没有多余空格
- **"OpenRouter API错误"**：验证您的OpenRouter API密钥有效且有足够的额度

### 连接问题
- **"无法连接到API服务器"**：确保API服务器在端口8001上运行
- **"CORS错误"**：API配置为允许所有来源，但如果您遇到问题，请尝试在同一台机器上运行前端和后端

### 生成问题
- **"生成Wiki时出错"**：对于非常大的仓库，请先尝试较小的仓库
- **"无效的仓库格式"**：确保您使用有效的GitHub、GitLab或Bitbucket URL格式
- **"无法获取仓库结构"**：对于私有仓库，确保您输入了具有适当权限的有效个人访问令牌
- **"图表渲染错误"**：应用程序将自动尝试修复损坏的图表

### 常见解决方案
1. **重启两个服务器**：有时简单的重启可以解决大多数问题
2. **检查控制台日志**：打开浏览器开发者工具查看任何JavaScript错误
3. **检查API日志**：查看运行API的终端中的Python错误

## 🤝 贡献

欢迎贡献！随时：
- 为bug或功能请求开issue
- 提交pull request改进代码
- 分享您的反馈和想法

## 📄 许可证

本项目根据MIT许可证授权 - 详情请参阅[LICENSE](LICENSE)文件。

## ⭐ 星标历史

[![星标历史图表](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)

## 🤖 基于提供者的模型选择系统

DeepWiki 现在实现了灵活的基于提供者的模型选择系统，支持多种 LLM 提供商：

### 支持的提供商和模型

- **Google**: 默认使用 `gemini-2.0-flash`，还支持 `gemini-1.5-flash`、`gemini-1.0-pro` 等
- **OpenAI**: 默认使用 `gpt-4o`，还支持 `o4-mini` 等
- **OpenRouter**: 通过统一 API 访问多种模型，包括 Claude、Llama、Mistral 等
- **Ollama**: 支持本地运行的开源模型，如 `llama3`

### 环境变量

每个提供商需要相应的 API 密钥环境变量：

```
# API 密钥
GOOGLE_API_KEY=你的谷歌API密钥        # 使用 Google Gemini 模型必需
OPENAI_API_KEY=你的OpenAI密钥        # 使用 OpenAI 模型必需
OPENROUTER_API_KEY=你的OpenRouter密钥 # 使用 OpenRouter 模型必需

# OpenAI API 基础 URL 配置
OPENAI_BASE_URL=https://自定义API端点.com/v1  # 可选，用于自定义 OpenAI API 端点
```

### 为服务提供者设计的自定义模型选择

自定义模型选择功能专为需要以下功能的服务提供者设计：

- 您可在您的组织内部为用户提供多种 AI 模型选择
- 您无需代码更改即可快速适应快速发展的 LLM 领域
- 您可支持预定义列表中没有的专业或微调模型

使用者可以通过从服务提供者预定义选项中选择或在前端界面中输入自定义模型标识符来实现其模型产品。

### 为企业私有渠道设计的基础 URL 配置

OpenAI 客户端的 base_url 配置主要为拥有私有 API 渠道的企业用户设计。此功能：

- 支持连接到私有或企业特定的 API 端点
- 允许组织使用自己的自托管或自定义部署的 LLM 服务
- 支持与第三方 OpenAI API 兼容服务的集成

**即将推出**：在未来的更新中，DeepWiki 将支持一种模式，用户需要在请求中提供自己的 API 密钥。这将允许拥有私有渠道的企业客户使用其现有的 API 安排，而不是与 DeepWiki 部署共享凭据。

### 环境变量

每个提供商需要其相应的API密钥环境变量：

```
# API密钥
GOOGLE_API_KEY=your_google_api_key        # Google Gemini模型必需
OPENAI_API_KEY=your_openai_api_key        # OpenAI模型必需
OPENROUTER_API_KEY=your_openrouter_api_key # OpenRouter模型必需

# OpenAI API基础URL配置
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # 可选，用于自定义OpenAI API端点

# 配置目录
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # 可选，用于自定义配置文件位置

# 授权模式
DEEPWIKI_AUTH_MODE=true  # 设置为 true 或 1 以启用授权模式
DEEPWIKI_AUTH_CODE=your_secret_code # 当 DEEPWIKI_AUTH_MODE 启用时所需的授权码
```
如果不使用ollama模式，那么需要配置OpenAI API密钥用于embeddings。其他密钥只有配置并使用使用对应提供商的模型时才需要。

## 授权模式

DeepWiki 可以配置为在授权模式下运行，在该模式下，生成 Wiki 需要有效的授权码。如果您想控制谁可以使用生成功能，这将非常有用。
限制使用前端页面生成wiki并保护已生成页面的缓存删除，但无法完全阻止直接访问 API 端点生成wiki。主要目的是为了保护管理员已生成的wiki页面，防止被访问者重新生成。

要启用授权模式，请设置以下环境变量：

- `DEEPWIKI_AUTH_MODE`: 将此设置为 `true` 或 `1`。启用后，前端将显示一个用于输入授权码的字段。
- `DEEPWIKI_AUTH_CODE`: 将此设置为所需的密钥。限制使用前端页面生成wiki并保护已生成页面的缓存删除，但无法完全阻止直接访问 API 端点生成wiki。

如果未设置 `DEEPWIKI_AUTH_MODE` 或将其设置为 `false`（或除 `true`/`1` 之外的任何其他值），则授权功能将被禁用，并且不需要任何代码。

### 配置文件

DeepWiki使用JSON配置文件管理系统的各个方面：

1. **`generator.json`**：文本生成模型配置
   - 定义可用的模型提供商（Google、OpenAI、OpenRouter、Ollama）
   - 指定每个提供商的默认和可用模型
   - 包含特定模型的参数，如temperature和top_p

2. **`embedder.json`**：嵌入模型和文本处理配置
   - 定义用于向量存储的嵌入模型
   - 包含用于RAG的检索器配置
   - 指定文档分块的文本分割器设置

3. **`repo.json`**：仓库处理配置
   - 包含排除特定文件和目录的文件过滤器
   - 定义仓库大小限制和处理规则

默认情况下，这些文件位于`api/config/`目录中。您可以使用`DEEPWIKI_CONFIG_DIR`环境变量自定义它们的位置。

### 面向服务提供商的自定义模型选择

自定义模型选择功能专为需要以下功能的服务提供者设计：

- 您可在您的组织内部为用户提供多种 AI 模型选择
- 您无需代码更改即可快速适应快速发展的 LLM 领域
- 您可支持预定义列表中没有的专业或微调模型

使用者可以通过从服务提供者预定义选项中选择或在前端界面中输入自定义模型标识符来实现其模型产品。

### 为企业私有渠道设计的基础 URL 配置

OpenAI 客户端的 base_url 配置主要为拥有私有 API 渠道的企业用户设计。此功能：

- 支持连接到私有或企业特定的 API 端点
- 允许组织使用自己的自托管或自定义部署的 LLM 服务
- 支持与第三方 OpenAI API 兼容服务的集成

**即将推出**：在未来的更新中，DeepWiki 将支持一种模式，用户需要在请求中提供自己的 API 密钥。这将允许拥有私有渠道的企业客户使用其现有的 API 安排，而不是与 DeepWiki 部署共享凭据。

## 🧩 使用 OpenAI 兼容的 Embedding 模型（如阿里巴巴 Qwen）

如果你希望使用 OpenAI 以外、但兼容 OpenAI 接口的 embedding 模型（如阿里巴巴 Qwen），请参考以下步骤：

1. 用 `api/config/embedder_openai_compatible.json` 的内容替换 `api/config/embedder.json`。
2. 在项目根目录的 `.env` 文件中，配置相应的环境变量，例如：
   ```
   OPENAI_API_KEY=你的_api_key
   OPENAI_BASE_URL=你的_openai_兼容接口地址
   ```
3. 程序会自动用环境变量的值替换 embedder.json 里的占位符。

这样即可无缝切换到 OpenAI 兼容的 embedding 服务，无需修改代码。



# DeepWiki-Open

![DeepWiki 横幅](screenshots/Deepwiki.png)

**DeepWiki**可以为任何GitHub、GitLab或BitBucket代码仓库自动创建美观、交互式的Wiki！只需输入仓库名称，DeepWiki将：

1. 分析代码结构
2. 生成全面的文档
3. 创建可视化图表解释一切如何运作
4. 将所有内容整理成易于导航的Wiki

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ 特点

- **即时文档**：几秒钟内将任何GitHub、GitLab或BitBucket仓库转换为Wiki
- **私有仓库支持**：使用个人访问令牌安全访问私有仓库
- **智能分析**：AI驱动的代码结构和关系理解
- **精美图表**：自动生成Mermaid图表可视化架构和数据流
- **简易导航**：简单、直观的界面探索Wiki
- **提问功能**：使用RAG驱动的AI与您的仓库聊天，获取准确答案
- **深度研究**：多轮研究过程，彻底调查复杂主题
- **多模型提供商**：支持Google Gemini、OpenAI、OpenRouter和本地Ollama模型

## 🚀 快速开始（超级简单！）

### 选项1：使用Docker

```bash
# 克隆仓库
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# 创建包含API密钥的.env文件
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# 可选：如果您想使用OpenRouter模型，添加OpenRouter API密钥
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# 使用Docker Compose运行
docker-compose up
```

(上述 Docker 命令以及 `docker-compose.yml` 配置会挂载您主机上的 `~/.adalflow` 目录到容器内的 `/root/.adalflow`。此路径用于存储：
- 克隆的仓库 (`~/.adalflow/repos/`)
- 仓库的嵌入和索引 (`~/.adalflow/databases/`)
- 缓存的已生成 Wiki 内容 (`~/.adalflow/wikicache/`)

这确保了即使容器停止或移除，您的数据也能持久保存。)

> 💡 **获取这些密钥的地方：**
> - 从[Google AI Studio](https://makersuite.google.com/app/apikey)获取Google API密钥
> - 从[OpenAI Platform](https://platform.openai.com/api-keys)获取OpenAI API密钥

### 选项2：手动设置（推荐）

#### 步骤1：设置API密钥

在项目根目录创建一个`.env`文件，包含以下密钥：

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# 可选：如果您想使用OpenRouter模型，添加此项
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### 步骤2：启动后端

```bash
# 安装Python依赖
pip install -r api/requirements.txt

# 启动API服务器
python -m api.main
```

#### 步骤3：启动前端

```bash
# 安装JavaScript依赖
npm install
# 或
yarn install

# 启动Web应用
npm run dev
# 或
yarn dev
```

#### 步骤4：使用DeepWiki！

1. 在浏览器中打开[http://localhost:3000](http://localhost:3000)
2. 输入GitHub、GitLab或Bitbucket仓库（如`https://github.com/openai/codex`、`https://github.com/microsoft/autogen`、`https://gitlab.com/gitlab-org/gitlab`或`https://bitbucket.org/redradish/atlassian_app_versions`）
3. 对于私有仓库，点击"+ 添加访问令牌"并输入您的GitHub或GitLab个人访问令牌
4. 点击"生成Wiki"，见证奇迹的发生！

## 🔍 工作原理

DeepWiki使用AI来：

1. 克隆并分析GitHub、GitLab或Bitbucket仓库（包括使用令牌认证的私有仓库）
2. 创建代码嵌入用于智能检索
3. 使用上下文感知AI生成文档（使用Google Gemini、OpenAI、OpenRouter或本地Ollama模型）
4. 创建可视化图表解释代码关系
5. 将所有内容组织成结构化Wiki
6. 通过提问功能实现与仓库的智能问答
7. 通过深度研究功能提供深入研究能力

```mermaid
graph TD
    A[用户输入GitHub/GitLab/Bitbucket仓库] --> AA{私有仓库?}
    AA -->|是| AB[添加访问令牌]
    AA -->|否| B[克隆仓库]
    AB --> B
    B --> C[分析代码结构]
    C --> D[创建代码嵌入]

    D --> M{选择模型提供商}
    M -->|Google Gemini| E1[使用Gemini生成]
    M -->|OpenAI| E2[使用OpenAI生成]
    M -->|OpenRouter| E3[使用OpenRouter生成]
    M -->|本地Ollama| E4[使用Ollama生成]

    E1 --> E[生成文档]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[创建可视化图表]
    E --> G[组织为Wiki]
    F --> G
    G --> H[交互式DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ 项目结构

```
deepwiki/
├── api/                  # 后端API服务器
│   ├── main.py           # API入口点
│   ├── api.py            # FastAPI实现
│   ├── rag.py        

## File Path: README.vi.md

# DeepWiki-Open

![DeepWiki Banner](screenshots/Deepwiki.png)

**Open DeepWiki** là 1 triển khai thay thế cho DeepWiki, tự động tạo ra các trang wiki cho bất kỳ Repository  nào trên GitHub, GitLab hoặc BitBucket! Chỉ cần nhập đường dẫn Repository, và DeepWiki sẽ:

1. Phân tích cấu trúc mã nguồn
2. Tạo tài liệu đầy đủ và chi tiết
3. Tạo sơ đồ trực quan để giải thích cách mọi thứ hoạt động
4. Sắp xếp tất cả documents thành một wiki dễ hiểu

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ Tính năng

- **Tạo Tài liệu tức thì**: Biến bất kỳ Repository GitHub, GitLab hoặc BitBucket nào thành wiki chỉ trong vài giây
- **Hỗ trợ Private Repository**: Truy cập Private Repository một cách an toàn với personal access tokens
- **Phân tích thông minh**: Hiểu cấu trúc và mối quan hệ của source codes nhờ AI
- **Tự động tạo Sơ đồ**: Tự động tạo sơ đồ Mermaid để trực quan hóa kiến trúc và luồng dữ liệu
- **Dễ dàng thao tác**:Giao diện wiki đơn giản, trực quan để khám phá
- **Trò chuyện với repository**: Trò chuyện với repo của bạn bằng AI (tích hợp RAG) để nhận câu trả lời chính xác
- **DeepResearch**:Quy trình Deep Research nhiều bước giúp phân tích kỹ lưỡng các chủ đề phức tạp
- **Hỗ trợ nhiều mô hình**: Hỗ trợ Google Gemini, OpenAI, OpenRouter, và  local Ollama models

## 🚀 Bắt đầu (Siêu dễ :))

### Option 1: Sử dụng Docker

```bash
# Clone repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Tạo .env file với API keys
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# Optional: Thêm OpenRouter API key nếu bạn muốn OpenRouter models
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# Run với Docker Compose
docker-compose up
```

> 💡 **Hướng dẫn lấy Keys**
> - Lấy Google API key từ [Google AI Studio](https://makersuite.google.com/app/apikey)
> - Lấy OpenAI API key từ [OpenAI Platform](https://platform.openai.com/api-keys)

### Option 2: Setup thủ công (Khuyên dùng)

#### Bước 1: Set Up API Keys

Tạo  `.env` file trong thư mục gốc của project với những keys vừa tạo:

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Thêm OpenRouter API key nếu bạn muốn OpenRouter models
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### Bước 2: Bắt đầu với Backend

```bash
# Cài đặt Python 

cung cấp LLM:

### Các nhà cung cấp và mô hình được hỗ trợ

- **Google**: Mặc định là `gemini-2.0-flash`, cũng hỗ trợ `gemini-1.5-flash`, `gemini-1.0-pro`, v.v.
- **OpenAI**: Mặc định là `gpt-4o`, cũng hỗ trợ `o4-mini`, v.v.
- **OpenRouter**: Truy cập nhiều mô hình qua một API thống nhất, bao gồm Claude, Llama, Mistral, v.v.
- **Ollama**: Hỗ trợ các mô hình mã nguồn mở chạy cục bộ như `llama3`

### Biến môi trường

Mỗi nhà cung cấp yêu cầu các biến môi trường API key tương ứng:

```
# API Keys
GOOGLE_API_KEY=google_api_key_của_bạn        # Bắt buộc cho các mô hình Google Gemini
OPENAI_API_KEY=openai_key_của_bạn            # Bắt buộc cho các mô hình OpenAI
OPENROUTER_API_KEY=openrouter_key_của_bạn    # Bắt buộc cho các mô hình OpenRouter

# Cấu hình URL cơ sở cho OpenAI API
OPENAI_BASE_URL=https://endpoint-tùy-chỉnh.com/v1  # Tùy chọn, cho các điểm cuối API OpenAI tùy chỉnh

# Thư mục cấu hình
DEEPWIKI_CONFIG_DIR=/đường/dẫn/đến/thư_mục/cấu_hình  # Tùy chọn, cho vị trí tệp cấu hình tùy chỉnh
```

### Tệp cấu hình

DeepWiki sử dụng các tệp cấu hình JSON để quản lý các khía cạnh khác nhau của hệ thống:

1. **`generator.json`**: Cấu hình cho các mô hình tạo văn bản
   - Xác định các nhà cung cấp mô hình có sẵn (Google, OpenAI, OpenRouter, Ollama)
   - Chỉ định các mô hình mặc định và có sẵn cho mỗi nhà cung cấp
   - Chứa các tham số đặc thù cho mô hình như temperature và top_p

2. **`embedder.json`**: Cấu hình cho mô hình embedding và xử lý văn bản
   - Xác định mô hình embedding cho lưu trữ vector
   - Chứa cấu hình bộ truy xuất cho RAG
   - Chỉ định cài đặt trình chia văn bản để phân đoạn tài liệu

3. **`repo.json`**: Cấu hình xử lý repository
   - Chứa bộ lọc tệp để loại trừ một số tệp và thư mục nhất định
   - Xác định giới hạn kích thước repository và quy tắc xử lý

Mặc định, các tệp này nằm trong thư mục `api/config/`. Bạn có thể tùy 

## File Path: api/websocket_wiki.py

examining the {repo_type} repository: {repo_url} ({repo_name}).
You are conducting a multi-turn Deep Research process to thoroughly investigate the specific topic in the user's query.
Your goal is to provide detailed, focused information EXCLUSIVELY about this topic.
IMPORTANT:You MUST respond in {language_name} language.
</role>

<guidelines>
- This is the first iteration of a multi-turn research process focused EXCLUSIVELY on the user's query
- Start your response with "## Research Plan"
- Outline your approach to investigating this specific topic
- If the topic is about a specific file or feature (like "Dockerfile"), focus ONLY on that file or feature
- Clearly state the specific topic you're researching to maintain focus throughout all iterations
- Identify the key aspects you'll need to research
- Provide initial findings based on the information available
- End with "## Next Steps" indicating what you'll investigate in the next iteration
- Do NOT provide a final conclusion yet - this is just the beginning of the research
- Do NOT include general repository information unless directly relevant to the query
- Focus EXCLUSIVELY on the specific topic being researched - do not drift to related topics
- Your research MUST directly address the original question
- NEVER respond with just "Continue the research" as an answer - always provide substantive research findings
- Remember that this topic will be maintained across all research iterations
</guidelines>

<style>
- Be concise but thorough
- Use markdown formatting to improve readability
- Cite specific files and code sections when relevant
</style>"""
            elif is_final_iteration:
                system_prompt = f"""<role>
You are an expert code analyst examining the {repo_type} repository: {repo_url} ({repo_name}).
You are in the final iteration of a Deep Research process focused EXCLUSIVELY on the latest user query.
Your goal is to synthesize all previous findings and provide a comprehensive conclusion that directly addresses this specific topic and ONLY this topic.
IMPORTANT:You MUST respond in {language_name} language.
</role>

<guidelines>
- This is the final iteration of the research process
- CAREFULLY review the entire conversation history to understand all previous findings
- Synthesize ALL findings from previous iterations into a comprehensive conclusion
- Start with "## Final Conclusion"
- Your conclusion 

## File Path: README.pt-br.md

A[Usuário insere repo GitHub/GitLab/Bitbucket] --> AA{Repo privado?}
    AA -->|Sim| AB[Adicionar token de acesso]
    AA -->|Não| B[Clonar Repositório]
    AB --> B
    B --> C[Analisar Estrutura do Código]
    C --> D[Criar Embeddings do Código]

    D --> M{Selecionar Provedor de Modelo}
    M -->|Google Gemini| E1[Gerar com Gemini]
    M -->|OpenAI| E2[Gerar com OpenAI]
    M -->|OpenRouter| E3[Gerar com OpenRouter]
    M -->|Ollama Local| E4[Gerar com Ollama]

    E1 --> E[Gerar Documentação]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[Criar Diagramas Visuais]
    E --> G[Organizar como Wiki]
    F --> G
    G --> H[DeepWiki Interativo]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ Estrutura do Projeto

```
deepwiki/
├── api/                  # Servidor API backend
│   ├── main.py           # Ponto de entrada da API
│   ├── api.py            # Implementação FastAPI
│   ├── rag.py            # Retrieval Augmented Generation
│   ├── data_pipeline.py  # Utilitários de processamento de dados
│   └── requirements.txt  # Dependências Python
│
├── src/                  # Aplicativo Next.js frontend
│   ├── app/              # Diretório do aplicativo Next.js
│   │   └── page.tsx      
<END_OF_CONTEXT>

<query>
分析这个项目的向量嵌入的实现方案和工作流程，涉及到的相关源文件
</query>

Assistant: 
2025-07-10 02:19:39,067 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 02:19:39,097 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 02:19:42,551 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:20:07,495 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
该项目的向量嵌入实现涉及以下几个核心组件和文件：

1. **配置文件**:
   - `api/config/embedder.json` 定义了用于向量存储的嵌入模型和相关参数。  
     Sources: [api/config/embedder.json]()

2. **嵌入模型初始化**:
   - 在 `api/data_pipeline.py` 中，通过 `prepare_data_pipeline` 函数初始化嵌入模型。该函数根据配置文件加载模型，并设置是否使用 Ollama 嵌入器。
     ```python
     def prepare_data_pipeline(is_ollama_embedder: bool = None):
         """
         Creates and returns the data transformation pipeline.
         ...
         """
     ```
     Sources: [api/data_pipeline.py:1-20]()

3. **文档处理与嵌入生成**:
   - `api/data_pipeline.py` 中的 `process_documents` 函数负责读取文档并生成嵌入。它遍历所有文件，提取内容并计算 token 数量，同时为每个文档创建嵌入。
     ```python
     def process_documents(repo_path: str, file_exts: List[str]):
         ...
         for file_path in all_files:
             try:
                 with open(file_path, 'r', encoding='utf-8') as f:
                     content = f.read()
                 token_count = count_tokens(content)
                 ...
                 documents.append(doc)
             except Exception as e:
                 logger.error(f"Error reading {file_path}: {e}")
     ```
     Sources: [api/data_pipeline.py:100-150]()

4. **嵌入模型调用**:
   - 在 `api/rag.py` 中，通过 `generate_embedding` 函数调用嵌入模型对文本进行编码。该函数接收文本并返回对应的向量表示。
     ```python
     def generate_embedding(text: str) -> List[float]:
         ...
     ```
     Sources: [api/rag.py:10-30]()

5. **向量存储与检索**:
   - 向量存储的具体实现未在源码中详细展示，但可以推测是通过某种数据库（如 FAISS 或 ChromaDB）进行存储。`api/rag.py` 中的 `retrieve` 函数可能负责从存储中检索相似的嵌入向量。
     ```python
     def retrieve(query_embedding: List[float], top_k: int = 5) -> List[str]:
         ...
     ```
     Sources: [api/rag.py:40-60]()

6. **模型选择与环境配置**:
   - 在 `README.zh.md` 和 `README.es.md` 中描述了支持的嵌入模型提供商（如 OpenAI、Google、Ollama）以及相关的环境变量配置。
     ```env
     OPENAI_API_KEY=your_openai_api_key
     OPENAI_BASE_URL=https://custom-api-endpoint.com/v1
     ```
     Sources: [README.zh.md:100-120](), [README.es.md:100-120]()

### 工作流程总结
1. 从配置文件中加载嵌入模型设置。
2. 使用 `data_pipeline.py` 处理文档并生成嵌入向量。
3. 将生成的嵌入向量存储到向量数据库中。
4. 在 RAG 流程中，使用 `generate_embedding` 对查询文本进行编码，并通过 `retrieve` 检索最相关的文档嵌入。
5. 根据检索结果生成回答。

该项目通过模块化设计实现了向量嵌入的完整流程，支持多种模型提供商，并提供了灵活的配置选项。
2025-07-10 02:29:42,859 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/generator.json
2025-07-10 02:29:42,860 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/embedder.json
2025-07-10 02:29:42,860 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/repo.json
2025-07-10 02:29:42,860 - INFO - api.config - config.py:99 - Loading configuration from /app/api/config/lang.json
2025-07-10 02:29:43,362 - INFO - faiss.loader - loader.py:125 - Loading faiss with AVX2 support.
2025-07-10 02:29:43,383 - INFO - faiss.loader - loader.py:127 - Successfully loaded faiss with AVX2 support.
2025-07-10 02:29:43,387 - INFO - faiss - __init__.py:174 - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-07-10 02:29:43,407 - INFO - __main__ - main.py:42 - Starting Streaming API on port 8001
2025-07-10 02:29:45,425 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:29:45,428 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:29:45,502 - INFO - api.api - api.py:591 - Scanning for project cache files in: /root/.adalflow/wikicache
2025-07-10 02:29:45,504 - INFO - api.api - api.py:629 - Found 5 processed project entries.
2025-07-10 02:29:47,689 - INFO - api.api - api.py:179 - Fetching model configurations
2025-07-10 02:29:47,707 - INFO - api.api - api.py:476 - Attempting to retrieve wiki cache for AsyncFuncAI/deepwiki-open (github), lang: zh
2025-07-10 02:29:55,481 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 31 tokens
2025-07-10 02:29:55,513 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:29:55,514 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:29:55,515 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:29:55,515 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:29:55,557 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:29:55,565 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['input_str', 'conversation_history', 'contexts', 'output_format_str', 'system_prompt']
2025-07-10 02:29:55,565 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 02:29:55,565 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/AsyncFuncAI/deepwiki-open...
2025-07-10 02:29:55,565 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: AsyncFuncAI_deepwiki-open
2025-07-10 02:29:55,565 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/AsyncFuncAI_deepwiki-open. Using existing repository.
2025-07-10 02:29:55,566 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/AsyncFuncAI_deepwiki-open', 'save_db_file': '/root/.adalflow/databases/AsyncFuncAI_deepwiki-open.pkl'}
2025-07-10 02:29:55,566 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 02:29:55,588 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 02:29:55,588 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:29:55,588 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:29:55,588 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:29:55,706 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:29:55,706 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:29:55,706 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:29:55,779 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 821 documents from existing database
2025-07-10 02:29:55,780 - INFO - api.rag - rag.py:421 - Loaded 821 documents for retrieval
2025-07-10 02:29:55,780 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 821 documents)
2025-07-10 02:29:55,781 - INFO - api.rag - rag.py:384 - Embedding validation complete: 821/821 documents have valid embeddings
2025-07-10 02:29:55,781 - INFO - api.rag - rag.py:429 - Using 821 documents with valid embeddings for retrieval
2025-07-10 02:29:55,781 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:29:55,793 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 821 chunks
2025-07-10 02:29:55,793 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 02:29:55,793 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/AsyncFuncAI/deepwiki-open
2025-07-10 02:29:55,793 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['分析这个项目的向量嵌入的实现方案和工作流程，涉及到的相关源文件']}
2025-07-10 02:29:56,505 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 02:29:56,508 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 02:29:56,509 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:29:56,509 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:29:56,509 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/AsyncFuncAI/deepwiki-open (deepwiki-open).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: src/app/[owner]/[repo]/page.tsx

 *   Ensure snippets are well-formatted within Markdown code blocks with appropriate language identifiers.

6.  **Source Citations (EXTREMELY IMPORTANT):**
    *   For EVERY piece of significant information, explanation, diagram, table entry, or code snippet, you MUST cite the specific source file(s) and relevant line numbers from which the information was derived.
    *   Place citations at the end of the paragraph, under the diagram/table, or after the code snippet.
    *   Use the exact format: \`Sources: [filename.ext:start_line-end_line]()\` for a range, or \`Sources: [filename.ext:line_number]()\` for a single line. Multiple files can be cited: \`Sources: [file1.ext:1-10](), [file2.ext:5](), [dir/file3.ext]()\` (if the whole file is relevant and line numbers are not applicable or too broad).
    *   If an entire section is overwhelmingly based on one or two files, you can cite them under the section heading in addition to more specific citations within the section.
    *   IMPORTANT: You MUST cite AT LEAST 5 different source files throughout the wiki page to ensure comprehensive coverage.

7.  **Technical Accuracy:** All information must be derived SOLELY from the \`[RELEVANT_SOURCE_FILES]\`. Do not infer, invent, or use external knowledge about similar systems or common practices unless it's directly supported by the provided code. If information is not present in the provided files, do not include it or explicitly state its absence if crucial to the topic.

8.  **Clarity and Conciseness:** Use clear, professional, and concise technical language suitable for other developers working on or learning about the project. Avoid unnecessary jargon, but use correct technical terms where appropriate.

9.  **Conclusion/Summary:** End with a brief summary paragraph if appropriate for "${page.title}", reiterating the key aspects covered and their significance within the project.

IMPORTANT: Generate the content in ${language === 'en' ? 'English' :
            language === 'ja' ? 'Japanese (日本語)' :
            language === 'zh' ? 'Mandarin Chinese (中文)' :
        

     // Create the prompt content - simplified to avoid message dialogs
 const promptContent =
`You are an expert technical writer and software architect.
Your task is to generate a comprehensive and accurate technical wiki page in Markdown format about a specific feature, system, or module within a given software project.

You will be given:
1. The "[WIKI_PAGE_TOPIC]" for the page you need to create.
2. A list of "[RELEVANT_SOURCE_FILES]" from the project that you MUST use as the sole basis for the content. You have access to the full content of these files. You MUST use AT LEAST 5 relevant source files for comprehensive coverage - if fewer are provided, search for additional related files in the codebase.

CRITICAL STARTING INSTRUCTION:
The very first thing on the page MUST be a \`<details>\` block listing ALL the \`[RELEVANT_SOURCE_FILES]\` you used to generate the content. There MUST be AT LEAST 5 source files listed - if fewer were provided, you MUST find additional related files to include.
Format it exactly like this:
<details>
<summary>Relevant source files</summary>

Remember, do not provide any acknowledgements, disclaimers, apologies, or any other preface before the \`<details>\` block. JUST START with the \`<details>\` block.
The following files were used as context for generating this wiki page:

${filePaths.map(path => `- [${path}](${generateFileUrl(path)})`).join('\n')}
<!-- Add additional relevant files if fewer than 5 were provided -->
</details>

Immediately after the \`<details>\` block, the main title of the page should be a H1 Markdown heading: \`# ${page.title}\`.

Based ONLY on the content of the \`[RELEVANT_SOURCE_FILES]\`:

1.  **Introduction:** Start with a concise introduction (1-2 paragraphs) explaining the purpose, scope, and high-level overview of "${page.title}" within the context of the overall project. If relevant, and if information is available in the provided files, link to other potential wiki pages using the format \`[Link Text](#page-anchor-or-id)\`.

2.  **Detailed Sections:** Break down "${page.title}" into logical sections using H2 (\`##\`) and H3 (\`###\`) Markdown headings. For each section:
    *   Explain the architecture, components, data flow, or logic relevant to the section's focus, as evidenced in the source files.
    *   Identify key functions, classes, data structures, API endpoints, or configuration elements pertinent 

## File Path: README.ja.md

E4[Ollamaで生成]

    E1 --> E[ドキュメントを生成]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[視覚的な図を作成]
    E --> G[Wikiとして整理]
    F --> G
    G --> H[インタラクティブなDeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ プロジェクト構造

```
deepwiki/
├── api/                  # バックエンドAPIサーバー
│   ├── main.py           # APIエントリーポイント
│   ├── api.py            # FastAPI実装
│   ├── rag.py            # 検索拡張生成
│   ├── data_pipeline.py  # データ処理ユーティリティ
│   └── requirements.txt  # Python依存関係
│
├── src/                  # フロントエンドNext.jsアプリ
│   ├── app/              # Next.jsアプリディレクトリ
│   │   └── page.tsx      # メインアプリケーションページ
│   └── components/       # Reactコンポーネント
│       └── Mermaid.tsx   # Mermaid図レンダラー
│
├── public/               # 静的アセット
├── package.json          # JavaScript依存関係
└── .env                  # 環境変数（作成する必要あり）
```

## 🛠️ 高度な設定

### 環境変数

| 変数                          | 説明           

          | ✗    | API とフロントエンドを同じマシンでホストする場合、`NEXT_PUBLIC_SERVER_BASE_URL`のポートを適宜変更してください |
| `SERVER_BASE_URL`             | API サーバーのベース URL（デフォルト：`http://localhost:8001`） | ✗    |                                                                                                               |

### 設定ファイル

DeepWikiはシステムの様々な側面を管理するためにJSON設定ファイルを使用しています：

1. **`generator.json`**: テキスト生成モデルの設定
   - 利用可能なモデルプロバイダー（Google、OpenAI、OpenRouter、Ollama）を定義
   - 各プロバイダーのデフォルトおよび利用可能なモデルを指定
   - temperatureやtop_pなどのモデル固有のパラメータを含む

2. **`embedder.json`**: 埋め込みモデルとテキスト処理の設定
   - ベクトルストレージ用の埋め込みモデルを定義
   - RAG用の検索設定を含む
   - ドキュメントチャンク分割のためのテキスト分割設定を指定

3. **`repo.json`**: リポジトリ処理の設定
   - 特定のファイルやディレクトリを除外するファイルフィルターを含む
   - リポジトリサイズ制限と処理ルールを定義

デフォルトでは、これらのファイルは`api/config/`ディレクトリにあります。`DEEPWIKI_CONFIG_DIR`環境変数を使用して、その場所をカスタマイズできます。

### Docker セットアップ

Docker を使用して DeepWiki を実行できます：

```bash
# GitHub Container Registryからイメージをプル
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# 環境変数を設定してコンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

このコマンドは、ホスト上の ⁠~/.adalflow をコンテナ内の ⁠/root/.adalflow にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (⁠~/.adalflow/repos/)
- それらのエンベディングとインデックス (⁠~/.adalflow/databases/)
- 生成された Wiki のキャッシュ (⁠~/.adalflow/wikicache/)

これにより、コンテナが停止または削除されてもデータが永続化されます。
または、提供されている ⁠docker-compose.yml ファイルを使用します。

```bash
# まず.envファイルをAPIキーで編集
docker-compose up
```

（⁠docker-compose.yml ファイルは、上記の ⁠docker run コマンドと同様に、データ永続化のために ⁠~/.adalflow をマウントするように事前設定されています。）

#### Docker で.env ファイルを使用する

.env ファイルをコンテナにマウントすることもできます：

```bash
# APIキーを含む.envファイルを作成
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# .envファイルをマウントしてコンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

このコマンドは、ホスト上の ⁠~/.adalflow をコンテナ内の ⁠/root/.adalflow にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (⁠~/.adalflow/repos/)
- それらのエンベディングとインデックス (⁠~/.adalflow/databases/)
- 生成された Wiki のキャッシュ (⁠~/.adalflow/wikicache/)

これにより、コンテナが停止または削除されてもデータが永続化されます。

#### Docker イメージをローカルでビルドする

Docker イメージをローカルでビルドしたい場合：

```bash
# リポジトリをクローン
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Dockerイメージをビルド
docker build -t deepwiki-open .

# コンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  deepwiki-open
```

# API サーバー詳細

API サーバーは以下を提供します：

- リポジトリのクローンとインデックス作成
- RAG（Retrieval Augmented Generation：検索拡張生成）
- ストリーミングチャット補完

詳細については、API README を参照してください。

## 🤖 プロバイダーベースのモデル選択システム

DeepWikiでは、複数のLLMプロバイダーをサポートする柔軟なプロバイダーベースのモデル選択システムを実装しています：

### サポートされているプロバイダーとモデル

- **Google**: デフォルトは `gemini-2.0-flash`、また `gemini-1.5-flash`、`gemini-1.0-pro` などもサポート
- **OpenAI**: デフォルトは `gpt-4o`、また `o4-mini` などもサポート
- **OpenRouter**: Claude、Llama、Mistralなど、統一APIを通じて複数のモデルにアクセス
- 

で.env ファイルを使用する

.env ファイルをコンテナにマウントすることもできます：

```bash
# APIキーを含む.envファイルを作成
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# .envファイルをマウントしてコンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

このコマンドは、ホスト上の ⁠~/.adalflow をコンテナ内の ⁠/root/.adalflow にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (⁠~/.adalflow/repos/)
- それらのエンベディングとインデックス (⁠~/.adalflow/databases/)
- 生成された Wiki のキャッシュ (⁠~/.adalflow/wikicache/)

これにより、コンテナが停止または削除されてもデータが永続化されます。

#### Docker イメージをローカルでビルドする

Docker イメージをローカルでビルドしたい場合：

```bash
# リポジトリをクローン
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Dockerイメージをビルド
docker build -t deepwiki-open .

# コンテナを実行
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  deepwiki-open
```

# API サーバー詳細

API サーバーは以下を提供します：

- リポジトリのクローンとインデックス作成
- RAG（Retrieval Augmented Generation：検索拡張生成）
- ストリーミングチャット補完

詳細については、API README を参照してください。

## 🤖 プロバイダーベースのモデル選択システム

DeepWikiでは、複数のLLMプロバイダーをサポートする柔軟なプロバイダーベースのモデル選択システムを実装しています：

### サポートされているプロバイダーとモデル

- **Google**: デフォルトは `gemini-2.0-flash`、また `gemini-1.5-flash`、`gemini-1.0-pro` などもサポート
- **OpenAI**: デフォルトは `gpt-4o`、また `o4-mini` などもサポート
- **OpenRouter**: Claude、Llama、Mistralなど、統一APIを通じて複数のモデルにアクセス
- **Ollama**: `llama3` などのローカルで実行するオープンソースモデルをサポート

### 環境変数

各プロバイダーには、対応するAPI鍵の環境変数が必要です：

```
# API鍵
GOOGLE_API_KEY=あなたのGoogle API鍵        # Google Geminiモデルに必要
OPENAI_API_KEY=あなたのOpenAI鍵            # OpenAIモデルに必要
OPENROUTER_API_KEY=あなたのOpenRouter鍵    # OpenRouterモデルに必要

# OpenAI APIベースURL設定
OPENAI_BASE_URL=https://カスタムAPIエンドポイント.com/v1  # オプション、カスタムOpenAI APIエンドポイント用
```

### サービスプロバイダー向けのカスタムモデル選択

カスタムモデル選択機能は、あなたの組織のユーザーに様々なAIモデルの選択肢を提供するために特別に設計されています：

- あなたは組織内のユーザーに様々なAIモデルの選択肢を提供できます
- あなたはコード変更なしで急速に進化するLLM環境に迅速に適応できます
- あなたは事前定義リストにない専門的またはファインチューニングされたモデルをサポートできます

サービスプロバイダーは、事前定義されたオプションから選択するか、フロントエンドインターフェースでカスタムモデル識別子を入力することで、モデル提供を実装できます。

### エンタープライズプライベートチャネル向けのベースURL設定

OpenAIクライアントのbase_url設定は、主にプライベートAPIチャネルを持つエンタープライズユーザー向けに設計されています。この機能は：

- プライベートまたは企業固有のAPIエンドポイントへの接続を可能に
- 組織が自己ホスト型または独自にデプロイされたLLMサービスを使用可能に
- サードパーティのOpenAI API互換サービスとの統合をサポート

**近日公開**: 将来のアップデートでは、ユーザーがリクエストで自分のAPI鍵を提供する必要があるモードをDeepWikiがサポートする予定です。これにより、プライベートチャネルを持つエンタープライズ顧客は、DeepWikiデプロイメントと認証情報を共有することなく、既存のAPI設定を使用できるようになります。

## 🔌 OpenRouter 連携

DeepWiki は、モデルプロバイダーとして OpenRouter をサポートするようになり、単一の API を通じて数百の AI モデルにアクセスできるようになりました。

- 複数のモデルオプション: OpenAI、Anthropic、Google、Meta、Mistralなど、統一APIを通じて複数のモデルにアクセス
- 簡単な設定: OpenRouter API キーを追加し、使用したいモデルを選択するだけ
- コスト効率: 予算とパフォーマンスのニーズに合ったモデルを選択
- 簡単な切り替え: コードを変更することなく、異なるモデル間を切り替え可能

### DeepWiki で OpenRouter を使用する方法

1. API キーを取得: OpenRouter でサインアップし、API キーを取得します
2. 環境に追加: ⁠.env ファイルに ⁠OPENROUTER_API_KEY=your_key を追加します
3. UI で有効化: ホームページの「OpenRouter API を使用」オプションをチェックします
4. モデルを選択: GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 などの人気モデルから選択します

OpenRouter は特に以下のような場合に便利です：

- 複数のサービスにサインアップせずに異なるモデルを試したい
- お住まいの地域で制限されている可能性のあるモデルにアクセスしたい
- 異なるモデルプロバイダー間でパフォーマンスを比較したい
- ニーズに基づいてコストとパフォーマンスを最適化したい

## 🤖 質問と詳細調査機能

### 質問機能

質問機能を使用すると、検索拡張生成（RAG）を使用してリポジトリとチャットできます：

- **コンテキスト対応の回答**: リポジトリの実際のコードに基づいた正確な回答を取得
- **RAG 搭載**: システムは関連するコードスニペットを取得して根拠のある回答を提供
- **リアルタイムストリーミング**: よりインタラクティブな体験のために、生成されるレスポンスをリアルタイムで確認
- **会話履歴**: システムは質問間のコンテキストを維持し、より一貫性のあるインタラクションを実現

### 詳細調査機能

詳細調査は、複数ターンの研究プロセスでリポジトリ分析を次のレベルに引き上げます：

- **詳細な調査**: 複数の研究反復を通じて複雑なトピックを徹底的に探索
- **構造化されたプロセス**: 明確な研究計画、更新、包括的な結論を含む
- **自動継続**: AI は結論に達するまで自動的に研究を継続（最大 5 回の反復）
- **研究段階**:
  1. **研究計画**: アプローチと初期調査結果の概要
  2. **研究更新**: 新しい洞察を加えて前の反復を発展
  3. **最終結論**: すべての反復に基づく包括的な回答を提供

詳細調査を使用するには、質問を送信する前に質問インターフェースの「詳細調査」スイッチをオンにするだけです。

## 📱 スクリーンショット

![DeepWikiメインインターフェース](screenshots/Interface.png)
_DeepWiki のメインインターフェース_

![プライベートリポジトリサポート](screenshots/privaterepo.png)
_個人アクセストークンを使用したプライベートリポジトリへのアクセス_

![詳細調査機能](screenshots/DeepResearch.png)
_詳細調査は複雑なトピックに対して多段階の調査を実施_

### デモビデオ

[![DeepWikiデモビデオ](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

_DeepWiki の動作を見る！_

## ❓ トラブルシューティング

### API キーの問題

- **「環境変数が見つかりません」**: `.env`ファイルがプロジェクトのルートにあり、必要な API キーが含まれていることを確認
- **「API キーが無効です」**: キー全体が余分なスペースなしで正しくコピーされていることを確認
- **「OpenRouter API エラー」**: OpenRouter API キーが有効で、十分なクレジットがあることを確認

### 接続の問題

- **「API サーバーに接続できません」**: API サーバーがポート 8001 で実行されていることを確認
- **「CORS エラー」**: API はすべてのオリジンを許可するように設定されていますが、問題がある場合は、フロントエンドとバックエンドを同じマシンで実行してみてください

### 生成の問題

- **「Wiki の生成中にエラーが発生しました」**: 非常に大きなリポジトリの場合は、まず小さいものから試してみてください
- **「無効なリポジトリ形式」**: 有効な GitHub、GitLab、または Bitbucket URL の形式を使用していることを確認
- **「リポジトリ構造を取得できませんでした」**: プライベートリポジトリの場合、適切な権限を持つ有効な個人アクセストークンを入力したことを確認
- **「図のレンダリングエラー」**: アプリは自動的に壊れた図を修正しようとします

### 一般的な解決策

1. **両方のサーバーを再起動**: 単純な再起動でほとんどの問題が解決することがあります
2. **コンソールログを確認**: ブラウザの開発者ツールを開いて JavaScript エラーを確認
3. **API ログを確認**: API が実行されているターミナルで Python エラーを確認

## 🤝 貢献

貢献は歓迎します！以下のことを自由に行ってください：

- バグや機能リクエストの問題を開く
- コードを改善するためのプルリクエストを提出
- フィードバックやアイデアを共有

## 📄 ライセンス

このプロジェクトは MIT ライセンスの下でライセンスされています - 詳細は[LICENSE](LICENSE)ファイルを参照してください。

## ⭐ スター履歴

[![スター履歴チャート](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)



# DeepWiki-Open

![DeepWiki バナー](screenshots/Deepwiki.png)

**DeepWiki**は、GitHub、GitLab、または Bitbucket リポジトリのための美しくインタラクティブな Wiki を自動的に作成します！リポジトリ名を入力するだけで、DeepWiki は以下を行います：

1. コード構造を分析
2. 包括的なドキュメントを生成
3. すべての仕組みを説明する視覚的な図を作成
4. すべてを簡単に閲覧できる Wiki に整理

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ 特徴

- **即時ドキュメント生成**: あらゆる GitHub、GitLab、または Bitbucket リポジトリを数秒で Wiki に変換
- **プライベートリポジトリ対応**: 個人アクセストークンを使用してプライベートリポジトリに安全にアクセス
- **スマート分析**: AI を活用したコード構造と関係の理解
- **美しい図表**: アーキテクチャとデータフローを視覚化する自動 Mermaid 図
- **簡単なナビゲーション**: Wiki を探索するためのシンプルで直感的なインターフェース
- **質問機能**: RAG 搭載 AI を使用してリポジトリとチャットし、正確な回答を得る
- **詳細調査**: 複雑なトピックを徹底的に調査する多段階研究プロセス
- **複数のモデルプロバイダー**: Google Gemini、OpenAI、OpenRouter、およびローカル Ollama モデルのサポート

## 🚀 クイックスタート（超簡単！）

### オプション 1: Docker を使用

```bash
# リポジトリをクローン
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# APIキーを含む.envファイルを作成
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# オプション: OpenRouterモデルを使用する場合はOpenRouter APIキーを追加
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# Docker Composeで実行
docker-compose up
```

(上記の Docker コマンドおよび`docker-compose.yml`の設定では、ホスト上の`~/.adalflow`ディレクトリをコンテナ内の`/root/.adalflow`にマウントします。このパスは以下のものを保存するために使用されます：

- クローンされたリポジトリ (`~/.adalflow/repos/`)
- それらのエンベディングとインデックス (`~/.adalflow/databases/`)
- 生成された Wiki のキャッシュ (`~/.adalflow/wikicache/`)

これにより、コンテナが停止または削除されてもデータが永続化されます。)

> 💡 **これらのキーの入手先:**
>
> - Google API キーは[Google AI Studio](https://makersuite.google.com/app/apikey)から取得
> - OpenAI API キーは[OpenAI Platform](https://platform.openai.com/api-keys)から取得

### オプション 2: 手動セットアップ（推奨）

#### ステップ 1: API キーの設定

プロジェクトのルートに`.env`ファイルを作成し、以下のキーを追加します：

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# オプション: OpenRouterモデルを使用する場合は追加
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### ステップ 2: バックエンドの起動

```bash
# Pythonの依存関係をインストール
pip install -r api/requirements.txt

# APIサーバーを起動
python -m api.main
```

#### ステップ 3: フロントエンドの起動

```bash
# JavaScript依存関係をインストール
npm install
# または
yarn install

# Webアプリを起動
npm run dev
# または
yarn dev
```

#### ステップ 4: DeepWiki を使用！

1. ブラウザで[http://localhost:3000](http://localhost:3000)を開く
2. GitHub、GitLab、または Bitbucket リポジトリを入力（例：`https://github.com/openai/codex`、`https://github.com/microsoft/autogen`、`https://gitlab.com/gitlab-org/gitlab`、または`https://bitbucket.org/redradish/atlassian_app_versions`）
3. プライベートリポジトリの場合は、「+ アクセストークンを追加」をクリックして GitHub または GitLab の個人アクセストークンを入力
4. 「Wiki を生成」をクリックして、魔法が起こるのを見守りましょう！

## 🔍 仕組み

DeepWiki は AI を使用して：

1. GitHub、GitLab、または Bitbucket リポジトリをクローンして分析（トークン認証によるプライベートリポジトリを含む）
2. スマート検索のためのコードの埋め込みを作成
3. コンテキスト対応 AI でドキュメントを生成（Google Gemini、OpenAI、OpenRouter、またはローカル Ollama モデルを使用）
4. コードの関係を説明する視覚的な図を作成
5. すべてを構造化された Wiki に整理
6. 質問機能を通じてリポジトリとのインテリジェントな Q&A を可能に
7. 詳細調査機能で深い研究能力を提供

```mermaid
graph TD
    A[ユーザーがGitHub/GitLab/Bitbucketリポジトリを入力] --> AA{プライベートリポジトリ?}
    AA -->|はい| AB[アクセストークンを追加]
    AA -->|いいえ| B[リポジトリをクローン]
    AB --> B
    B --> C[コード構造を分析]
    C --> D[コード埋め込みを作成]

    D --> M{モデルプロバイダーを選択}
    M -->|Google Gemini| E1[Geminiで生成]
    M -->|OpenAI| E2[OpenAIで生成]
    M -->|OpenRouter| E3[OpenRouterで生成]
    M -->|ローカルOllama| E4[Ollamaで生成]

    E1 --> E[ドキュメントを生成]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[視覚的な図を作成]
    E --> G[Wikiとして整理]
    F --> G
    G --> H[インタラクティブなDeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ プロジェクト構造

```
deepwiki/
├── api/ 

## File Path: README.kr.md

또는
yarn install

# 웹 앱 실행
npm run dev
# 또는
yarn dev
```

#### 4단계: DeepWiki 사용하기!

1. 브라우저에서 [http://localhost:3000](http://localhost:3000) 열기
2. GitHub, GitLab 또는 Bitbucket 저장소 입력 (예: `https://github.com/openai/codex`, `https://github.com/microsoft/autogen`, `https://gitlab.com/gitlab-org/gitlab`, `https://bitbucket.org/redradish/atlassian_app_versions`)
3. 비공개 저장소인 경우 "+ 액세스 토큰 추가" 클릭 후 GitHub 또는 GitLab 개인 액세스 토큰 입력
4. "Generate Wiki" 클릭 후 마법을 지켜보기!

## 🔍 작동 방식

DeepWiki는 AI를 사용하여 다음을 수행합니다:

1. GitHub, GitLab 또는 Bitbucket 저장소 복제 및 분석 (토큰 인증이 필요한 비공개 저장소 포함)
2. 스마트 검색을 위한 코드 임베딩 생성
3. 문맥 인지 AI로 문서 생성 (Google Gemini, OpenAI, OpenRouter 또는 로컬 Ollama 모델 사용)
4. 코드 관계를 설명하는 시각적 다이어그램 생성
5. 모든 것을 구조화된 위키로 정리
6. Ask 기능을 통한 저장소와의 지능형 Q&A 지원
7. DeepResearch로 심층 연구 기능 제공

```mermaid
graph TD
    A[사용자가 GitHub/GitLab/Bitbucket 저장소 입력] --> AA{비공개 저장소인가?}
    AA -->|예| AB[액세스 토큰 추가]
    AA -->|아니오| B[저장소 복제]
    AB --> B
    B --> C[코드 구조 분석]
    C --> D[코드 임베딩 생성]

    D --> M{모델 제공자 선택}
    M -->|Google Gemini| E1[Gemini로 생성]
    M -->|OpenAI| E2[OpenAI로 생성]
    M -->|OpenRouter| E3[OpenRouter로 생성]
    M -->|로컬 Ollama| E4[Ollama로 생성]

    E1 --> E[문서 생성]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[시각적 다이어그램 생성]
    E --> G[위키로 정리]
    F --> G
    G --> H[대화형 DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ 프로젝트 구조

```
deepwiki/
├── api/                  # 백엔드 API 서버
│   ├── main.py           # API 진입점
│   ├── api.py      

Interface](screenshots/Interface.png)
*DeepWiki의 메인 인터페이스*

![Private Repository Support](screenshots/privaterepo.png)
*개인 액세스 토큰으로 비공개 저장소 접근*

![DeepResearch Feature](screenshots/DeepResearch.png)
*DeepResearch는 복잡한 주제에 대해 다중 턴 조사를 수행*

### 데모 영상

[![DeepWiki Demo Video](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*DeepWiki 작동 영상 보기!*

## ❓ 문제 해결

### API 키 문제
- **"환경 변수 누락"**: `.env` 파일이 프로젝트 루트에 있고 필요한 API 키가 포함되어 있는지 확인
- **"API 키가 유효하지 않음"**: 키를 정확히 복사했는지, 공백이 없는지 확인
- **"OpenRouter API 오류"**: OpenRouter API 키가 유효하고 충분한 크레딧이 있는지 확인

### 연결 문제
- **"API 서버에 연결할 수 없음"**: API 서버가 포트 8001에서 실행 중인지 확인
- **"CORS 오류"**: API가 모든 출처를 허용하도록 설정되어 있지만 문제가 있으면 프론트엔드와 백엔드를 같은 머신에서 실행해 보세요

### 생성 문제
- **"위키 생성 오류"**: 아주 큰 저장소는 먼저 작은 저장소로 시도해 보세요
- **"잘못된 저장소 형식"**: 유효한 GitHub, GitLab 또는 Bitbucket URL 형식인지 확인
- **"저장소 구조를 가져올 수 없음"**: 비공개 저장소라면 적절한 권한의 개인 액세스 토큰을 입력했는지 확인
- **"다이어그램 렌더링 오류"**: 앱이 자동으로 다이어그램 오류를 수정하려 시도합니다

### 일반적인 해결법
1. **서버 둘 다 재시작**: 간단한 재시작으로 대부분 문제 해결
2. **콘솔 로그 확인**: 브라우저 개발자 도구에서 자바스크립트 오류 확인
3. **API 로그 확인**: API 실행 터미널에서 Python 오류 확인

## 🤝 기여

기여를 환영합니다! 다음을 자유롭게 해주세요:
- 버그나 기능 요청을 위한 이슈 열기
- 코드 개선을 위한 풀 리퀘스트 제출
- 피드백과 아이디어 공유

## 📄 라이선스

이 프로젝트는 MIT 라이선스 하에 있습니다 - 자세한 내용은 [LICENSE](LICENSE) 파일 참고.

## ⭐ 스타 히스토리

[![Star History Chart](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)



         # JavaScript 의존성
└── .env                  # 환경 변수 (직접 생성)
```

## 🛠️ 고급 설정

### 환경 변수

| 변수명 | 설명 | 필수 | 비고 |
|----------|-------------|----------|------|
| `GOOGLE_API_KEY` | AI 생성용 Google Gemini API 키 | 예 |
| `OPENAI_API_KEY` | 임베딩용 OpenAI API 키 | 예 |
| `OPENROUTER_API_KEY` | 대체 모델용 OpenRouter API 키 | 아니오 | OpenRouter 모델 사용 시 필요 |
| `PORT` | API 서버 포트 (기본값: 8001) | 아니오 | API와 프론트엔드를 같은 머신에서 호스팅 시 `SERVER_BASE_URL`의 포트도 변경 필요 |
| `SERVER_BASE_URL` | API 서버 기본 URL (기본값: http://localhost:8001) | 아니오 |

### 설정 파일

DeepWiki는 시스템의 다양한 측면을 관리하기 위해 JSON 설정 파일을 사용합니다:

1. **`generator.json`**: 텍스트 생성 모델 설정
   - 사용 가능한 모델 제공자(Google, OpenAI, OpenRouter, Ollama) 정의
   - 각 제공자의 기본 및 사용 가능한 모델 지정
   - temperature와 top_p 같은 모델별 매개변수 포함

2. **`embedder.json`**: 임베딩 모델 및 텍스트 처리 설정
   - 벡터 저장소용 임베딩 모델 정의
   - RAG를 위한 검색기 설정 포함
   - 문서 청킹을 위한 텍스트 분할기 설정 지정

3. **`repo.json`**: 저장소 처리 설정
   - 특정 파일 및 디렉토리를 제외하는 파일 필터 포함
   - 저장소 크기 제한 및 처리 규칙 정의

기본적으로 이러한 파일은 `api/config/` 디렉토리에 위치합니다. `DEEPWIKI_CONFIG_DIR` 환경 변수를 사용하여 위치를 사용자 정의할 수 있습니다.

### Docker 설정

Docker를 사용하여 DeepWiki를 실행할 수 있습니다:

```bash
# GitHub 컨테이너 레지스트리에서 이미지 가져오기
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# 환경 변수와 함께 컨테이너 실행
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

이 명령어는 또한 호스트의 `~/.adalflow`를 컨테이너의 `/root/.adalflow`에 마운트합니다. 이 경로는 다음을 저장하는 데 사용됩니다:
- 복제된 저장소 (`~/.adalflow/repos/`)
- 해당 저장소의 임베딩 및 인덱스 (`~/.adalflow/databases/`)
- 생성된 위키의 캐시 (`~/.adalflow/wikicache/`)

이를 통해 컨테이너가 중지되거나 제거되어도 데이터가 유지됩니다.

또는 제공된 `docker-compose.yml` 파일을 사용하세요:

```bash
# API 키가 포함된 .env 파일을 먼저 편집
docker-compose up
```

(`docker-compose.yml` 파일은 위의 `docker run` 명령어와 유사하게 데이터 지속성을 위해 `~/.adalflow`를 마운트하도록 미리 구성되어 있습니다.)

#### Docker에서 .env 파일 사용하기

.env 파일을 컨테이너에 마운트할 수도 있습니다:

```bash
# API 키가 포함된 .env 파일 

## File Path: api/data_pipeline.py

                 "file_path": relative_path,
                            "type": ext[1:],
                            "is_code": False,
                            "is_implementation": False,
                            "title": relative_path,
                            "token_count": token_count,
                        },
                    )
                    documents.append(doc)
            except Exception as e:
                logger.error(f"Error reading {file_path}: {e}")

    logger.info(f"Found {len(documents)} documents")
    return documents

def prepare_data_pipeline(is_ollama_embedder: bool = None):
    """
    Creates and returns the data transformation pipeline.

    Args:
        is_ollama_embedder (bool, optional): Whether to use Ollama for embedding.
                                        

## File Path: test/__init__.py

# Test package for deepwiki-open data pipeline


## File Path: README.es.md

las iteraciones

Para usar Investigación Profunda, simplemente activa el interruptor "Investigación Profunda" en la interfaz de Preguntas antes de enviar tu pregunta.

## 📱 Capturas de Pantalla

![Interfaz Principal de DeepWiki](screenshots/Interface.png)
*La interfaz principal de DeepWiki*

![Soporte para Repositorios Privados](screenshots/privaterepo.png)
*Acceso a repositorios privados con tokens de acceso personal*

![Función de Investigación Profunda](screenshots/DeepResearch.png)
*Investigación Profunda realiza investigaciones de múltiples turnos para temas complejos*

### Video de Demostración

[![Video de Demostración de DeepWiki](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*¡Mira DeepWiki en acción!*

## ❓ Solución de Problemas

### Problemas con Claves API
- **"Faltan variables de entorno"**: Asegúrate de que tu archivo `.env` esté en la raíz del proyecto y contenga las claves API requeridas
- **"Clave API no válida"**: Verifica que hayas copiado la clave completa correctamente sin espacios adicionales
- **"Error de API OpenRouter"**: Verifica que tu clave API de OpenRouter sea válida y tenga créditos suficientes

### Problemas de Conexión
- **"No se puede conectar al servidor API"**: Asegúrate de que el servidor API esté ejecutándose en el puerto 8001
- **"Error CORS"**: La API está configurada para permitir todos los orígenes, pero si tienes problemas, intenta ejecutar tanto el frontend como el backend en la misma máquina

### Problemas de Generación
- **"Error al generar wiki"**: Para repositorios muy grandes, prueba primero con uno más pequeño
- **"Formato de repositorio no válido"**: Asegúrate de usar un formato de URL válido para GitHub, GitLab o Bitbucket
- **"No se pudo obtener la estructura del repositorio"**: Para repositorios privados, asegúrate de haber ingresado un token de acceso personal válido con los permisos apropiados
- **"Error de renderizado de diagrama"**: La aplicación intentará automáticamente arreglar los diagramas rotos

### Soluciones Comunes
1. **Reiniciar ambos servidores**: A veces un simple reinicio soluciona la mayoría de los problemas
2. **Revisar los registros de la consola**: Abre las herramientas de desarrollo del navegador para ver cualquier error de JavaScript
3. **Revisar los registros de la API**: Mira la terminal donde se ejecuta la API para ver errores de Python

## 🤝 Contribuir

¡Las contribuciones son bienvenidas! Siéntete libre de:
- Abrir issues para bugs o solicitudes de funciones
- Enviar pull requests para mejorar el código
- Compartir tus comentarios e ideas

## 📄 Licencia

Este proyecto está licenciado bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para más detalles.

## ⭐ Historial 

el de `api/config/embedder_openai_compatible.json`.
2. En el archivo `.env` de la raíz del proyecto, configura las variables de entorno necesarias, por ejemplo:
   ```
   OPENAI_API_KEY=tu_api_key
   OPENAI_BASE_URL=tu_endpoint_compatible_openai
   ```
3. El programa sustituirá automáticamente los placeholders de embedder.json por los valores de tus variables de entorno.

Así puedes cambiar fácilmente a cualquier servicio de embedding compatible con OpenAI sin modificar el código.

## 🤖 Funciones de Preguntas e Investigación Profunda

### Función de Preguntas

La función de Preguntas te permite chatear con tu repositorio usando Generación Aumentada por Recuperación (RAG):

- **Respuestas Conscientes del Contexto**: Obtén respuestas precisas basadas en el código real de tu repositorio
- **Potenciada por RAG**: El sistema recupera fragmentos de código relevantes para proporcionar respuestas fundamentadas
- **Transmisión en Tiempo Real**: Ve las respuestas mientras se generan para una experiencia más interactiva
- **Historial de Conversación**: El sistema mantiene el contexto entre preguntas para interacciones más coherentes

### Función de Investigación Profunda

Investigación Profunda lleva el análisis de repositorios al siguiente nivel con un proceso de investigación de múltiples turnos:

- **Investigación en Profundidad**: Explora a fondo temas complejos a través de múltiples iteraciones de investigación
- **Proceso Estructurado**: Sigue un plan de investigación claro con actualizaciones y una conclusión completa
- **Continuación Automática**: La IA continúa automáticamente la investigación hasta llegar a una conclusión (hasta 5 iteraciones)
- **Etapas de Investigación**:
  1. **Plan de Investigación**: Describe el enfoque y los hallazgos iniciales
  2. **Actualizaciones de Investigación**: Desarrolla las iteraciones anteriores con nuevas perspectivas
  3. **Conclusión Final**: Proporciona una respuesta completa basada en todas las iteraciones

Para usar Investigación Profunda, simplemente activa el interruptor "Investigación Profunda" en la interfaz de Preguntas antes de enviar tu pregunta.

## 📱 Capturas de Pantalla

![Interfaz Principal de DeepWiki](screenshots/Interface.png)
*La interfaz principal de DeepWiki*

![Soporte para Repositorios Privados](screenshots/privaterepo.png)
*Acceso a repositorios privados con tokens de acceso personal*

![Función de Investigación Profunda](screenshots/DeepResearch.png)
*Investigación Profunda realiza investigaciones de múltiples turnos para temas complejos*

### Video de Demostración

[![Video de Demostración de DeepWiki](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*¡Mira DeepWiki en acción!*

## ❓ Solución de Problemas

### Problemas con Claves API
- **"Faltan variables de entorno"**: Asegúrate de que tu archivo `.env` esté en la raíz del proyecto y contenga las claves API requeridas
- **"Clave API no válida"**: Verifica 

Modelos Soportados

- **Google**: Predeterminado `gemini-2.0-flash`, también soporta `gemini-1.5-flash`, `gemini-1.0-pro`, etc.
- **OpenAI**: Predeterminado `gpt-4o`, también soporta `o4-mini`, etc.
- **OpenRouter**: Acceso a múltiples modelos a través de una API unificada, incluyendo Claude, Llama, Mistral, etc.
- **Ollama**: Soporte para modelos de código abierto ejecutados localmente como `llama3`

### Variables de Entorno

Cada proveedor requiere sus correspondientes variables de entorno para las claves API:

```
# Claves API
GOOGLE_API_KEY=tu_clave_api_google        # Requerida para modelos Google Gemini
OPENAI_API_KEY=tu_clave_api_openai        # Requerida para modelos OpenAI
OPENROUTER_API_KEY=tu_clave_api_openrouter # Requerida para modelos OpenRouter

# Configuración de URL Base de OpenAI API
OPENAI_BASE_URL=https://punto-final-personalizado.com/v1  # Opcional, para endpoints personalizados de OpenAI API

# Directorio de Configuración
DEEPWIKI_CONFIG_DIR=/ruta/a/directorio/config/personalizado  # Opcional, para ubicación personalizada de archivos de configuración
```

### Archivos de Configuración

DeepWiki utiliza archivos de configuración JSON para gestionar varios aspectos del sistema:

1. **`generator.json`**: Configuración para modelos de generación de texto
   - Define los proveedores de modelos disponibles (Google, OpenAI, OpenRouter, Ollama)
   - Especifica los modelos predeterminados y disponibles para cada proveedor
   - Contiene parámetros específicos de los modelos como temperatura y top_p

2. **`embedder.json`**: Configuración para modelos de embeddings y procesamiento de texto
   - Define modelos de embeddings para almacenamiento vectorial
   - Contiene configuración del recuperador para RAG
   - Especifica ajustes del divisor de texto para fragmentación de documentos

3. **`repo.json`**: Configuración para manejo de repositorios
   - Contiene filtros de archivos para excluir ciertos archivos y directorios
   - Define límites de tamaño de repositorio y reglas de procesamiento

Por defecto, estos archivos se encuentran en el directorio `api/config/`. Puedes personalizar su ubicación usando la variable de entorno `DEEPWIKI_CONFIG_DIR`.

### Selección de Modelos Personalizados para Proveedores de Servicios

La función de selección de modelos personalizados está diseñada específicamente para proveedores de servicios que necesitan:

- Puede ofrecer a los usuarios dentro de su organización una selección de diferentes modelos de IA
- Puede adaptarse rápidamente al panorama de LLM en rápida evolución sin cambios de código
- Puede soportar modelos especializados o ajustados que no están en la lista predefinida

Usted puede implementar sus ofertas de modelos seleccionando entre las opciones predefinidas o 

## File Path: README.zh.md

**RAG驱动**：系统检索相关代码片段，提供有根据的响应
- **实时流式传输**：实时查看生成的响应，获得更交互式的体验
- **对话历史**：系统在问题之间保持上下文，实现更连贯的交互

### 深度研究功能

深度研究通过多轮研究过程将仓库分析提升到新水平：

- **深入调查**：通过多次研究迭代彻底探索复杂主题
- **结构化过程**：遵循清晰的研究计划，包含更新和全面结论
- **自动继续**：AI自动继续研究直到达成结论（最多5次迭代）
- **研究阶段**：
  1. **研究计划**：概述方法和初步发现
  2. **研究更新**：在前一轮迭代基础上增加新见解
  3. **最终结论**：基于所有迭代提供全面答案

要使用深度研究，只需在提交问题前在提问界面中切换"深度研究"开关。

## 📱 截图

![DeepWiki主界面](screenshots/Interface.png)
*DeepWiki的主界面*

![私有仓库支持](screenshots/privaterepo.png)
*使用个人访问令牌访问私有仓库*

![深度研究功能](screenshots/DeepResearch.png)
*深度研究为复杂主题进行多轮调查*

### 演示视频

[![DeepWiki演示视频](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*观看DeepWiki实际操作！*

## ❓ 故障排除

### API密钥问题
- **"缺少环境变量"**：确保您的`.env`文件位于项目根目录并包含所需的API密钥
- **"API密钥无效"**：检查您是否正确复制了完整密钥，没有多余空格
- **"OpenRouter API错误"**：验证您的OpenRouter API密钥有效且有足够的额度

### 连接问题
- **"无法连接到API服务器"**：确保API服务器在端口8001上运行
- **"CORS错误"**：API配置为允许所有来源，但如果您遇到问题，请尝试在同一台机器上运行前端和后端

### 生成问题
- **"生成Wiki时出错"**：对于非常大的仓库，请先尝试较小的仓库
- **"无效的仓库格式"**：确保您使用有效的GitHub、GitLab或Bitbucket URL格式
- **"无法获取仓库结构"**：对于私有仓库，确保您输入了具有适当权限的有效个人访问令牌
- **"图表渲染错误"**：应用程序将自动尝试修复损坏的图表

### 常见解决方案
1. **重启两个服务器**：有时简单的重启可以解决大多数问题
2. **检查控制台日志**：打开浏览器开发者工具查看任何JavaScript错误
3. **检查API日志**：查看运行API的终端中的Python错误

## 🤝 贡献

欢迎贡献！随时：
- 为bug或功能请求开issue
- 提交pull request改进代码
- 分享您的反馈和想法

## 📄 许可证

本项目根据MIT许可证授权 - 详情请参阅[LICENSE](LICENSE)文件。

## ⭐ 星标历史

[![星标历史图表](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)

## 🤖 基于提供者的模型选择系统

DeepWiki 现在实现了灵活的基于提供者的模型选择系统，支持多种 LLM 提供商：

### 支持的提供商和模型

- **Google**: 默认使用 `gemini-2.0-flash`，还支持 `gemini-1.5-flash`、`gemini-1.0-pro` 等
- **OpenAI**: 默认使用 `gpt-4o`，还支持 `o4-mini` 等
- **OpenRouter**: 通过统一 API 访问多种模型，包括 Claude、Llama、Mistral 等
- **Ollama**: 支持本地运行的开源模型，如 `llama3`

### 环境变量

每个提供商需要相应的 API 密钥环境变量：

```
# API 密钥
GOOGLE_API_KEY=你的谷歌API密钥        # 使用 Google Gemini 模型必需
OPENAI_API_KEY=你的OpenAI密钥        # 使用 OpenAI 模型必需
OPENROUTER_API_KEY=你的OpenRouter密钥 # 使用 OpenRouter 模型必需

# OpenAI API 基础 URL 配置
OPENAI_BASE_URL=https://自定义API端点.com/v1  # 可选，用于自定义 OpenAI API 端点
```

### 为服务提供者设计的自定义模型选择

自定义模型选择功能专为需要以下功能的服务提供者设计：

- 您可在您的组织内部为用户提供多种 AI 模型选择
- 您无需代码更改即可快速适应快速发展的 LLM 领域
- 您可支持预定义列表中没有的专业或微调模型

使用者可以通过从服务提供者预定义选项中选择或在前端界面中输入自定义模型标识符来实现其模型产品。

### 为企业私有渠道设计的基础 URL 配置

OpenAI 客户端的 base_url 配置主要为拥有私有 API 渠道的企业用户设计。此功能：

- 支持连接到私有或企业特定的 API 端点
- 允许组织使用自己的自托管或自定义部署的 LLM 服务
- 支持与第三方 OpenAI API 兼容服务的集成

**即将推出**：在未来的更新中，DeepWiki 将支持一种模式，用户需要在请求中提供自己的 API 密钥。这将允许拥有私有渠道的企业客户使用其现有的 API 安排，而不是与 DeepWiki 部署共享凭据。

### 环境变量

每个提供商需要其相应的API密钥环境变量：

```
# API密钥
GOOGLE_API_KEY=your_google_api_key        # Google Gemini模型必需
OPENAI_API_KEY=your_openai_api_key        # OpenAI模型必需
OPENROUTER_API_KEY=your_openrouter_api_key # OpenRouter模型必需

# OpenAI API基础URL配置
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # 可选，用于自定义OpenAI API端点

# 配置目录
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # 可选，用于自定义配置文件位置

# 授权模式
DEEPWIKI_AUTH_MODE=true  # 设置为 true 或 1 以启用授权模式
DEEPWIKI_AUTH_CODE=your_secret_code # 当 DEEPWIKI_AUTH_MODE 启用时所需的授权码
```
如果不使用ollama模式，那么需要配置OpenAI API密钥用于embeddings。其他密钥只有配置并使用使用对应提供商的模型时才需要。

## 授权模式

DeepWiki 可以配置为在授权模式下运行，在该模式下，生成 Wiki 需要有效的授权码。如果您想控制谁可以使用生成功能，这将非常有用。
限制使用前端页面生成wiki并保护已生成页面的缓存删除，但无法完全阻止直接访问 API 端点生成wiki。主要目的是为了保护管理员已生成的wiki页面，防止被访问者重新生成。

要启用授权模式，请设置以下环境变量：

- `DEEPWIKI_AUTH_MODE`: 将此设置为 `true` 或 `1`。启用后，前端将显示一个用于输入授权码的字段。
- `DEEPWIKI_AUTH_CODE`: 将此设置为所需的密钥。限制使用前端页面生成wiki并保护已生成页面的缓存删除，但无法完全阻止直接访问 API 端点生成wiki。

如果未设置 `DEEPWIKI_AUTH_MODE` 或将其设置为 `false`（或除 `true`/`1` 之外的任何其他值），则授权功能将被禁用，并且不需要任何代码。

### 配置文件

DeepWiki使用JSON配置文件管理系统的各个方面：

1. **`generator.json`**：文本生成模型配置
   - 定义可用的模型提供商（Google、OpenAI、OpenRouter、Ollama）
   - 指定每个提供商的默认和可用模型
   - 包含特定模型的参数，如temperature和top_p

2. **`embedder.json`**：嵌入模型和文本处理配置
   - 定义用于向量存储的嵌入模型
   - 包含用于RAG的检索器配置
   - 指定文档分块的文本分割器设置

3. **`repo.json`**：仓库处理配置
   - 包含排除特定文件和目录的文件过滤器
   - 定义仓库大小限制和处理规则

默认情况下，这些文件位于`api/config/`目录中。您可以使用`DEEPWIKI_CONFIG_DIR`环境变量自定义它们的位置。

### 面向服务提供商的自定义模型选择

自定义模型选择功能专为需要以下功能的服务提供者设计：

- 您可在您的组织内部为用户提供多种 AI 模型选择
- 您无需代码更改即可快速适应快速发展的 LLM 领域
- 您可支持预定义列表中没有的专业或微调模型

使用者可以通过从服务提供者预定义选项中选择或在前端界面中输入自定义模型标识符来实现其模型产品。

### 为企业私有渠道设计的基础 URL 配置

OpenAI 客户端的 base_url 配置主要为拥有私有 API 渠道的企业用户设计。此功能：

- 支持连接到私有或企业特定的 API 端点
- 允许组织使用自己的自托管或自定义部署的 LLM 服务
- 支持与第三方 OpenAI API 兼容服务的集成

**即将推出**：在未来的更新中，DeepWiki 将支持一种模式，用户需要在请求中提供自己的 API 密钥。这将允许拥有私有渠道的企业客户使用其现有的 API 安排，而不是与 DeepWiki 部署共享凭据。

## 🧩 使用 OpenAI 兼容的 Embedding 模型（如阿里巴巴 Qwen）

如果你希望使用 OpenAI 以外、但兼容 OpenAI 接口的 embedding 模型（如阿里巴巴 Qwen），请参考以下步骤：

1. 用 `api/config/embedder_openai_compatible.json` 的内容替换 `api/config/embedder.json`。
2. 在项目根目录的 `.env` 文件中，配置相应的环境变量，例如：
   ```
   OPENAI_API_KEY=你的_api_key
   OPENAI_BASE_URL=你的_openai_兼容接口地址
   ```
3. 程序会自动用环境变量的值替换 embedder.json 里的占位符。

这样即可无缝切换到 OpenAI 兼容的 embedding 服务，无需修改代码。



# DeepWiki-Open

![DeepWiki 横幅](screenshots/Deepwiki.png)

**DeepWiki**可以为任何GitHub、GitLab或BitBucket代码仓库自动创建美观、交互式的Wiki！只需输入仓库名称，DeepWiki将：

1. 分析代码结构
2. 生成全面的文档
3. 创建可视化图表解释一切如何运作
4. 将所有内容整理成易于导航的Wiki

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ 特点

- **即时文档**：几秒钟内将任何GitHub、GitLab或BitBucket仓库转换为Wiki
- **私有仓库支持**：使用个人访问令牌安全访问私有仓库
- **智能分析**：AI驱动的代码结构和关系理解
- **精美图表**：自动生成Mermaid图表可视化架构和数据流
- **简易导航**：简单、直观的界面探索Wiki
- **提问功能**：使用RAG驱动的AI与您的仓库聊天，获取准确答案
- **深度研究**：多轮研究过程，彻底调查复杂主题
- **多模型提供商**：支持Google Gemini、OpenAI、OpenRouter和本地Ollama模型

## 🚀 快速开始（超级简单！）

### 选项1：使用Docker

```bash
# 克隆仓库
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# 创建包含API密钥的.env文件
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# 可选：如果您想使用OpenRouter模型，添加OpenRouter API密钥
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# 使用Docker Compose运行
docker-compose up
```

(上述 Docker 命令以及 `docker-compose.yml` 配置会挂载您主机上的 `~/.adalflow` 目录到容器内的 `/root/.adalflow`。此路径用于存储：
- 克隆的仓库 (`~/.adalflow/repos/`)
- 仓库的嵌入和索引 (`~/.adalflow/databases/`)
- 缓存的已生成 Wiki 内容 (`~/.adalflow/wikicache/`)

这确保了即使容器停止或移除，您的数据也能持久保存。)

> 💡 **获取这些密钥的地方：**
> - 从[Google AI Studio](https://makersuite.google.com/app/apikey)获取Google API密钥
> - 从[OpenAI Platform](https://platform.openai.com/api-keys)获取OpenAI API密钥

### 选项2：手动设置（推荐）

#### 步骤1：设置API密钥

在项目根目录创建一个`.env`文件，包含以下密钥：

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# 可选：如果您想使用OpenRouter模型，添加此项
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### 步骤2：启动后端

```bash
# 安装Python依赖
pip install -r api/requirements.txt

# 启动API服务器
python -m api.main
```

#### 步骤3：启动前端

```bash
# 安装JavaScript依赖
npm install
# 或
yarn install

# 启动Web应用
npm run dev
# 或
yarn dev
```

#### 步骤4：使用DeepWiki！

1. 在浏览器中打开[http://localhost:3000](http://localhost:3000)
2. 输入GitHub、GitLab或Bitbucket仓库（如`https://github.com/openai/codex`、`https://github.com/microsoft/autogen`、`https://gitlab.com/gitlab-org/gitlab`或`https://bitbucket.org/redradish/atlassian_app_versions`）
3. 对于私有仓库，点击"+ 添加访问令牌"并输入您的GitHub或GitLab个人访问令牌
4. 点击"生成Wiki"，见证奇迹的发生！

## 🔍 工作原理

DeepWiki使用AI来：

1. 克隆并分析GitHub、GitLab或Bitbucket仓库（包括使用令牌认证的私有仓库）
2. 创建代码嵌入用于智能检索
3. 使用上下文感知AI生成文档（使用Google Gemini、OpenAI、OpenRouter或本地Ollama模型）
4. 创建可视化图表解释代码关系
5. 将所有内容组织成结构化Wiki
6. 通过提问功能实现与仓库的智能问答
7. 通过深度研究功能提供深入研究能力

```mermaid
graph TD
    A[用户输入GitHub/GitLab/Bitbucket仓库] --> AA{私有仓库?}
    AA -->|是| AB[添加访问令牌]
    AA -->|否| B[克隆仓库]
    AB --> B
    B --> C[分析代码结构]
    C --> D[创建代码嵌入]

    D --> M{选择模型提供商}
    M -->|Google Gemini| E1[使用Gemini生成]
    M -->|OpenAI| E2[使用OpenAI生成]
    M -->|OpenRouter| E3[使用OpenRouter生成]
    M -->|本地Ollama| E4[使用Ollama生成]

    E1 --> E[生成文档]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[创建可视化图表]
    E --> G[组织为Wiki]
    F --> G
    G --> H[交互式DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ 项目结构

```
deepwiki/
├── api/                  # 后端API服务器
│   ├── main.py           # API入口点
│   ├── api.py            # FastAPI实现
│   ├── rag.py        

## File Path: README.vi.md

# DeepWiki-Open

![DeepWiki Banner](screenshots/Deepwiki.png)

**Open DeepWiki** là 1 triển khai thay thế cho DeepWiki, tự động tạo ra các trang wiki cho bất kỳ Repository  nào trên GitHub, GitLab hoặc BitBucket! Chỉ cần nhập đường dẫn Repository, và DeepWiki sẽ:

1. Phân tích cấu trúc mã nguồn
2. Tạo tài liệu đầy đủ và chi tiết
3. Tạo sơ đồ trực quan để giải thích cách mọi thứ hoạt động
4. Sắp xếp tất cả documents thành một wiki dễ hiểu

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ Tính năng

- **Tạo Tài liệu tức thì**: Biến bất kỳ Repository GitHub, GitLab hoặc BitBucket nào thành wiki chỉ trong vài giây
- **Hỗ trợ Private Repository**: Truy cập Private Repository một cách an toàn với personal access tokens
- **Phân tích thông minh**: Hiểu cấu trúc và mối quan hệ của source codes nhờ AI
- **Tự động tạo Sơ đồ**: Tự động tạo sơ đồ Mermaid để trực quan hóa kiến trúc và luồng dữ liệu
- **Dễ dàng thao tác**:Giao diện wiki đơn giản, trực quan để khám phá
- **Trò chuyện với repository**: Trò chuyện với repo của bạn bằng AI (tích hợp RAG) để nhận câu trả lời chính xác
- **DeepResearch**:Quy trình Deep Research nhiều bước giúp phân tích kỹ lưỡng các chủ đề phức tạp
- **Hỗ trợ nhiều mô hình**: Hỗ trợ Google Gemini, OpenAI, OpenRouter, và  local Ollama models

## 🚀 Bắt đầu (Siêu dễ :))

### Option 1: Sử dụng Docker

```bash
# Clone repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Tạo .env file với API keys
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# Optional: Thêm OpenRouter API key nếu bạn muốn OpenRouter models
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# Run với Docker Compose
docker-compose up
```

> 💡 **Hướng dẫn lấy Keys**
> - Lấy Google API key từ [Google AI Studio](https://makersuite.google.com/app/apikey)
> - Lấy OpenAI API key từ [OpenAI Platform](https://platform.openai.com/api-keys)

### Option 2: Setup thủ công (Khuyên dùng)

#### Bước 1: Set Up API Keys

Tạo  `.env` file trong thư mục gốc của project với những keys vừa tạo:

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Thêm OpenRouter API key nếu bạn muốn OpenRouter models
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### Bước 2: Bắt đầu với Backend

```bash
# Cài đặt Python 

cung cấp LLM:

### Các nhà cung cấp và mô hình được hỗ trợ

- **Google**: Mặc định là `gemini-2.0-flash`, cũng hỗ trợ `gemini-1.5-flash`, `gemini-1.0-pro`, v.v.
- **OpenAI**: Mặc định là `gpt-4o`, cũng hỗ trợ `o4-mini`, v.v.
- **OpenRouter**: Truy cập nhiều mô hình qua một API thống nhất, bao gồm Claude, Llama, Mistral, v.v.
- **Ollama**: Hỗ trợ các mô hình mã nguồn mở chạy cục bộ như `llama3`

### Biến môi trường

Mỗi nhà cung cấp yêu cầu các biến môi trường API key tương ứng:

```
# API Keys
GOOGLE_API_KEY=google_api_key_của_bạn        # Bắt buộc cho các mô hình Google Gemini
OPENAI_API_KEY=openai_key_của_bạn            # Bắt buộc cho các mô hình OpenAI
OPENROUTER_API_KEY=openrouter_key_của_bạn    # Bắt buộc cho các mô hình OpenRouter

# Cấu hình URL cơ sở cho OpenAI API
OPENAI_BASE_URL=https://endpoint-tùy-chỉnh.com/v1  # Tùy chọn, cho các điểm cuối API OpenAI tùy chỉnh

# Thư mục cấu hình
DEEPWIKI_CONFIG_DIR=/đường/dẫn/đến/thư_mục/cấu_hình  # Tùy chọn, cho vị trí tệp cấu hình tùy chỉnh
```

### Tệp cấu hình

DeepWiki sử dụng các tệp cấu hình JSON để quản lý các khía cạnh khác nhau của hệ thống:

1. **`generator.json`**: Cấu hình cho các mô hình tạo văn bản
   - Xác định các nhà cung cấp mô hình có sẵn (Google, OpenAI, OpenRouter, Ollama)
   - Chỉ định các mô hình mặc định và có sẵn cho mỗi nhà cung cấp
   - Chứa các tham số đặc thù cho mô hình như temperature và top_p

2. **`embedder.json`**: Cấu hình cho mô hình embedding và xử lý văn bản
   - Xác định mô hình embedding cho lưu trữ vector
   - Chứa cấu hình bộ truy xuất cho RAG
   - Chỉ định cài đặt trình chia văn bản để phân đoạn tài liệu

3. **`repo.json`**: Cấu hình xử lý repository
   - Chứa bộ lọc tệp để loại trừ một số tệp và thư mục nhất định
   - Xác định giới hạn kích thước repository và quy tắc xử lý

Mặc định, các tệp này nằm trong thư mục `api/config/`. Bạn có thể tùy 

## File Path: api/websocket_wiki.py

examining the {repo_type} repository: {repo_url} ({repo_name}).
You are conducting a multi-turn Deep Research process to thoroughly investigate the specific topic in the user's query.
Your goal is to provide detailed, focused information EXCLUSIVELY about this topic.
IMPORTANT:You MUST respond in {language_name} language.
</role>

<guidelines>
- This is the first iteration of a multi-turn research process focused EXCLUSIVELY on the user's query
- Start your response with "## Research Plan"
- Outline your approach to investigating this specific topic
- If the topic is about a specific file or feature (like "Dockerfile"), focus ONLY on that file or feature
- Clearly state the specific topic you're researching to maintain focus throughout all iterations
- Identify the key aspects you'll need to research
- Provide initial findings based on the information available
- End with "## Next Steps" indicating what you'll investigate in the next iteration
- Do NOT provide a final conclusion yet - this is just the beginning of the research
- Do NOT include general repository information unless directly relevant to the query
- Focus EXCLUSIVELY on the specific topic being researched - do not drift to related topics
- Your research MUST directly address the original question
- NEVER respond with just "Continue the research" as an answer - always provide substantive research findings
- Remember that this topic will be maintained across all research iterations
</guidelines>

<style>
- Be concise but thorough
- Use markdown formatting to improve readability
- Cite specific files and code sections when relevant
</style>"""
            elif is_final_iteration:
                system_prompt = f"""<role>
You are an expert code analyst examining the {repo_type} repository: {repo_url} ({repo_name}).
You are in the final iteration of a Deep Research process focused EXCLUSIVELY on the latest user query.
Your goal is to synthesize all previous findings and provide a comprehensive conclusion that directly addresses this specific topic and ONLY this topic.
IMPORTANT:You MUST respond in {language_name} language.
</role>

<guidelines>
- This is the final iteration of the research process
- CAREFULLY review the entire conversation history to understand all previous findings
- Synthesize ALL findings from previous iterations into a comprehensive conclusion
- Start with "## Final Conclusion"
- Your conclusion 

## File Path: README.pt-br.md

A[Usuário insere repo GitHub/GitLab/Bitbucket] --> AA{Repo privado?}
    AA -->|Sim| AB[Adicionar token de acesso]
    AA -->|Não| B[Clonar Repositório]
    AB --> B
    B --> C[Analisar Estrutura do Código]
    C --> D[Criar Embeddings do Código]

    D --> M{Selecionar Provedor de Modelo}
    M -->|Google Gemini| E1[Gerar com Gemini]
    M -->|OpenAI| E2[Gerar com OpenAI]
    M -->|OpenRouter| E3[Gerar com OpenRouter]
    M -->|Ollama Local| E4[Gerar com Ollama]

    E1 --> E[Gerar Documentação]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[Criar Diagramas Visuais]
    E --> G[Organizar como Wiki]
    F --> G
    G --> H[DeepWiki Interativo]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ Estrutura do Projeto

```
deepwiki/
├── api/                  # Servidor API backend
│   ├── main.py           # Ponto de entrada da API
│   ├── api.py            # Implementação FastAPI
│   ├── rag.py            # Retrieval Augmented Generation
│   ├── data_pipeline.py  # Utilitários de processamento de dados
│   └── requirements.txt  # Dependências Python
│
├── src/                  # Aplicativo Next.js frontend
│   ├── app/              # Diretório do aplicativo Next.js
│   │   └── page.tsx      
<END_OF_CONTEXT>

<query>
分析这个项目的向量嵌入的实现方案和工作流程，涉及到的相关源文件
</query>

Assistant: 
2025-07-10 02:29:56,510 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 02:29:56,541 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 02:30:00,097 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:30:38,713 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
该项目的向量嵌入实现方案和工作流程主要涉及以下文件和组件：

### 1. **嵌入配置文件**
- **`api/config/embedder.json`**  
  定义了用于生成向量嵌入的模型及其相关参数，例如：
  - 使用的嵌入模型（如 OpenAI、Ollama 等）
  - 检索器（retriever）的配置
  - 文本分割器（text splitter）的设置

  示例配置：
  ```json
  {
    "model": {
      "provider": "openai",
      "name": "text-embedding-ada-002"
    },
    "retriever": {
      "similarity_threshold": 0.8,
      "max_results": 5
    },
    "text_splitter": {
      "chunk_size": 1000,
      "chunk_overlap": 200
    }
  }
  ```

### 2. **数据处理流水线**
- **`api/data_pipeline.py`**  
  负责处理代码文件的读取、文本分割和嵌入生成。关键函数包括：
  - `load_documents()`: 读取仓库中的文件，并生成文档对象（包含路径、类型、token 数量等）。
  - `prepare_data_pipeline()`: 构建数据转换流水线，根据配置选择是否使用 Ollama 嵌入模型。
  - `generate_embeddings()`: 调用嵌入模型 API 为文本生成向量。

  示例代码片段：
  ```python
  def generate_embeddings(texts, is_ollama_embedder: bool = None):
      if is_ollama_embedder:
          # 使用 Ollama 模型生成嵌入
          pass
      else:
          # 使用 OpenAI 或其他模型生成嵌入
          pass
  ```

### 3. **检索增强生成 (RAG)**
- **`api/rag.py`**  
  实现了基于向量嵌入的检索增强生成功能。关键逻辑包括：
  - 根据用户查询生成嵌入向量。
  - 在嵌入数据库中搜索相似度最高的文档片段。
  - 将检索到的文档内容与查询结合，生成最终的回答。

  示例代码片段：
  ```python
  def retrieve_and_generate(query, embeddings, retriever_config):
      query_embedding = generate_embeddings([query])
      results = search_similar(embeddings, query_embedding, retriever_config)
      context = "\n".join([doc["content"] for doc in results])
      return generate_response(context + "\n\n" + query)
  ```

### 4. **嵌入数据库**
- **`~/.adalflow/databases/`**  
  该目录存储了代码文件的嵌入向量和索引信息。每个仓库的嵌入数据以结构化格式保存，以便快速检索。

### 5. **模型选择与配置**
- **`api/websocket_wiki.py`**  
  根据用户选择的模型提供商（如 Google、OpenAI、OpenRouter、Ollama）动态加载对应的嵌入模型。环境变量如 `OPENAI_API_KEY` 和 `OPENROUTER_API_KEY` 用于指定模型的访问权限。

### 工作流程总结
1. 用户输入一个代码仓库地址。
2. 项目克隆并读取代码文件，生成文档对象。
3. 根据配置选择嵌入模型，对文档内容进行向量化处理。
4. 将生成的嵌入向量和原始文档存储到数据库中。
5. 当用户提出查询时，系统根据查询生成嵌入向量，并在数据库中检索最相关的文档片段。
6. 使用 RAG 技术将检索到的文档内容与查询结合，生成最终回答。

Sources: [api/config/embedder.json](), [api/data_pipeline.py](), [api/rag.py](), [api/websocket_wiki.py](), [README.zh.md:100-120]()
2025-07-10 02:32:15,558 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 19 tokens
2025-07-10 02:32:15,589 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:32:15,590 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 02:32:15,590 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:32:15,590 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:32:15,623 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:32:15,631 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['input_str', 'conversation_history', 'contexts', 'output_format_str', 'system_prompt']
2025-07-10 02:32:15,631 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 02:32:15,631 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/AsyncFuncAI/deepwiki-open...
2025-07-10 02:32:15,632 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: AsyncFuncAI_deepwiki-open
2025-07-10 02:32:15,632 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/AsyncFuncAI_deepwiki-open. Using existing repository.
2025-07-10 02:32:15,632 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/AsyncFuncAI_deepwiki-open', 'save_db_file': '/root/.adalflow/databases/AsyncFuncAI_deepwiki-open.pkl'}
2025-07-10 02:32:15,632 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 02:32:15,642 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 02:32:15,642 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:32:15,643 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:32:15,643 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:32:15,701 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 02:32:15,701 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 02:32:15,701 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['version: \'3.8\'\n\nservices:\n  deepwiki:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - "${PORT:-8001}:${PORT:-8001}"  # API port\n      - "3000:3000"  # Next.js port\n    env_file:\n      - .env\n    environment:\n      - PORT=${PORT:-8001}\n      - NODE_ENV=production\n      - SERVER_BASE_URL=http://localhost:${PORT:-8001}\n      - LOG_LEVEL=${LOG_LEVEL:-INFO}\n      - LOG_FILE_PATH=${LOG_FILE_PATH:-api/logs/application.log}\n    volumes:\n      - ~/.adalflow:/root/.adalflow      # Persist repository and embedding data\n      - ./api/logs:/app/api/logs          # Persist log files across container restarts\n    # Resource limits for docker-compose up (not Swarm mode)\n    mem_limit: 6g\n    mem_reservation: 2g\n    # Health check configuration\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8001}/health"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 30s\n']}}}
2025-07-10 02:32:15,777 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 821 documents from existing database
2025-07-10 02:32:15,777 - INFO - api.rag - rag.py:421 - Loaded 821 documents for retrieval
2025-07-10 02:32:15,778 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 821 documents)
2025-07-10 02:32:15,778 - INFO - api.rag - rag.py:384 - Embedding validation complete: 821/821 documents have valid embeddings
2025-07-10 02:32:15,778 - INFO - api.rag - rag.py:429 - Using 821 documents with valid embeddings for retrieval
2025-07-10 02:32:15,778 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 02:32:15,788 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 821 chunks
2025-07-10 02:32:15,788 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 02:32:15,788 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/AsyncFuncAI/deepwiki-open
2025-07-10 02:32:15,789 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['嵌入数据库是用什么技术和组件实现的']}
2025-07-10 02:32:16,239 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 02:32:16,242 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 02:32:16,243 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 02:32:16,243 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 02:32:16,243 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/AsyncFuncAI/deepwiki-open (deepwiki-open).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: api/data_pipeline.py

e:
        raise ValueError(f"Failed to get file content: {str(e)}")


def get_file_content(repo_url: str, file_path: str, type: str = "github", access_token: str = None) -> str:
    """
    Retrieves the content of a file from a Git repository (GitHub or GitLab).

    Args:
        repo_url (str): The URL of the repository
        file_path (str): The path to the file within the repository
        access_token (str, optional): Access token for private repositories

    Returns:
        str: The content of the file as a string

    Raises:
        ValueError: If the file cannot be fetched or if the URL is not valid
    """
    if type == "github":
        return get_github_file_content(repo_url, file_path, access_token)
    elif type == "gitlab":
        return get_gitlab_file_content(repo_url, file_path, access_token)
    elif type == "bitbucket":
        return get_bitbucket_file_content(repo_url, file_path, access_token)
    else:
        raise ValueError("Unsupported repository URL. Only GitHub and GitLab are supported.")

class DatabaseManager:
    """
    Manages the creation, loading, transformation, and persistence of LocalDB instances.
    """

    def __init__(self):
        self.db = None
        self.repo_url_or_path = None
        self.repo_paths = None

    def prepare_database(self, repo_url_or_path: str, type: str = "github", access_token: str = None, is_ollama_embedder: bool = None,
                       excluded_dirs: List[str] = None, excluded_files: List[str] = None,
                      

       self.db = None
        self.repo_url_or_path = None
        self.repo_paths = None

    def prepare_database(self, repo_url_or_path: str, type: str = "github", access_token: str = None, is_ollama_embedder: bool = None,
                       excluded_dirs: List[str] = None, excluded_files: List[str] = None,
                       included_dirs: List[str] = None, included_files: List[str] = None) -> List[Document]:
        """
        Create a new database from the repository.

        Args:
            repo_url_or_path (str): The URL or local path of the repository
            access_token (str, optional): Access token for private repositories
            is_ollama_embedder (bool, optional): Whether to use Ollama for embedding.
                                               If None, will be determined from configuration.
            excluded_dirs (List[str], optional): List of directories to exclude from processing
            excluded_files (List[str], optional): List of file patterns to exclude from processing
            included_dirs (List[str], optional): List of directories to include exclusively
            included_files (List[str], optional): List of file patterns to include exclusively

        Returns:
     

   embedder_transformer = ToEmbeddings(
            embedder=embedder, batch_size=batch_size
        )

    data_transformer = adal.Sequential(
        splitter, embedder_transformer
    )  # sequential will chain together splitter and embedder
    return data_transformer

def transform_documents_and_save_to_db(
    documents: List[Document], db_path: str, is_ollama_embedder: bool = None
) -> LocalDB:
    """
    Transforms a list of documents and saves them to a local database.

    Args:
        documents (list): A list of `Document` objects.
        db_path (str): The path to the local database file.
        is_ollama_embedder (bool, optional): Whether to use Ollama for embedding.
                                           If None, will be determined from configuration.
    """
    # Get the data transformer
    data_transformer = prepare_data_pipeline(is_ollama_embedder)

    # Save the documents to a local database
    db = LocalDB()
    db.register_transformer(transformer=data_transformer, key="split_and_embed")
    db.load(documents)
    db.transform(key="split_and_embed")
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    db.save_state(filepath=db_path)
    return db

def get_github_file_content(repo_url: str, file_path: str, access_token: str = None) -> str:
    """
    Retrieves the content of a file from a GitHub repository using the GitHub API.
    Supports both public GitHub (github.com) and GitHub Enterprise (custom domains).
    
    Args:
        repo_url (str): The URL of the GitHub repository 
                       (e.g., "https://github.com/username/repo" or "https://github.company.com/username/repo")
   

    """
        Prepare the retriever for a repository.
        This is a compatibility method for the isolated API.

        Args:
            repo_url_or_path (str): The URL or local path of the repository
            access_token (str, optional): Access token for private repositories

        Returns:
            List[Document]: List of Document objects
        """
        return self.prepare_database(repo_url_or_path, type, access_token)


        logger.error(f"Error reading {file_path}: {e}")

    logger.info(f"Found {len(documents)} documents")
    return documents

def prepare_data_pipeline(is_ollama_embedder: bool = None):
    """
    Creates and returns the data transformation pipeline.

    Args:
        is_ollama_embedder (bool, optional): Whether to use Ollama for embedding.
                                           If None, will be determined from configuration.

    Returns:
        adal.Sequential: The data transformation pipeline
    """
    from api.config import get_embedder_config, is_ollama_embedder as check_ollama

    # Determine if using Ollama embedder if not specified
    if is_ollama_embedder is None:
        is_ollama_embedder = check_ollama()

    splitter = TextSplitter(**configs["text_splitter"])
    embedder_config = get_embedder_config()

    embedder = get_embedder()

    if is_ollama_embedder:
        # Use Ollama document processor for single-document processing
        embedder_transformer = OllamaDocumentProcessor(embedder=embedder)
    else:
        # Use batch processing for other embedders
        batch_size = embedder_config.get("batch_size", 500)
        embedder_transformer = ToEmbeddings(
            embedder=embedder, batch_size=batch_size
        )

    data_transformer = adal.Sequential(
        splitter, embedder_transformer
    )  # sequential will chain together splitter and embedder
    return data_transformer

def transform_documents_and_save_to_db(
    documents: List[Document], db_path: str, is_ollama_embedder: bool = None
) -> LocalDB:
    """
    Transforms a list of documents and saves them to a local database.

    Args:
    

   except Exception as e:
                logger.error(f"Error loading existing database: {e}")
                # Continue to create a new database

        # prepare the database
        logger.info("Creating new database...")
        documents = read_all_documents(
            self.repo_paths["save_repo_dir"],
            is_ollama_embedder=is_ollama_embedder,
            excluded_dirs=excluded_dirs,
            excluded_files=excluded_files,
            included_dirs=included_dirs,
            included_files=included_files
        )
        self.db = transform_documents_and_save_to_db(
            documents, self.repo_paths["save_db_file"], is_ollama_embedder=is_ollama_embedder
        )
        logger.info(f"Total documents: {len(documents)}")
        transformed_docs = self.db.get_transformed_data(key="split_and_embed")
        logger.info(f"Total transformed documents: {len(transformed_docs)}")
        return transformed_docs

    def prepare_retriever(self, repo_url_or_path: str, type: str = "github", access_token: str = None):
        """
        Prepare the retriever for a repository.
        This is a compatibility method for the isolated API.

        Args:
            repo_url_or_path (str): The URL or local path of the repository
            access_token (str, optional): Access token for private repositories

        Returns:
           

## File Path: src/app/[owner]/[repo]/page.tsx

accessed, and managed (e.g., database schema, data pipelines, state management).
- Frontend Components (UI elements, if applicable.)
- Backend Systems (server-side components)
- Model Integration (AI model connections)
- Deployment/Infrastructure (how to deploy, what's the infrastructure like)
- Extensibility and Customization: If the project architecture supports it, explain how to extend or customize its functionality (e.g., plugins, theming, custom modules, hooks).

Each section should contain relevant pages. For example, the "Frontend Components" section might include pages for "Home Page", "Repository Wiki Page", "Ask Component", etc.

Return your analysis in the following XML format:

<wiki_structure>
  <title>[Overall title for the wiki]</title>
  <description>[Brief description of the repository]</description>
  <sections>
    <section id="section-1">
      <title>[Section title]</title>
      <pages>
        <page_ref>page-1</page_ref>
        <page_ref>page-2</page_ref>
      </pages>
      <subsections>
        <section_ref>section-2</section_ref>
      </subsections>
    </section>
    <!-- More sections as needed -->
  </sections>
  <pages>
    <page id="page-1">
      <title>[Page title]</title>
      <description>[Brief description of what this page will cover]</description>
      <importance>high|medium|low</importance>
      <relevant_files>
        <file_path>[Path to a relevant file]</file_path>
        <!-- More file paths as needed -->
      </relevant_files>
      <related_pages>
        <related>page-2</related>
        <!-- More related page IDs as needed -->
      </related_pages>
      <parent_section>section-1</parent_section>
    </page>
    <!-- More pages as needed -->
  </pages>
</wiki_structure>
` : `
Return your analysis in the following XML format:

<wiki_structure>
  <title>[Overall title for the wiki]</title>
  <description>[Brief description of the repository]</description>
  <pages>
    <page id="page-1">
      <title>[Page title]</title>
      <description>[Brief description of what this page will cover]</description>
 

         ws.onopen = () => {
            console.log('WebSocket connection established for wiki structure');
            // Send the request as JSON
            ws.send(JSON.stringify(requestBody));
            resolve();
          };

          ws.onerror = (error) => {
            console.error('WebSocket error:', error);
            reject(new Error('WebSocket connection failed'));
          };

          // If the connection doesn't open within 5 seconds, fall back to HTTP
          const timeout = setTimeout(() => {
            reject(new Error('WebSocket connection timeout'));
          }, 5000);

          // Clear the timeout if the connection opens successfully
          ws.onopen = () => {
            clearTimeout(timeout);
            console.log('WebSocket connection established for wiki structure');
            // Send the request as JSON
            ws.send(JSON.stringify(requestBody));
            resolve();
          };
        });

        // Create a promise that resolves when the WebSocket response is complete
        await new Promise<void>((resolve, reject) => {
 

configuration, API keys, and try again. If the issue persists, consider switching to a different embedding provider in the model settings.'
              ) : (
                messages.repoPage?.errorMessageDefault || 'Please check that your repository exists and is public. Valid formats are "owner/repo", "https://github.com/owner/repo", "https://gitlab.com/owner/repo", "https://bitbucket.org/owner/repo", or local folder paths like "C:\\path\\to\\folder" or "/path/to/folder".'
              )}
            </p>
            <div className="mt-5">
              <Link
                href="/"
                className="btn-japanese px-5 py-2 inline-flex items-center gap-1.5"
              >
                <FaHome className="text-sm" />
                {messages.repoPage?.backToHome || 'Back to Home'}
              </Link>
            </div>
          </div>
        ) : wikiStructure ? (
          <div className="h-full overflow-y-auto flex flex-col lg:flex-row gap-4 w-full overflow-hidden bg-[var(--card-bg)] rounded-lg shadow-custom card-japanese">
            {/* Wiki Navigation */}
            <div className="h-full w-full lg:w-[280px] xl:w-[320px] flex-shrink-0 bg-[var(--background)]/50 rounded-lg rounded-r-none p-5 border-b lg:border-b-0 lg:border-r border-[var(--border-color)] overflow-y-auto">
              <h3 className="text-lg font-bold text-[var(--foreground)] mb-3 font-serif">{wikiStructure.title}</h3>
 

the purpose, scope, and high-level overview of "${page.title}" within the context of the overall project. If relevant, and if information is available in the provided files, link to other potential wiki pages using the format \`[Link Text](#page-anchor-or-id)\`.

2.  **Detailed Sections:** Break down "${page.title}" into logical sections using H2 (\`##\`) and H3 (\`###\`) Markdown headings. For each section:
    *   Explain the architecture, components, data flow, or logic relevant to the section's focus, as evidenced in the source files.
    *   Identify key functions, classes, data structures, API endpoints, or configuration elements pertinent to that section.

3.  **Mermaid Diagrams:**
    *   EXTENSIVELY use Mermaid diagrams (e.g., \`flowchart TD\`, \`sequenceDiagram\`, \`classDiagram\`, \`erDiagram\`, \`graph TD\`) to visually represent architectures, flows, relationships, and schemas found in the source files.
    *   Ensure diagrams are accurate and directly derived from information in the \`[RELEVANT_SOURCE_FILES]\`.
    *   Provide a brief explanation before or after each diagram to give context.
    *   CRITICAL: All diagrams MUST follow strict vertical orientation:
       - Use "graph TD" (top-down) directive for flow diagrams
       - NEVER use "graph LR" (left-right)
       - Maximum node width should be 3-4 words
       - For sequence diagrams:
         - Start with "sequenceDiagram" directive on its own line
         - Define ALL participants at the beginning
         - Use descriptive but concise participant names
         - Use the correct arrow types:
           - ->> for request/asynchronous messages
           - -->> for response messages
           - -x for failed messages
       

          console.error('Error saving wiki data to server cache:', response.status, await response.text());
            }
          } catch (error) {
            console.error('Error saving to server cache:', error);
          }
        }
      }
    };

    saveCache();
  }, [isLoading, error, wikiStructure, generatedPages, effectiveRepoInfo.owner, effectiveRepoInfo.repo, effectiveRepoInfo.type, effectiveRepoInfo.repoUrl, repoUrl, language, isComprehensiveView]);

  const handlePageSelect = (pageId: string) => {
    if (currentPageId != pageId) {
      setCurrentPageId(pageId)
    }
  };

  const [isModelSelectionModalOpen, setIsModelSelectionModalOpen] = useState(false);

  return (
    <div className="h-screen paper-texture p-4 md:p-8 flex flex-col">
      <style>{wikiStyles}</style>

      <header className="max-w-[90%] xl:max-w-[1400px] mx-auto mb-8 h-fit w-full">
        <div className="flex flex-col md:flex-row md:items-center md:justify-between gap-4">
          <div className="flex items-center gap-4">
            <Link href="/" className="text-[var(--accent-primary)] hover:text-[var(--highlight)] flex items-center gap-1.5 transition-colors border-b border-[var(--border-color)] hover:border-[var(--accent-primary)] pb-0.5">
              <FaHome /> {messages.repoPage?.home || 'Home'}
            </Link>
          </div>
        </div>
      </header>

      <main className="flex-1 max-w-[90%] xl:max-w-[1400px] mx-auto overflow-y-auto">
        {isLoading ? (
          <div className="flex flex-col items-center justify-center p-8 bg-[var(--card-bg)] rounded-lg shadow-custom card-japanese">
            <div className="relative mb-6">
              <div className="absolute -inset-4 bg-[var(--accent-primary)]/10 rounded-full blur-md animate-pulse"></div>
 

## File Path: next.config.ts

import type { NextConfig } from "next";

const TARGET_SERVER_BASE_URL = process.env.SERVER_BASE_URL || 'http://localhost:8001';

const nextConfig: NextConfig = {
  /* config options here */
  output: 'standalone',
  // Optimize build for Docker
  experimental: {
    optimizePackageImports: ['@mermaid-js/mermaid', 'react-syntax-highlighter'],
  },
  // Reduce memory usage during build
  webpack: (config, { isServer }) => {
    if (!isServer) {
      config.resolve.fallback = {
        ...config.resolve.fallback,
        fs: false,
      };
    }
    // Optimize bundle size
    config.optimization = {
      ...config.optimization,
      splitChunks: {
        chunks: 'all',
        cacheGroups: {
          vendor: {
            test: /[\\/]node_modules[\\/]/,
            name: 'vendors',
            chunks: 'all',
          },
        },
      },
    };
    return config;
  },
  async rewrites() {
    return [
      {
        source: '/api/wiki_cache/:path*',
        destination: `${TARGET_SERVER_BASE_URL}/api/wiki_cache/:path*`,
      },
      {
        source: '/export/wiki/:path*',
        destination: `${TARGET_SERVER_BASE_URL}/export/wiki/:path*`,
      },
      {
        source: '/api/wiki_cache',
        destination: `${TARGET_SERVER_BASE_URL}/api/wiki_cache`,
      },
      {
        source: '/local_repo/structure',
        destination: `${TARGET_SERVER_BASE_URL}/local_repo/structure`,
 

'/api/wiki_cache/:path*',
        destination: `${TARGET_SERVER_BASE_URL}/api/wiki_cache/:path*`,
      },
      {
        source: '/export/wiki/:path*',
        destination: `${TARGET_SERVER_BASE_URL}/export/wiki/:path*`,
      },
      {
        source: '/api/wiki_cache',
        destination: `${TARGET_SERVER_BASE_URL}/api/wiki_cache`,
      },
      {
        source: '/local_repo/structure',
        destination: `${TARGET_SERVER_BASE_URL}/local_repo/structure`,
      },
      {
        source: '/api/auth/status',
        destination: `${TARGET_SERVER_BASE_URL}/auth/status`,
      },
      {
        source: '/api/auth/validate',
        destination: `${TARGET_SERVER_BASE_URL}/auth/validate`,
      },
      {
        source: '/api/lang/config',
        destination: `${TARGET_SERVER_BASE_URL}/lang/config`,
      },
    ];
  },
};

export default nextConfig;


## File Path: README.es.md

reglas de procesamiento

Por defecto, estos archivos se encuentran en el directorio `api/config/`. Puedes personalizar su ubicación usando la variable de entorno `DEEPWIKI_CONFIG_DIR`.

### Selección de Modelos Personalizados para Proveedores de Servicios

La función de selección de modelos personalizados está diseñada específicamente para proveedores de servicios que necesitan:

- Puede ofrecer a los usuarios dentro de su organización una selección de diferentes modelos de IA
- Puede adaptarse rápidamente al panorama de LLM en rápida evolución sin cambios de código
- Puede soportar modelos especializados o ajustados que no están en la lista predefinida

Usted puede implementar sus ofertas de modelos seleccionando entre las opciones predefinidas o ingresando identificadores de modelos personalizados en la interfaz frontend.

### Configuración de URL Base para Canales Privados Empresariales

La configuración de base_url del Cliente OpenAI está diseñada principalmente para usuarios empresariales con canales API privados. Esta función:

- Permite la conexión a endpoints API privados o específicos de la empresa
- Permite a las organizaciones usar sus propios servicios LLM auto-alojados o desplegados a medida
- Soporta integración con servicios de terceros compatibles con la API de OpenAI

**Próximamente**: En futuras actualizaciones, DeepWiki soportará un modo donde los usuarios deberán proporcionar sus propias claves API en las solicitudes. Esto permitirá a los clientes empresariales con canales privados utilizar sus disposiciones API existentes sin compartir credenciales con el despliegue de DeepWiki.

## 🧩 Uso de modelos de embedding compatibles con OpenAI (por ejemplo, Alibaba Qwen)

Si deseas usar modelos de embedding compatibles con la API de OpenAI (como Alibaba Qwen), sigue estos pasos:

1. Sustituye el contenido de `api/config/embedder.json` por el de `api/config/embedder_openai_compatible.json`.
2. En el archivo `.env` de la raíz del proyecto, configura las variables de entorno necesarias, por ejemplo:
   ```
   OPENAI_API_KEY=tu_api_key
   OPENAI_BASE_URL=tu_endpoint_compatible_openai
   ```
3. El programa sustituirá automáticamente los placeholders de embedder.json por los valores de tus variables de entorno.

Así puedes cambiar fácilmente a cualquier servicio de embedding compatible con OpenAI sin modificar el código.

## 🤖 Funciones de Preguntas e Investigación Profunda

### Función de Preguntas

La función de Preguntas te permite chatear con tu repositorio usando Generación Aumentada por Recuperación (RAG):

- **Respuestas Conscientes del Contexto**: Obtén respuestas precisas basadas en el código real 

Comunes
1. **Reiniciar ambos servidores**: A veces un simple reinicio soluciona la mayoría de los problemas
2. **Revisar los registros de la consola**: Abre las herramientas de desarrollo del navegador para ver cualquier error de JavaScript
3. **Revisar los registros de la API**: Mira la terminal donde se ejecuta la API para ver errores de Python

## 🤝 Contribuir

¡Las contribuciones son bienvenidas! Siéntete libre de:
- Abrir issues para bugs o solicitudes de funciones
- Enviar pull requests para mejorar el código
- Compartir tus comentarios e ideas

## 📄 Licencia

Este proyecto está licenciado bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para más detalles.

## ⭐ Historial de Estrellas

[![Gráfico de Historial de Estrellas](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)



## File Path: README.zh.md

**RAG驱动**：系统检索相关代码片段，提供有根据的响应
- **实时流式传输**：实时查看生成的响应，获得更交互式的体验
- **对话历史**：系统在问题之间保持上下文，实现更连贯的交互

### 深度研究功能

深度研究通过多轮研究过程将仓库分析提升到新水平：

- **深入调查**：通过多次研究迭代彻底探索复杂主题
- **结构化过程**：遵循清晰的研究计划，包含更新和全面结论
- **自动继续**：AI自动继续研究直到达成结论（最多5次迭代）
- **研究阶段**：
  1. **研究计划**：概述方法和初步发现
  2. **研究更新**：在前一轮迭代基础上增加新见解
  3. **最终结论**：基于所有迭代提供全面答案

要使用深度研究，只需在提交问题前在提问界面中切换"深度研究"开关。

## 📱 截图

![DeepWiki主界面](screenshots/Interface.png)
*DeepWiki的主界面*

![私有仓库支持](screenshots/privaterepo.png)
*使用个人访问令牌访问私有仓库*

![深度研究功能](screenshots/DeepResearch.png)
*深度研究为复杂主题进行多轮调查*

### 演示视频

[![DeepWiki演示视频](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*观看DeepWiki实际操作！*

## ❓ 故障排除

### API密钥问题
- **"缺少环境变量"**：确保您的`.env`文件位于项目根目录并包含所需的API密钥
- **"API密钥无效"**：检查您是否正确复制了完整密钥，没有多余空格
- **"OpenRouter API错误"**：验证您的OpenRouter API密钥有效且有足够的额度

### 连接问题
- **"无法连接到API服务器"**：确保API服务器在端口8001上运行
- **"CORS错误"**：API配置为允许所有来源，但如果您遇到问题，请尝试在同一台机器上运行前端和后端

### 生成问题
- **"生成Wiki时出错"**：对于非常大的仓库，请先尝试较小的仓库
- **"无效的仓库格式"**：确保您使用有效的GitHub、GitLab或Bitbucket URL格式
- **"无法获取仓库结构"**：对于私有仓库，确保您输入了具有适当权限的有效个人访问令牌
- **"图表渲染错误"**：应用程序将自动尝试修复损坏的图表

### 常见解决方案
1. **重启两个服务器**：有时简单的重启可以解决大多数问题
2. **检查控制台日志**：打开浏览器开发者工具查看任何JavaScript错误
3. **检查API日志**：查看运行API的终端中的Python错误

## 🤝 贡献

欢迎贡献！随时：
- 为bug或功能请求开issue
- 提交pull request改进代码
- 分享您的反馈和想法

## 📄 许可证

本项目根据MIT许可证授权 - 详情请参阅[LICENSE](LICENSE)文件。

## ⭐ 星标历史

[![星标历史图表](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)

## 🤖 基于提供者的模型选择系统

DeepWiki 现在实现了灵活的基于提供者的模型选择系统，支持多种 LLM 提供商：

### 支持的提供商和模型

- **Google**: 默认使用 `gemini-2.0-flash`，还支持 `gemini-1.5-flash`、`gemini-1.0-pro` 等
- **OpenAI**: 默认使用 `gpt-4o`，还支持 `o4-mini` 等
- **OpenRouter**: 通过统一 API 访问多种模型，包括 Claude、Llama、Mistral 等
- **Ollama**: 支持本地运行的开源模型，如 `llama3`

### 环境变量

每个提供商需要相应的 API 密钥环境变量：

```
# API 密钥
GOOGLE_API_KEY=你的谷歌API密钥        # 使用 Google Gemini 模型必需
OPENAI_API_KEY=你的OpenAI密钥        # 使用 OpenAI 模型必需
OPENROUTER_API_KEY=你的OpenRouter密钥 # 使用 OpenRouter 模型必需

# OpenAI API 基础 URL 配置
OPENAI_BASE_URL=https://自定义API端点.com/v1  # 可选，用于自定义 OpenAI API 端点
```

### 为服务提供者设计的自定义模型选择

自定义模型选择功能专为需要以下功能的服务提供者设计：

- 您可在您的组织内部为用户提供多种 AI 模型选择
- 您无需代码更改即可快速适应快速发展的 LLM 领域
- 您可支持预定义列表中没有的专业或微调模型

使用者可以通过从服务提供者预定义选项中选择或在前端界面中输入自定义模型标识符来实现其模型产品。

### 为企业私有渠道设计的基础 URL 配置

OpenAI 客户端的 base_url 配置主要为拥有私有 API 渠道的企业用户设计。此功能：

- 支持连接到私有或企业特定的 API 端点
- 允许组织使用自己的自托管或自定义部署的 LLM 服务
- 支持与第三方 OpenAI API 兼容服务的集成

**即将推出**：在未来的更新中，DeepWiki 将支持一种模式，用户需要在请求中提供自己的 API 密钥。这将允许拥有私有渠道的企业客户使用其现有的 API 安排，而不是与 DeepWiki 部署共享凭据。

### 环境变量

每个提供商需要其相应的API密钥环境变量：

```
# API密钥
GOOGLE_API_KEY=your_google_api_key        # Google Gemini模型必需
OPENAI_API_KEY=your_openai_api_key        # OpenAI模型必需
OPENROUTER_API_KEY=your_openrouter_api_key # OpenRouter模型必需

# OpenAI API基础URL配置
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # 可选，用于自定义OpenAI API端点

# 配置目录
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # 可选，用于自定义配置文件位置

# 授权模式
DEEPWIKI_AUTH_MODE=true  # 设置为 true 或 1 以启用授权模式
DEEPWIKI_AUTH_CODE=your_secret_code # 当 DEEPWIKI_AUTH_MODE 启用时所需的授权码
```
如果不使用ollama模式，那么需要配置OpenAI API密钥用于embeddings。其他密钥只有配置并使用使用对应提供商的模型时才需要。

## 授权模式

DeepWiki 可以配置为在授权模式下运行，在该模式下，生成 Wiki 需要有效的授权码。如果您想控制谁可以使用生成功能，这将非常有用。
限制使用前端页面生成wiki并保护已生成页面的缓存删除，但无法完全阻止直接访问 API 端点生成wiki。主要目的是为了保护管理员已生成的wiki页面，防止被访问者重新生成。

要启用授权模式，请设置以下环境变量：

- `DEEPWIKI_AUTH_MODE`: 将此设置为 `true` 或 `1`。启用后，前端将显示一个用于输入授权码的字段。
- `DEEPWIKI_AUTH_CODE`: 将此设置为所需的密钥。限制使用前端页面生成wiki并保护已生成页面的缓存删除，但无法完全阻止直接访问 API 端点生成wiki。

如果未设置 `DEEPWIKI_AUTH_MODE` 或将其设置为 `false`（或除 `true`/`1` 之外的任何其他值），则授权功能将被禁用，并且不需要任何代码。

### 配置文件

DeepWiki使用JSON配置文件管理系统的各个方面：

1. **`generator.json`**：文本生成模型配置
   - 定义可用的模型提供商（Google、OpenAI、OpenRouter、Ollama）
   - 指定每个提供商的默认和可用模型
   - 包含特定模型的参数，如temperature和top_p

2. **`embedder.json`**：嵌入模型和文本处理配置
   - 定义用于向量存储的嵌入模型
   - 包含用于RAG的检索器配置
   - 指定文档分块的文本分割器设置

3. **`repo.json`**：仓库处理配置
   - 包含排除特定文件和目录的文件过滤器
   - 定义仓库大小限制和处理规则

默认情况下，这些文件位于`api/config/`目录中。您可以使用`DEEPWIKI_CONFIG_DIR`环境变量自定义它们的位置。

### 面向服务提供商的自定义模型选择

自定义模型选择功能专为需要以下功能的服务提供者设计：

- 您可在您的组织内部为用户提供多种 AI 模型选择
- 您无需代码更改即可快速适应快速发展的 LLM 领域
- 您可支持预定义列表中没有的专业或微调模型

使用者可以通过从服务提供者预定义选项中选择或在前端界面中输入自定义模型标识符来实现其模型产品。

### 为企业私有渠道设计的基础 URL 配置

OpenAI 客户端的 base_url 配置主要为拥有私有 API 渠道的企业用户设计。此功能：

- 支持连接到私有或企业特定的 API 端点
- 允许组织使用自己的自托管或自定义部署的 LLM 服务
- 支持与第三方 OpenAI API 兼容服务的集成

**即将推出**：在未来的更新中，DeepWiki 将支持一种模式，用户需要在请求中提供自己的 API 密钥。这将允许拥有私有渠道的企业客户使用其现有的 API 安排，而不是与 DeepWiki 部署共享凭据。

## 🧩 使用 OpenAI 兼容的 Embedding 模型（如阿里巴巴 Qwen）

如果你希望使用 OpenAI 以外、但兼容 OpenAI 接口的 embedding 模型（如阿里巴巴 Qwen），请参考以下步骤：

1. 用 `api/config/embedder_openai_compatible.json` 的内容替换 `api/config/embedder.json`。
2. 在项目根目录的 `.env` 文件中，配置相应的环境变量，例如：
   ```
   OPENAI_API_KEY=你的_api_key
   OPENAI_BASE_URL=你的_openai_兼容接口地址
   ```
3. 程序会自动用环境变量的值替换 embedder.json 里的占位符。

这样即可无缝切换到 OpenAI 兼容的 embedding 服务，无需修改代码。



# DeepWiki-Open

![DeepWiki 横幅](screenshots/Deepwiki.png)

**DeepWiki**可以为任何GitHub、GitLab或BitBucket代码仓库自动创建美观、交互式的Wiki！只需输入仓库名称，DeepWiki将：

1. 分析代码结构
2. 生成全面的文档
3. 创建可视化图表解释一切如何运作
4. 将所有内容整理成易于导航的Wiki

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)

[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [简体中文](./README.zh.md) | [日本語](./README.ja.md) | [Español](./README.es.md) | [한국어](./README.kr.md) | [Tiếng Việt](./README.vi.md) | [Português Brasileiro](./README.pt-br.md)

## ✨ 特点

- **即时文档**：几秒钟内将任何GitHub、GitLab或BitBucket仓库转换为Wiki
- **私有仓库支持**：使用个人访问令牌安全访问私有仓库
- **智能分析**：AI驱动的代码结构和关系理解
- **精美图表**：自动生成Mermaid图表可视化架构和数据流
- **简易导航**：简单、直观的界面探索Wiki
- **提问功能**：使用RAG驱动的AI与您的仓库聊天，获取准确答案
- **深度研究**：多轮研究过程，彻底调查复杂主题
- **多模型提供商**：支持Google Gemini、OpenAI、OpenRouter和本地Ollama模型

## 🚀 快速开始（超级简单！）

### 选项1：使用Docker

```bash
# 克隆仓库
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# 创建包含API密钥的.env文件
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# 可选：如果您想使用OpenRouter模型，添加OpenRouter API密钥
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env

# 使用Docker Compose运行
docker-compose up
```

(上述 Docker 命令以及 `docker-compose.yml` 配置会挂载您主机上的 `~/.adalflow` 目录到容器内的 `/root/.adalflow`。此路径用于存储：
- 克隆的仓库 (`~/.adalflow/repos/`)
- 仓库的嵌入和索引 (`~/.adalflow/databases/`)
- 缓存的已生成 Wiki 内容 (`~/.adalflow/wikicache/`)

这确保了即使容器停止或移除，您的数据也能持久保存。)

> 💡 **获取这些密钥的地方：**
> - 从[Google AI Studio](https://makersuite.google.com/app/apikey)获取Google API密钥
> - 从[OpenAI Platform](https://platform.openai.com/api-keys)获取OpenAI API密钥

### 选项2：手动设置（推荐）

#### 步骤1：设置API密钥

在项目根目录创建一个`.env`文件，包含以下密钥：

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# 可选：如果您想使用OpenRouter模型，添加此项
OPENROUTER_API_KEY=your_openrouter_api_key
```

#### 步骤2：启动后端

```bash
# 安装Python依赖
pip install -r api/requirements.txt

# 启动API服务器
python -m api.main
```

#### 步骤3：启动前端

```bash
# 安装JavaScript依赖
npm install
# 或
yarn install

# 启动Web应用
npm run dev
# 或
yarn dev
```

#### 步骤4：使用DeepWiki！

1. 在浏览器中打开[http://localhost:3000](http://localhost:3000)
2. 输入GitHub、GitLab或Bitbucket仓库（如`https://github.com/openai/codex`、`https://github.com/microsoft/autogen`、`https://gitlab.com/gitlab-org/gitlab`或`https://bitbucket.org/redradish/atlassian_app_versions`）
3. 对于私有仓库，点击"+ 添加访问令牌"并输入您的GitHub或GitLab个人访问令牌
4. 点击"生成Wiki"，见证奇迹的发生！

## 🔍 工作原理

DeepWiki使用AI来：

1. 克隆并分析GitHub、GitLab或Bitbucket仓库（包括使用令牌认证的私有仓库）
2. 创建代码嵌入用于智能检索
3. 使用上下文感知AI生成文档（使用Google Gemini、OpenAI、OpenRouter或本地Ollama模型）
4. 创建可视化图表解释代码关系
5. 将所有内容组织成结构化Wiki
6. 通过提问功能实现与仓库的智能问答
7. 通过深度研究功能提供深入研究能力

```mermaid
graph TD
    A[用户输入GitHub/GitLab/Bitbucket仓库] --> AA{私有仓库?}
    AA -->|是| AB[添加访问令牌]
    AA -->|否| B[克隆仓库]
    AB --> B
    B --> C[分析代码结构]
    C --> D[创建代码嵌入]

    D --> M{选择模型提供商}
    M -->|Google Gemini| E1[使用Gemini生成]
    M -->|OpenAI| E2[使用OpenAI生成]
    M -->|OpenRouter| E3[使用OpenRouter生成]
    M -->|本地Ollama| E4[使用Ollama生成]

    E1 --> E[生成文档]
    E2 --> E
    E3 --> E
    E4 --> E

    D --> F[创建可视化图表]
    E --> G[组织为Wiki]
    F --> G
    G --> H[交互式DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4 process;
    class H result;
```

## 🛠️ 项目结构

```
deepwiki/
├── api/                  # 后端API服务器
│   ├── main.py           # API入口点
│   ├── api.py            # FastAPI实现
│   ├── rag.py        

## File Path: README.vi.md

cho mô hình embedding và xử lý văn bản
   - Xác định mô hình embedding cho lưu trữ vector
   - Chứa cấu hình bộ truy xuất cho RAG
   - Chỉ định cài đặt trình chia văn bản để phân đoạn tài liệu

3. **`repo.json`**: Cấu hình xử lý repository
   - Chứa bộ lọc tệp để loại trừ một số tệp và thư mục nhất định
   - Xác định giới hạn kích thước repository và quy tắc xử lý

Mặc định, các tệp này nằm trong thư mục `api/config/`. Bạn có thể tùy chỉnh vị trí của chúng bằng biến môi trường `DEEPWIKI_CONFIG_DIR`.

### Lựa chọn mô hình tùy chỉnh cho nhà cung cấp dịch vụ

Tính năng lựa chọn mô hình tùy chỉnh được thiết kế đặc biệt cho các nhà cung cấp dịch vụ cần:

- Bạn có thể cung cấp cho người dùng trong tổ chức của mình nhiều lựa chọn mô hình AI khác nhau
- Bạn có thể thích ứng nhanh chóng với môi trường LLM đang phát triển nhanh chóng mà không cần thay đổi mã
- Bạn có thể hỗ trợ các mô hình chuyên biệt hoặc được tinh chỉnh không có trong danh sách định nghĩa trước

Bạn có thể triển khai các mô hình cung cấp bằng cách chọn từ các tùy chọn định nghĩa trước hoặc nhập định danh mô hình tùy chỉnh trong giao diện người dùng.

### Cấu hình URL cơ sở cho các kênh riêng doanh nghiệp

Cấu hình base_url của OpenAI Client được thiết kế chủ yếu cho người dùng doanh nghiệp có các kênh API riêng. Tính năng này:

- Cho phép kết nối với các điểm cuối API riêng hoặc dành riêng cho doanh nghiệp
- Cho phép các tổ chức sử dụng dịch vụ LLM tự lưu trữ hoặc triển khai tùy chỉnh
- Hỗ trợ tích hợp với các dịch vụ tương thích API OpenAI của bên thứ ba

**Sắp ra mắt**: Trong các bản cập nhật tương lai, DeepWiki sẽ hỗ trợ chế độ mà người dùng cần cung cấp API key của riêng họ trong 

## File Path: src/app/page.tsx

mb-3 font-serif">{t('home.sequenceDiagram')}</h4>
                <Mermaid chart={DEMO_SEQUENCE_CHART} />
              </div>
            </div>
          </div>
            </>
          )}
        </div>
      </main>

      <footer className="max-w-6xl mx-auto mt-8 flex flex-col gap-4 w-full">
        <div
          className="flex flex-col sm:flex-row justify-between items-center gap-4 bg-[var(--card-bg)] rounded-lg p-4 border border-[var(--border-color)] shadow-custom">
          <p className="text-[var(--muted)] text-sm font-serif">{t('footer.copyright')}</p>

          <div className="flex items-center gap-6">
            <div className="flex items-center space-x-5">
              <a href="https://github.com/AsyncFuncAI/deepwiki-open" target="_blank" rel="noopener noreferrer"
                className="text-[var(--muted)] hover:text-[var(--accent-primary)] transition-colors">
                <FaGithub className="text-xl" />
              </a>
              <a href="https://buymeacoffee.com/sheing" target="_blank" rel="noopener noreferrer"
                className="text-[var(--muted)] hover:text-[var(--accent-primary)] transition-colors">
                <FaCoffee className="text-xl" />
              </a>
              <a href="https://x.com/sashimikun_void" target="_blank" rel="noopener noreferrer"
                className="text-[var(--muted)] hover:text-[var(--accent-primary)] transition-colors">
    

## File Path: src/types/wiki/wikipage.tsx

// Wiki Interfaces
export interface WikiPage {
  id: string;
  title: string;
  content: string;
  filePaths: string[];
  importance: 'high' | 'medium' | 'low';
  relatedPages: string[];
  // New fields for hierarchy
  parentId?: string;
  isSection?: boolean;
  children?: string[]; // IDs of child pages
}
<END_OF_CONTEXT>

<query>
嵌入数据库是用什么技术和组件实现的
</query>

Assistant: 
2025-07-10 02:32:16,244 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 02:32:16,285 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 02:32:18,241 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:32:42,188 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
嵌入数据库的实现主要依赖以下技术和组件：

1. **数据处理流水线**：
   - 使用 `adalflow` 库构建流水线。
   - 包括文本分割器 (`TextSplitter`) 和嵌入转换器 (`ToEmbeddings` 或 `OllamaDocumentProcessor`)。
   - 流水线通过 `adal.Sequential` 将这些组件串联，形成从文本分割到嵌入的处理流程。

2. **嵌入模型**：
   - 支持多种嵌入模型，包括 OpenAI 兼容模型和 Ollama 本地模型。
   - 模型配置通过 `api/config/embedder.json` 文件定义，支持自定义模型选择。
   - 如果使用 Ollama，则通过 `OllamaDocumentProcessor` 单文档处理；否则通过 `ToEmbeddings` 进行批量处理。

3. **数据库存储**：
   - 使用 `LocalDB` 类来创建、加载和管理本地数据库。
   - 数据库支持注册转换器 (`register_transformer`)，并可以保存和加载状态 (`save_state`, `load`)。
   - 嵌入后的数据以转换后的格式存储，可以通过 `get_transformed_data` 获取。

4. **文件获取与处理**：
   - 使用 `get_file_content` 函数从 GitHub、GitLab 或 Bitbucket 获取文件内容。
   - 文本内容经过处理后，通过 `read_all_documents` 函数读取并准备为 `Document` 对象。
   - 文档对象随后通过 `transform_documents_and_save_to_db` 函数进行转换并保存到数据库。

5. **配置与环境变量**：
   - 通过 `api/config/` 目录下的配置文件管理模型和嵌入设置。
   - 环境变量（如 `OPENAI_API_KEY`, `OPENAI_BASE_URL`）用于指定 API 密钥和自定义端点。

6. **支持私有仓库**：
   - 提供访问令牌参数 (`access_token`) 用于认证私有仓库的访问权限。

总结：嵌入数据库的实现结合了数据处理流水线、嵌入模型、本地数据库存储和灵活的配置管理，支持多种嵌入模型和私有仓库访问。
2025-07-10 03:20:56,161 - INFO - api.api - api.py:179 - Fetching model configurations
2025-07-10 03:20:56,193 - INFO - api.api - api.py:476 - Attempting to retrieve wiki cache for google-gemini/gemini-cli (github), lang: zh
2025-07-10 03:22:15,440 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 16 tokens
2025-07-10 03:22:15,501 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:22:15,503 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:22:15,504 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:22:15,504 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:22:15,549 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:22:15,560 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['input_str', 'conversation_history', 'contexts', 'output_format_str', 'system_prompt']
2025-07-10 03:22:15,560 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 03:22:15,560 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/google-gemini/gemini-cli...
2025-07-10 03:22:15,560 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: google-gemini_gemini-cli
2025-07-10 03:22:15,561 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/google-gemini_gemini-cli. Using existing repository.
2025-07-10 03:22:15,562 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/google-gemini_gemini-cli', 'save_db_file': '/root/.adalflow/databases/google-gemini_gemini-cli.pkl'}
2025-07-10 03:22:15,562 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 03:22:15,642 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 03:22:15,643 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:22:15,643 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:22:15,643 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:22:15,701 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:22:15,702 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:22:15,702 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:22:15,768 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 1962 documents from existing database
2025-07-10 03:22:15,768 - INFO - api.rag - rag.py:421 - Loaded 1962 documents for retrieval
2025-07-10 03:22:15,769 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 1962 documents)
2025-07-10 03:22:15,771 - INFO - api.rag - rag.py:384 - Embedding validation complete: 1962/1962 documents have valid embeddings
2025-07-10 03:22:15,771 - INFO - api.rag - rag.py:429 - Using 1962 documents with valid embeddings for retrieval
2025-07-10 03:22:15,773 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:22:15,800 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 1962 chunks
2025-07-10 03:22:15,800 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 03:22:15,801 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/google-gemini/gemini-cli
2025-07-10 03:22:15,801 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['代码生成功能，是diff-based edit,还是full-content based edit']}
2025-07-10 03:22:16,189 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 03:22:16,196 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 03:22:16,196 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:22:16,196 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:22:16,197 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/google-gemini/gemini-cli (gemini-cli).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: packages/core/src/tools/modifiable-tool.ts

ToolParams; updatedDiff: string } {
  let oldContent = '';
  let newContent = '';

  try {
    oldContent = fs.readFileSync(tmpOldPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    oldContent = '';
  }

  try {
    newContent = fs.readFileSync(tempNewPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    newContent = '';
  }

  const updatedParams = modifyContext.createUpdatedParams(
    oldContent,
    newContent,
    originalParams,
  );
  const updatedDiff = Diff.createPatch(
    path.basename(modifyContext.getFilePath(originalParams)),
    oldContent,
    newContent,
    'Current',
    'Proposed',
    DEFAULT_DIFF_OPTIONS,
  );

  return { updatedParams, updatedDiff };
}

function deleteTempFiles(oldPath: string, newPath: string): void {
  try {
    fs.unlinkSync(oldPath);
  } catch {
    console.error(`Error deleting temp diff file: ${oldPath}`);
  }

  try {
    fs.unlinkSync(newPath);
  } catch {
    console.error(`Error deleting temp diff file: ${newPath}`);
  }
}

/**
 * Triggers an external editor for the user to modify the proposed content,
 * and returns the updated tool parameters and the diff after the user has modified the proposed content.
 */
export async function modifyWithEditor<ToolParams>(
  originalParams: ToolParams,
  modifyContext: ModifyContext<ToolParams>,
  editorType: EditorType,
  _abortSignal: AbortSignal,
): Promise<ModifyResult<ToolParams>> {
  const currentContent = await modifyContext.getCurrentContent(originalParams);
  const proposedContent =
    await modifyContext.getProposedContent(originalParams);

  const { oldPath, newPath } = createTempFilesForModify(
    currentContent,
    proposedContent,
    modifyContext.getFilePath(originalParams),
  );

  try {
    await openDiff(oldPath, newPath, editorType);
    const result = getUpdatedParams(
      oldPath,
      newPath,
      originalParams,
      modifyContext,
    );

    return result;
  } finally {
    deleteTempFiles(oldPath, newPath);
  }
}


/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { EditorType, openDiff } from '../utils/editor.js';
import os from 'os';
import path from 'path';
import fs from 'fs';
import * as Diff from 'diff';
import { DEFAULT_DIFF_OPTIONS } from './diffOptions.js';
import { isNodeError } from '../utils/errors.js';
import { Tool } from './tools.js';

/**
 * A tool that supports a modify operation.
 */
export interface ModifiableTool<ToolParams> extends Tool<ToolParams> {
  getModifyContext(abortSignal: AbortSignal): ModifyContext<ToolParams>;
}

export interface ModifyContext<ToolParams> {
  getFilePath: (params: ToolParams) => string;

  getCurrentContent: (params: ToolParams) => Promise<string>;

  getProposedContent: (params: ToolParams) => Promise<string>;

  createUpdatedParams: (
    oldContent: string,
    modifiedProposedContent: string,
    originalParams: ToolParams,
  ) => ToolParams;
}

export interface ModifyResult<ToolParams> {
  updatedParams: ToolParams;
  updatedDiff: string;
}

export function isModifiableTool<TParams>(
  tool: Tool<TParams>,
): tool is ModifiableTool<TParams> {
  return 'getModifyContext' in tool;
}

function createTempFilesForModify(
  currentContent: string,
  proposedContent: string,
  file_path: string,
): { oldPath: string; newPath: string } {
  const tempDir = os.tmpdir();
  const diffDir = path.join(tempDir, 'gemini-cli-tool-modify-diffs');

  if (!fs.existsSync(diffDir)) {
    fs.mkdirSync(diffDir, { recursive: true });
  }

  const ext = path.extname(file_path);
  const fileName = path.basename(file_path, ext);
  const timestamp = Date.now();
  const tempOldPath = path.join(
    diffDir,
    `gemini-cli-modify-${fileName}-old-${timestamp}${ext}`,
  );
  const tempNewPath = path.join(
    diffDir,
    `gemini-cli-modify-${fileName}-new-${timestamp}${ext}`,
  );

  fs.writeFileSync(tempOldPath, currentContent, 'utf8');
  fs.writeFileSync(tempNewPath, proposedContent, 'utf8');

  return { oldPath: tempOldPath, newPath: tempNewPath };
}

function getUpdatedParams<ToolParams>(
  tmpOldPath: string,
  tempNewPath: string,
  originalParams: ToolParams,
  modifyContext: ModifyContext<ToolParams>,
): { updatedParams: ToolParams; updatedDiff: string } {
  let oldContent = '';
  let newContent = '';

  try {
    oldContent = fs.readFileSync(tmpOldPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    oldContent = '';
  }

  try {
    newContent = fs.readFileSync(tempNewPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    newContent = '';
  }

  const updatedParams = modifyContext.createUpdatedParams(
    oldContent,
    newContent,
    originalParams,
 

## File Path: packages/core/src/tools/edit.ts

err;
          return '';
        }
      },
      getProposedContent: async (params: EditToolParams): Promise<string> => {
        try {
          const currentContent = fs.readFileSync(params.file_path, 'utf8');
          return this._applyReplacement(
            currentContent,
            params.old_string,
            params.new_string,
            params.old_string === '' && currentContent === '',
          );
        } catch (err) {
          if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
          return '';
        }
      },
      createUpdatedParams: (
        oldContent: string,
        modifiedProposedContent: string,
        originalParams: EditToolParams,
      ): EditToolParams => ({
        ...originalParams,
        old_string: oldContent,
        new_string: modifiedProposedContent,
        modified_by_user: true,
      }),
    };
  }
}


* @returns Result of the edit operation
   */
  async execute(
    params: EditToolParams,
    signal: AbortSignal,
  ): Promise<ToolResult> {
    const validationError = this.validateToolParams(params);
    if (validationError) {
      return {
        llmContent: `Error: Invalid parameters provided. Reason: ${validationError}`,
        returnDisplay: `Error: ${validationError}`,
      };
    }

    let editData: CalculatedEdit;
    try {
      editData = await this.calculateEdit(params, signal);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      return {
        llmContent: `Error preparing edit: ${errorMsg}`,
        returnDisplay: `Error preparing edit: ${errorMsg}`,
      };
    }

    if (editData.error) {
      return {
        llmContent: editData.error.raw,
        returnDisplay: `Error: ${editData.error.display}`,
      };
    }

    try {
      this.ensureParentDirectoriesExist(params.file_path);
      fs.writeFileSync(params.file_path, editData.newContent, 'utf8');

      let displayResult: ToolResultDisplay;
      if (editData.isNewFile) {
        displayResult = `Created ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`;
      } else {
        // Generate diff for display, even though core logic doesn't technically need it
        // The CLI wrapper will use this part of the ToolResult
        const fileName = path.basename(params.file_path);
        const fileDiff = Diff.createPatch(
          fileName,
          editData.currentContent ?? '', // Should not be null here 

= `Created ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`;
      } else {
        // Generate diff for display, even though core logic doesn't technically need it
        // The CLI wrapper will use this part of the ToolResult
        const fileName = path.basename(params.file_path);
        const fileDiff = Diff.createPatch(
          fileName,
          editData.currentContent ?? '', // Should not be null here if not isNewFile
          editData.newContent,
          'Current',
          'Proposed',
          DEFAULT_DIFF_OPTIONS,
        );
        displayResult = { fileDiff, fileName };
      }

      const llmSuccessMessageParts = [
        editData.isNewFile
          ? `Created new file: ${params.file_path} with provided content.`
          : `Successfully modified file: ${params.file_path} (${editData.occurrences} replacements).`,
      ];
      if (params.modified_by_user) {
        llmSuccessMessageParts.push(
          `User modified the \`new_string\` content to be: ${params.new_string}.`,
        );
      }

      return {
        llmContent: llmSuccessMessageParts.join(' '),
        returnDisplay: displayResult,
      };
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      return {
        llmContent: `Error executing edit: ${errorMsg}`,
     

   title: `Confirm Edit: ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`,
      fileName,
      fileDiff,
      onConfirm: async (outcome: ToolConfirmationOutcome) => {
        if (outcome === ToolConfirmationOutcome.ProceedAlways) {
          this.config.setApprovalMode(ApprovalMode.AUTO_EDIT);
        }
      },
    };
    return confirmationDetails;
  }

  getDescription(params: EditToolParams): string {
    if (!params.file_path || !params.old_string || !params.new_string) {
      return `Model did not provide valid parameters for edit tool`;
    }
    const relativePath = makeRelative(params.file_path, this.rootDirectory);
    if (params.old_string === '') {
      return `Create ${shortenPath(relativePath)}`;
    }

    const oldStringSnippet =
      params.old_string.split('\n')[0].substring(0, 30) +
      (params.old_string.length > 30 ? '...' : '');
    const newStringSnippet =
      params.new_string.split('\n')[0].substring(0, 30) +
      (params.new_string.length > 30 ? '...' : '');

    if (params.old_string === params.new_string) {
      return `No file changes to ${shortenPath(relativePath)}`;
    }
    return `${shortenPath(relativePath)}: ${oldStringSnippet} => ${newStringSnippet}`;
  }

  /**
   * Executes the edit operation with the given parameters.
   * @param params Parameters for the edit operation
   * @returns Result of the edit operation
   */
  async execute(
    params: EditToolParams,
    signal: AbortSignal,
  ): Promise<ToolResult> {
    const validationError = this.validateToolParams(params);
    if (validationError) {
      return {
        llmContent: `Error: Invalid parameters provided. Reason: ${validationError}`,
        returnDisplay: `Error: ${validationError}`,
      };
    }

    let editData: CalculatedEdit;
    try {
      editData = await 

*/
  async shouldConfirmExecute(
    params: EditToolParams,
    abortSignal: AbortSignal,
  ): Promise<ToolCallConfirmationDetails | false> {
    if (this.config.getApprovalMode() === ApprovalMode.AUTO_EDIT) {
      return false;
    }
    const validationError = this.validateToolParams(params);
    if (validationError) {
      console.error(
        `[EditTool Wrapper] Attempted confirmation with invalid parameters: ${validationError}`,
      );
      return false;
    }

    let editData: CalculatedEdit;
    try {
      editData = await this.calculateEdit(params, abortSignal);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      console.log(`Error preparing edit: ${errorMsg}`);
      return false;
    }

    if (editData.error) {
      console.log(`Error: ${editData.error.display}`);
      return false;
    }

    const fileName = path.basename(params.file_path);
    const fileDiff = Diff.createPatch(
      fileName,
      editData.currentContent ?? '',
      editData.newContent,
      'Current',
      'Proposed',
      DEFAULT_DIFF_OPTIONS,
    );
    const confirmationDetails: ToolEditConfirmationDetails = {
      type: 'edit',
      title: `Confirm Edit: ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`,
      fileName,
      fileDiff,
      onConfirm: async (outcome: ToolConfirmationOutcome) => {
        if (outcome === ToolConfirmationOutcome.ProceedAlways) {
          this.config.setApprovalMode(ApprovalMode.AUTO_EDIT);
        }
      },
    };
    return confirmationDetails;
  }

  getDescription(params: EditToolParams): string {
    if (!params.file_path || !params.old_string || !params.new_string) {
      return `Model 

 `User modified the \`new_string\` content to be: ${params.new_string}.`,
        );
      }

      return {
        llmContent: llmSuccessMessageParts.join(' '),
        returnDisplay: displayResult,
      };
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      return {
        llmContent: `Error executing edit: ${errorMsg}`,
        returnDisplay: `Error writing file: ${errorMsg}`,
      };
    }
  }

  /**
   * Creates parent directories if they don't exist
   */
  private ensureParentDirectoriesExist(filePath: string): void {
    const dirName = path.dirname(filePath);
    if (!fs.existsSync(dirName)) {
      fs.mkdirSync(dirName, { recursive: true });
    }
  }

  getModifyContext(_: AbortSignal): ModifyContext<EditToolParams> {
    return {
      getFilePath: (params: EditToolParams) => params.file_path,
      getCurrentContent: async (params: EditToolParams): Promise<string> => {
        try {
          return fs.readFileSync(params.file_path, 'utf8');
        } catch (err) {
          if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
          return '';
        }
      },
      getProposedContent: async (params: EditToolParams): Promise<string> => {
        try {
          const currentContent = fs.readFileSync(params.file_path, 'utf8');
          return this._applyReplacement(
            currentContent,
            params.old_string,
     

## File Path: packages/core/src/tools/modifiable-tool.test.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  vi,
  describe,
  it,
  expect,
  beforeEach,
  afterEach,
  type Mock,
} from 'vitest';
import {
  modifyWithEditor,
  ModifyContext,
  ModifiableTool,
  isModifiableTool,
} from './modifiable-tool.js';
import { EditorType } from '../utils/editor.js';
import fs from 'fs';
import os from 'os';
import * as path from 'path';

// Mock dependencies
const mockOpenDiff = vi.hoisted(() => vi.fn());
const mockCreatePatch = vi.hoisted(() => vi.fn());

vi.mock('../utils/editor.js', () => ({
  openDiff: mockOpenDiff,
}));

vi.mock('diff', () => ({
  createPatch: mockCreatePatch,
}));

vi.mock('fs');
vi.mock('os');

interface TestParams {
  filePath: string;
  someOtherParam: string;
  modifiedContent?: string;
}

describe('modifyWithEditor', () => {
  let tempDir: string;
  let mockModifyContext: ModifyContext<TestParams>;
  let mockParams: TestParams;
  let currentContent: string;
  let proposedContent: string;
  let modifiedContent: string;
  let abortSignal: AbortSignal;

  beforeEach(() => {
    vi.resetAllMocks();

    tempDir = '/tmp/test-dir';
    abortSignal = new AbortController().signal;

    currentContent = 'original content\nline 2\nline 3';
    proposedContent = 'modified content\nline 2\nline 3';
    modifiedContent = 'user modified content\nline 2\nline 3\nnew line';
    mockParams = {
      filePath: path.join(tempDir, 'test.txt'),
      someOtherParam: 'value',
    };

    mockModifyContext = {
      getFilePath: vi.fn().mockReturnValue(mockParams.filePath),
      getCurrentContent: vi.fn().mockResolvedValue(currentContent),
      getProposedContent: vi.fn().mockResolvedValue(proposedContent),
      createUpdatedParams: vi
        .fn()
        .mockImplementation((oldContent, modifiedContent, originalParams) => ({
          ...originalParams,
          modifiedContent,
          oldContent,
        })),
    };

    (os.tmpdir as Mock).mockReturnValue(tempDir);

    (fs.existsSync as Mock).mockReturnValue(true);
    (fs.mkdirSync as Mock).mockImplementation(() => undefined);
    (fs.writeFileSync as Mock).mockImplementation(() => {});
    (fs.unlinkSync as Mock).mockImplementation(() => {});

    (fs.readFileSync as Mock).mockImplementation((filePath: string) => {
      if (filePath.includes('-new-')) {
    

as EditorType,
      abortSignal,
    );

    expect(mockCreatePatch).toHaveBeenCalledWith(
      path.basename(mockParams.filePath),
      currentContent,
      '',
      'Current',
      'Proposed',
      expect.objectContaining({
        context: 3,
        ignoreWhitespace: true,
      }),
    );

    expect(result.updatedParams).toBeDefined();
    expect(result.updatedDiff).toBe('mock diff content');
  });

  it('should clean up temp files even if editor fails', async () => {
    const editorError = new Error('Editor failed to open');
    mockOpenDiff.mockRejectedValue(editorError);

    await expect(
      modifyWithEditor(
        mockParams,
        mockModifyContext,
        'vscode' as EditorType,
        abortSignal,
      ),
    ).rejects.toThrow('Editor failed to open');

    expect(fs.unlinkSync).toHaveBeenCalledTimes(2);
  });

  it('should handle temp file cleanup errors gracefully', async () => {
    const consoleErrorSpy = vi
      .spyOn(console, 'error')
      .mockImplementation(() => {});
    (fs.unlinkSync as Mock).mockImplementation((_filePath: string) => {
      throw new Error('Failed to delete file');
    });

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    expect(consoleErrorSpy).toHaveBeenCalledTimes(2);
    expect(consoleErrorSpy).toHaveBeenCalledWith(
      expect.stringContaining('Error deleting temp diff file:'),
    );

    consoleErrorSpy.mockRestore();
  });

  it('should create temp files with correct naming with extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file.txt');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
     

 it('should create temp directory if it does not exist', async () => {
      (fs.existsSync as Mock).mockReturnValue(false);

      await modifyWithEditor(
        mockParams,
        mockModifyContext,
        'vscode' as EditorType,
        abortSignal,
      );

      expect(fs.mkdirSync).toHaveBeenCalledWith(
        path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        { recursive: true },
      );
    });

    it('should not create temp directory if it already exists', async () => {
      (fs.existsSync as Mock).mockReturnValue(true);

      await modifyWithEditor(
        mockParams,
        mockModifyContext,
        'vscode' as EditorType,
        abortSignal,
      );

      expect(fs.mkdirSync).not.toHaveBeenCalled();
    });
  });

  it('should handle missing old temp file gracefully', async () => {
    (fs.readFileSync as Mock).mockImplementation((filePath: string) => {
      if (filePath.includes('-old-')) {
        const error = new Error('ENOENT: no such file or directory');
        (error as NodeJS.ErrnoException).code = 'ENOENT';
        throw error;
      }
      return modifiedContent;
    });

    const result = await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    expect(mockCreatePatch).toHaveBeenCalledWith(
      path.basename(mockParams.filePath),
      '',
      modifiedContent,
      'Current',
      'Proposed',
      expect.objectContaining({
 

      modifiedContent,
        mockParams,
      );

      expect(mockCreatePatch).toHaveBeenCalledWith(
        path.basename(mockParams.filePath),
        currentContent,
        modifiedContent,
        'Current',
        'Proposed',
        expect.objectContaining({
          context: 3,
          ignoreWhitespace: true,
        }),
      );

      expect(fs.unlinkSync).toHaveBeenCalledTimes(2);
      expect(fs.unlinkSync).toHaveBeenNthCalledWith(
        1,
        expect.stringContaining('-old-'),
      );
      expect(fs.unlinkSync).toHaveBeenNthCalledWith(
        2,
        expect.stringContaining('-new-'),
      );

      expect(result).toEqual({
        updatedParams: {
          ...mockParams,
          modifiedContent,
          oldContent: currentContent,
        },
        updatedDiff: 'mock diff content',
      });
    });

    it('should create temp directory if it does not exist', async () => {
      (fs.existsSync as Mock).mockReturnValue(false);

      await modifyWithEditor(
        mockParams,
        mockModifyContext,
        'vscode' as EditorType,
        abortSignal,
      );

      expect(fs.mkdirSync).toHaveBeenCalledWith(
        path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        { recursive: true },
      

=> ({
          ...originalParams,
          modifiedContent,
          oldContent,
        })),
    };

    (os.tmpdir as Mock).mockReturnValue(tempDir);

    (fs.existsSync as Mock).mockReturnValue(true);
    (fs.mkdirSync as Mock).mockImplementation(() => undefined);
    (fs.writeFileSync as Mock).mockImplementation(() => {});
    (fs.unlinkSync as Mock).mockImplementation(() => {});

    (fs.readFileSync as Mock).mockImplementation((filePath: string) => {
      if (filePath.includes('-new-')) {
        return modifiedContent;
      }
      return currentContent;
    });

    mockCreatePatch.mockReturnValue('mock diff content');
    mockOpenDiff.mockResolvedValue(undefined);
  });

  afterEach(() => {
    vi.restoreAllMocks();
  });

  describe('successful modification', () => {
    it('should successfully modify content with VSCode editor', async () => {
      const result = await modifyWithEditor(
        mockParams,
        mockModifyContext,
        'vscode' as EditorType,
        abortSignal,
      );

      expect(mockModifyContext.getCurrentContent).toHaveBeenCalledWith(
        mockParams,
      );
      expect(mockModifyContext.getProposedContent).toHaveBeenCalledWith(
        mockParams,
      );
      expect(mockModifyContext.getFilePath).toHaveBeenCalledWith(mockParams);

      expect(fs.writeFileSync).toHaveBeenCalledTimes(2);
      expect(fs.writeFileSync).toHaveBeenNthCalledWith(
        1,
        expect.stringContaining(
          path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        ),
        currentContent,
        'utf8',
      );
      expect(fs.writeFileSync).toHaveBeenNthCalledWith(
        2,
       

    );
      expect(mockModifyContext.getFilePath).toHaveBeenCalledWith(mockParams);

      expect(fs.writeFileSync).toHaveBeenCalledTimes(2);
      expect(fs.writeFileSync).toHaveBeenNthCalledWith(
        1,
        expect.stringContaining(
          path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        ),
        currentContent,
        'utf8',
      );
      expect(fs.writeFileSync).toHaveBeenNthCalledWith(
        2,
        expect.stringContaining(
          path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        ),
        proposedContent,
        'utf8',
      );

      expect(mockOpenDiff).toHaveBeenCalledWith(
        expect.stringContaining('-old-'),
        expect.stringContaining('-new-'),
        'vscode',
      );

      expect(fs.readFileSync).toHaveBeenCalledWith(
        expect.stringContaining('-old-'),
        'utf8',
      );
      expect(fs.readFileSync).toHaveBeenCalledWith(
        expect.stringContaining('-new-'),
        'utf8',
      );

      expect(mockModifyContext.createUpdatedParams).toHaveBeenCalledWith(
        currentContent,
        modifiedContent,
        mockParams,
      );

      expect(mockCreatePatch).toHaveBeenCalledWith(
        path.basename(mockParams.filePath),
        currentContent,
        modifiedContent,
        'Current',
        'Proposed',
        expect.objectContaining({
          context: 3,
          ignoreWhitespace: true,
    

   mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    expect(consoleErrorSpy).toHaveBeenCalledTimes(2);
    expect(consoleErrorSpy).toHaveBeenCalledWith(
      expect.stringContaining('Error deleting temp diff file:'),
    );

    consoleErrorSpy.mockRestore();
  });

  it('should create temp files with correct naming with extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file.txt');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    const writeFileCalls = (fs.writeFileSync as Mock).mock.calls;
    expect(writeFileCalls).toHaveLength(2);

    const oldFilePath = writeFileCalls[0][0];
    const newFilePath = writeFileCalls[1][0];

    expect(oldFilePath).toMatch(/gemini-cli-modify-test-file-old-\d+\.txt$/);
    expect(newFilePath).toMatch(/gemini-cli-modify-test-file-new-\d+\.txt$/);
    expect(oldFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
    expect(newFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
  });

  it('should create temp files with correct naming without extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    const writeFileCalls = (fs.writeFileSync as Mock).mock.calls;
    expect(writeFileCalls).toHaveLength(2);

    const oldFilePath = writeFileCalls[0][0];
    const newFilePath = writeFileCalls[1][0];

    expect(oldFilePath).toMatch(/gemini-cli-modify-test-file-old-\d+$/);
    expect(newFilePath).toMatch(/gemini-cli-modify-test-file-new-\d+$/);
    expect(oldFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
    expect(newFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
  });
});

describe('isModifiableTool', () => {
  it('should return true for objects with getModifyContext method', () => {
    const mockTool = {
      name: 'test-tool',
      getModifyContext: vi.fn(),
    } as unknown as ModifiableTool<TestParams>;

    expect(isModifiableTool(mockTool)).toBe(true);
  });

  it('should return false for objects without getModifyContext method', () => {
    const mockTool = {
      name: 'test-tool',
 

## File Path: packages/core/src/tools/write-file.ts

 !fileExists ||
      (correctedContentResult.error !== undefined &&
        !correctedContentResult.fileExists);

    try {
      const dirName = path.dirname(params.file_path);
      if (!fs.existsSync(dirName)) {
        fs.mkdirSync(dirName, { recursive: true });
      }

      fs.writeFileSync(params.file_path, fileContent, 'utf8');

      // Generate diff for display result
      const fileName = path.basename(params.file_path);
      // If there was a readError, originalContent in correctedContentResult is '',
      // but for the diff, we want to show the original content as it was before the write if possible.
      // However, if it was unreadable, currentContentForDiff will be empty.
      const currentContentForDiff = correctedContentResult.error
        ? '' // Or some indicator of unreadable content
        : originalContent;

      const fileDiff = Diff.createPatch(
        fileName,
        currentContentForDiff,
        fileContent,
        'Original',
        'Written',
        DEFAULT_DIFF_OPTIONS,
      );

      const llmSuccessMessageParts = [
        isNewFile
          ? `Successfully created and wrote to new file: ${params.file_path}.`
          : `Successfully overwrote file: ${params.file_path}.`,
      ];
      if (params.modified_by_user) {
        llmSuccessMessageParts.push(
          `User modified the \`content\` to be: ${params.content}`,
        );
      }

      const displayResult: FileDiff 

         new_string: proposedContent,
          file_path: filePath,
        },
        this.config.getGeminiClient(),
        abortSignal,
      );
      correctedContent = correctedParams.new_string;
    } else {
      // This implies new file (ENOENT)
      correctedContent = await ensureCorrectFileContent(
        proposedContent,
        this.config.getGeminiClient(),
        abortSignal,
      );
    }
    return { originalContent, correctedContent, fileExists };
  }

  getModifyContext(
    abortSignal: AbortSignal,
  ): ModifyContext<WriteFileToolParams> {
    return {
      getFilePath: (params: WriteFileToolParams) => params.file_path,
      getCurrentContent: async (params: WriteFileToolParams) => {
        const correctedContentResult = await this._getCorrectedFileContent(
          params.file_path,
          params.content,
          abortSignal,
        );
        return correctedContentResult.originalContent;
      },
      getProposedContent: async (params: WriteFileToolParams) => {
        const correctedContentResult = await this._getCorrectedFileContent(
          params.file_path,
          params.content,
          abortSignal,
        );
        return correctedContentResult.correctedContent;
      },
      createUpdatedParams: (
        _oldContent: string,
        modifiedProposedContent: string,
        originalParams: WriteFileToolParams,
      ) => 

## File Path: packages/core/src/tools/edit.test.ts

ensureCorrectEdit for diff generation', async () => {
      const originalContent = 'This is the original string to be replaced.';
      const originalOldString = 'original string';
      const originalNewString = 'new string';

      const correctedOldString = 'original string to be replaced'; // More specific
      const correctedNewString = 'completely new string'; // Different replacement
      const expectedFinalContent = 'This is the completely new string.';

      fs.writeFileSync(filePath, originalContent);
      const params: EditToolParams = {
        file_path: filePath,
        old_string: originalOldString,
        new_string: originalNewString,
      };

      // The main beforeEach already calls mockEnsureCorrectEdit.mockReset()
      // Set a specific mock for this test case
      let mockCalled = false;
      mockEnsureCorrectEdit.mockImplementationOnce(
        async (_, content, p, client) => {
          mockCalled = true;
          expect(content).toBe(originalContent);
          expect(p).toBe(params);
          expect(client).toBe(geminiClient);
          return {
            params: {
              file_path: filePath,
              old_string: correctedOldString,
              new_string: correctedNewString,
            },
            occurrences: 1,
          };
        },
    

the new string value',
      };
      // shortenPath will be called internally, resulting in just the file name
      // The snippets are truncated at 30 chars + '...'
      expect(tool.getDescription(params)).toBe(
        `${testFileName}: this is the old string value => this is the new string value`,
      );
    });

    it('should handle very short strings correctly in the description', () => {
      const testFileName = 'short.txt';
      const params: EditToolParams = {
        file_path: path.join(rootDir, testFileName),
        old_string: 'old',
        new_string: 'new',
      };
      expect(tool.getDescription(params)).toBe(`${testFileName}: old => new`);
    });

    it('should truncate long strings in the description', () => {
      const testFileName = 'long.txt';
      const params: EditToolParams = {
        file_path: path.join(rootDir, testFileName),
        old_string:
          'this is a very long old string that will definitely be truncated',
        new_string:
          'this is a very long new string that will also be truncated',
      };
      expect(tool.getDescription(params)).toBe(
        `${testFileName}: this is a very long old string... => this is a very long new string...`,
      );
    });
  });
});


{
      fs.writeFileSync(filePath, 'Existing content', 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: '',
        new_string: 'new content',
      };
      const result = await tool.execute(params, new AbortController().signal);
      expect(result.llmContent).toMatch(/File already exists, cannot create/);
      expect(result.returnDisplay).toMatch(
        /Attempted to create a file that already exists/,
      );
    });

    it('should include modification message when proposed content is modified', async () => {
      const initialContent = 'This is some old text.';
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
        modified_by_user: true,
      };

      (mockConfig.getApprovalMode as Mock).mockReturnValueOnce(
        ApprovalMode.AUTO_EDIT,
      );
      const result = await tool.execute(params, new AbortController().signal);

      expect(result.llmContent).toMatch(
        /User modified the `new_string` content/,
      );
    });

    it('should not include modification message when proposed content is not modified', async () => {
      const initialContent = 'This is some old text.';
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
        modified_by_user: false,
      };

  
<END_OF_CONTEXT>

<query>
代码生成功能，是diff-based edit,还是full-content based edit
</query>

Assistant: 
2025-07-10 03:22:16,197 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 03:22:16,225 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 03:22:17,709 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:22:26,132 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
代码生成功能使用的是 **diff-based edit**。

具体证据如下：

1. **`modifyWithEditor` 函数**（在 `modifiable-tool.ts` 中）：
   - 创建临时文件 `oldPath` 和 `newPath`，分别存储当前内容和提议内容。
   - 调用 `Diff.createPatch()` 生成 diff 内容，用于展示差异。
   - 最终返回的 `updatedDiff` 是通过 `diff` 模块生成的差异补丁。

2. **`EditTool` 的 `execute` 方法**（在 `edit.ts` 中）：
   - 在执行编辑操作后，会调用 `Diff.createPatch()` 生成文件的差异，用于展示给用户。
   - 如果是新文件，会生成一个“创建文件”的描述；如果是修改已有文件，则生成 diff 并返回。

3. **`WriteFileTool` 的 `execute` 方法**（在 `write-file.ts` 中）：
   - 同样使用 `Diff.createPatch()` 生成差异内容，用于展示文件的修改前和修改后内容。

4. **测试代码**（如 `modifiable-tool.test.ts` 和 `edit.test.ts`）：
   - 明确验证了 diff 的生成逻辑是否正确，包括对 `Diff.createPatch()` 的 mock 测试。

综上所述，该工具链中的代码生成功能是基于 **diff-based edit** 的，通过比较旧内容与新内容的差异来执行编辑操作，并将这些差异展示给用户。
2025-07-10 03:24:01,013 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 19 tokens
2025-07-10 03:24:01,058 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:24:01,059 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:24:01,059 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:24:01,059 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:24:01,096 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:24:01,103 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['input_str', 'conversation_history', 'contexts', 'output_format_str', 'system_prompt']
2025-07-10 03:24:01,104 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 03:24:01,104 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/google-gemini/gemini-cli...
2025-07-10 03:24:01,105 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: google-gemini_gemini-cli
2025-07-10 03:24:01,105 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/google-gemini_gemini-cli. Using existing repository.
2025-07-10 03:24:01,105 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/google-gemini_gemini-cli', 'save_db_file': '/root/.adalflow/databases/google-gemini_gemini-cli.pkl'}
2025-07-10 03:24:01,105 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 03:24:01,148 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 03:24:01,149 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:24:01,149 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:24:01,149 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:24:01,210 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:24:01,210 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:24:01,210 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:24:01,268 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 1962 documents from existing database
2025-07-10 03:24:01,269 - INFO - api.rag - rag.py:421 - Loaded 1962 documents for retrieval
2025-07-10 03:24:01,271 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 1962 documents)
2025-07-10 03:24:01,272 - INFO - api.rag - rag.py:384 - Embedding validation complete: 1962/1962 documents have valid embeddings
2025-07-10 03:24:01,272 - INFO - api.rag - rag.py:429 - Using 1962 documents with valid embeddings for retrieval
2025-07-10 03:24:01,272 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:24:01,290 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 1962 chunks
2025-07-10 03:24:01,290 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 03:24:01,290 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/google-gemini/gemini-cli
2025-07-10 03:24:01,290 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['既然是 diff-based edit，那么提供的prompt 是什么']}
2025-07-10 03:24:01,732 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 03:24:01,736 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 03:24:01,736 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:24:01,736 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:24:01,736 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/google-gemini/gemini-cli (gemini-cli).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: packages/core/src/tools/modifiable-tool.ts

ToolParams; updatedDiff: string } {
  let oldContent = '';
  let newContent = '';

  try {
    oldContent = fs.readFileSync(tmpOldPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    oldContent = '';
  }

  try {
    newContent = fs.readFileSync(tempNewPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    newContent = '';
  }

  const updatedParams = modifyContext.createUpdatedParams(
    oldContent,
    newContent,
    originalParams,
  );
  const updatedDiff = Diff.createPatch(
    path.basename(modifyContext.getFilePath(originalParams)),
    oldContent,
    newContent,
    'Current',
    'Proposed',
    DEFAULT_DIFF_OPTIONS,
  );

  return { updatedParams, updatedDiff };
}

function deleteTempFiles(oldPath: string, newPath: string): void {
  try {
    fs.unlinkSync(oldPath);
  } catch {
    console.error(`Error deleting temp diff file: ${oldPath}`);
  }

  try {
    fs.unlinkSync(newPath);
  } catch {
    console.error(`Error deleting temp diff file: ${newPath}`);
  }
}

/**
 * Triggers an external editor for the user to modify the proposed content,
 * and returns the updated tool parameters and the diff after the user has modified the proposed content.
 */
export async function modifyWithEditor<ToolParams>(
  originalParams: ToolParams,
  modifyContext: ModifyContext<ToolParams>,
  editorType: EditorType,
  _abortSignal: AbortSignal,
): Promise<ModifyResult<ToolParams>> {
  const currentContent = await modifyContext.getCurrentContent(originalParams);
  const proposedContent =
    await modifyContext.getProposedContent(originalParams);

  const { oldPath, newPath } = createTempFilesForModify(
    currentContent,
    proposedContent,
    modifyContext.getFilePath(originalParams),
  );

  try {
    await openDiff(oldPath, newPath, editorType);
    const result = getUpdatedParams(
      oldPath,
      newPath,
      originalParams,
      modifyContext,
    );

    return result;
  } finally {
    deleteTempFiles(oldPath, newPath);
  }
}


/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { EditorType, openDiff } from '../utils/editor.js';
import os from 'os';
import path from 'path';
import fs from 'fs';
import * as Diff from 'diff';
import { DEFAULT_DIFF_OPTIONS } from './diffOptions.js';
import { isNodeError } from '../utils/errors.js';
import { Tool } from './tools.js';

/**
 * A tool that supports a modify operation.
 */
export interface ModifiableTool<ToolParams> extends Tool<ToolParams> {
  getModifyContext(abortSignal: AbortSignal): ModifyContext<ToolParams>;
}

export interface ModifyContext<ToolParams> {
  getFilePath: (params: ToolParams) => string;

  getCurrentContent: (params: ToolParams) => Promise<string>;

  getProposedContent: (params: ToolParams) => Promise<string>;

  createUpdatedParams: (
    oldContent: string,
    modifiedProposedContent: string,
    originalParams: ToolParams,
  ) => ToolParams;
}

export interface ModifyResult<ToolParams> {
  updatedParams: ToolParams;
  updatedDiff: string;
}

export function isModifiableTool<TParams>(
  tool: Tool<TParams>,
): tool is ModifiableTool<TParams> {
  return 'getModifyContext' in tool;
}

function createTempFilesForModify(
  currentContent: string,
  proposedContent: string,
  file_path: string,
): { oldPath: string; newPath: string } {
  const tempDir = os.tmpdir();
  const diffDir = path.join(tempDir, 'gemini-cli-tool-modify-diffs');

  if (!fs.existsSync(diffDir)) {
    fs.mkdirSync(diffDir, { recursive: true });
  }

  const ext = path.extname(file_path);
  const fileName = path.basename(file_path, ext);
  const timestamp = Date.now();
  const tempOldPath = path.join(
    diffDir,
    `gemini-cli-modify-${fileName}-old-${timestamp}${ext}`,
  );
  const tempNewPath = path.join(
    diffDir,
    `gemini-cli-modify-${fileName}-new-${timestamp}${ext}`,
  );

  fs.writeFileSync(tempOldPath, currentContent, 'utf8');
  fs.writeFileSync(tempNewPath, proposedContent, 'utf8');

  return { oldPath: tempOldPath, newPath: tempNewPath };
}

function getUpdatedParams<ToolParams>(
  tmpOldPath: string,
  tempNewPath: string,
  originalParams: ToolParams,
  modifyContext: ModifyContext<ToolParams>,
): { updatedParams: ToolParams; updatedDiff: string } {
  let oldContent = '';
  let newContent = '';

  try {
    oldContent = fs.readFileSync(tmpOldPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    oldContent = '';
  }

  try {
    newContent = fs.readFileSync(tempNewPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    newContent = '';
  }

  const updatedParams = modifyContext.createUpdatedParams(
    oldContent,
    newContent,
    originalParams,
 

## File Path: packages/core/src/tools/edit.ts

= `Created ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`;
      } else {
        // Generate diff for display, even though core logic doesn't technically need it
        // The CLI wrapper will use this part of the ToolResult
        const fileName = path.basename(params.file_path);
        const fileDiff = Diff.createPatch(
          fileName,
          editData.currentContent ?? '', // Should not be null here if not isNewFile
          editData.newContent,
          'Current',
          'Proposed',
          DEFAULT_DIFF_OPTIONS,
        );
        displayResult = { fileDiff, fileName };
      }

      const llmSuccessMessageParts = [
        editData.isNewFile
          ? `Created new file: ${params.file_path} with provided content.`
          : `Successfully modified file: ${params.file_path} (${editData.occurrences} replacements).`,
      ];
      if (params.modified_by_user) {
        llmSuccessMessageParts.push(
          `User modified the \`new_string\` content to be: ${params.new_string}.`,
        );
      }

      return {
        llmContent: llmSuccessMessageParts.join(' '),
        returnDisplay: displayResult,
      };
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      return {
        llmContent: `Error executing edit: ${errorMsg}`,
     

   title: `Confirm Edit: ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`,
      fileName,
      fileDiff,
      onConfirm: async (outcome: ToolConfirmationOutcome) => {
        if (outcome === ToolConfirmationOutcome.ProceedAlways) {
          this.config.setApprovalMode(ApprovalMode.AUTO_EDIT);
        }
      },
    };
    return confirmationDetails;
  }

  getDescription(params: EditToolParams): string {
    if (!params.file_path || !params.old_string || !params.new_string) {
      return `Model did not provide valid parameters for edit tool`;
    }
    const relativePath = makeRelative(params.file_path, this.rootDirectory);
    if (params.old_string === '') {
      return `Create ${shortenPath(relativePath)}`;
    }

    const oldStringSnippet =
      params.old_string.split('\n')[0].substring(0, 30) +
      (params.old_string.length > 30 ? '...' : '');
    const newStringSnippet =
      params.new_string.split('\n')[0].substring(0, 30) +
      (params.new_string.length > 30 ? '...' : '');

    if (params.old_string === params.new_string) {
      return `No file changes to ${shortenPath(relativePath)}`;
    }
    return `${shortenPath(relativePath)}: ${oldStringSnippet} => ${newStringSnippet}`;
  }

  /**
   * Executes the edit operation with the given parameters.
   * @param params Parameters for the edit operation
   * @returns Result of the edit operation
   */
  async execute(
    params: EditToolParams,
    signal: AbortSignal,
  ): Promise<ToolResult> {
    const validationError = this.validateToolParams(params);
    if (validationError) {
      return {
        llmContent: `Error: Invalid parameters provided. Reason: ${validationError}`,
        returnDisplay: `Error: ${validationError}`,
      };
    }

    let editData: CalculatedEdit;
    try {
      editData = await 

*/
  async shouldConfirmExecute(
    params: EditToolParams,
    abortSignal: AbortSignal,
  ): Promise<ToolCallConfirmationDetails | false> {
    if (this.config.getApprovalMode() === ApprovalMode.AUTO_EDIT) {
      return false;
    }
    const validationError = this.validateToolParams(params);
    if (validationError) {
      console.error(
        `[EditTool Wrapper] Attempted confirmation with invalid parameters: ${validationError}`,
      );
      return false;
    }

    let editData: CalculatedEdit;
    try {
      editData = await this.calculateEdit(params, abortSignal);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      console.log(`Error preparing edit: ${errorMsg}`);
      return false;
    }

    if (editData.error) {
      console.log(`Error: ${editData.error.display}`);
      return false;
    }

    const fileName = path.basename(params.file_path);
    const fileDiff = Diff.createPatch(
      fileName,
      editData.currentContent ?? '',
      editData.newContent,
      'Current',
      'Proposed',
      DEFAULT_DIFF_OPTIONS,
    );
    const confirmationDetails: ToolEditConfirmationDetails = {
      type: 'edit',
      title: `Confirm Edit: ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`,
      fileName,
      fileDiff,
      onConfirm: async (outcome: ToolConfirmationOutcome) => {
        if (outcome === ToolConfirmationOutcome.ProceedAlways) {
          this.config.setApprovalMode(ApprovalMode.AUTO_EDIT);
        }
      },
    };
    return confirmationDetails;
  }

  getDescription(params: EditToolParams): string {
    if (!params.file_path || !params.old_string || !params.new_string) {
      return `Model 

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import * as fs from 'fs';
import * as path from 'path';
import * as Diff from 'diff';
import {
  BaseTool,
  ToolCallConfirmationDetails,
  ToolConfirmationOutcome,
  ToolEditConfirmationDetails,
  ToolResult,
  ToolResultDisplay,
} from './tools.js';
import { Type } from '@google/genai';
import { SchemaValidator } from '../utils/schemaValidator.js';
import { makeRelative, shortenPath } from '../utils/paths.js';
import { isNodeError } from '../utils/errors.js';
import { Config, ApprovalMode } from '../config/config.js';
import { ensureCorrectEdit } from '../utils/editCorrector.js';
import { DEFAULT_DIFF_OPTIONS } from './diffOptions.js';
import { ReadFileTool } from './read-file.js';
import { ModifiableTool, ModifyContext } from './modifiable-tool.js';

/**
 * Parameters for the Edit tool
 */
export interface EditToolParams {
  /**
   * The absolute path to the file to modify
   */
  file_path: string;

  /**
   * The text to replace
   */
  old_string: string;

  /**
   * The text to replace it with
   */
  new_string: string;

  /**
   * Number of replacements expected. Defaults to 1 if not specified.
   * Use when you want to replace multiple occurrences.
   */
  expected_replacements?: number;

  /**
   * Whether the edit was modified manually by the user.
   */
  modified_by_user?: boolean;
}

interface CalculatedEdit {
  currentContent: string | null;
  newContent: string;
  occurrences: number;
  error?: { display: string; raw: string };
  isNewFile: boolean;
}

/**
 * Implementation of the Edit tool logic
 */
export class EditTool
  extends BaseTool<EditToolParams, ToolResult>
  implements ModifiableTool<EditToolParams>
{
  static readonly Name = 'replace';
  private readonly rootDirectory: string;

  /**
   * Creates a new instance of the EditLogic
   * @param rootDirectory Root directory to ground this tool in.
   */
  constructor(private readonly config: Config) {
    super(
      EditTool.Name,
      'Edit',
      `Replaces text within a file. By default, replaces a single occurrence, but can replace multiple occurrences when \`expected_replacements\` is specified. This tool requires providing significant context around the change to ensure precise targeting. Always use the ${ReadFileTool.Name} tool to examine the 

 `User modified the \`new_string\` content to be: ${params.new_string}.`,
        );
      }

      return {
        llmContent: llmSuccessMessageParts.join(' '),
        returnDisplay: displayResult,
      };
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      return {
        llmContent: `Error executing edit: ${errorMsg}`,
        returnDisplay: `Error writing file: ${errorMsg}`,
      };
    }
  }

  /**
   * Creates parent directories if they don't exist
   */
  private ensureParentDirectoriesExist(filePath: string): void {
    const dirName = path.dirname(filePath);
    if (!fs.existsSync(dirName)) {
      fs.mkdirSync(dirName, { recursive: true });
    }
  }

  getModifyContext(_: AbortSignal): ModifyContext<EditToolParams> {
    return {
      getFilePath: (params: EditToolParams) => params.file_path,
      getCurrentContent: async (params: EditToolParams): Promise<string> => {
        try {
          return fs.readFileSync(params.file_path, 'utf8');
        } catch (err) {
          if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
          return '';
        }
      },
      getProposedContent: async (params: EditToolParams): Promise<string> => {
        try {
          const currentContent = fs.readFileSync(params.file_path, 'utf8');
          return this._applyReplacement(
            currentContent,
            params.old_string,
     

* @returns Result of the edit operation
   */
  async execute(
    params: EditToolParams,
    signal: AbortSignal,
  ): Promise<ToolResult> {
    const validationError = this.validateToolParams(params);
    if (validationError) {
      return {
        llmContent: `Error: Invalid parameters provided. Reason: ${validationError}`,
        returnDisplay: `Error: ${validationError}`,
      };
    }

    let editData: CalculatedEdit;
    try {
      editData = await this.calculateEdit(params, signal);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      return {
        llmContent: `Error preparing edit: ${errorMsg}`,
        returnDisplay: `Error preparing edit: ${errorMsg}`,
      };
    }

    if (editData.error) {
      return {
        llmContent: editData.error.raw,
        returnDisplay: `Error: ${editData.error.display}`,
      };
    }

    try {
      this.ensureParentDirectoriesExist(params.file_path);
      fs.writeFileSync(params.file_path, editData.newContent, 'utf8');

      let displayResult: ToolResultDisplay;
      if (editData.isNewFile) {
        displayResult = `Created ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`;
      } else {
        // Generate diff for display, even though core logic doesn't technically need it
        // The CLI wrapper will use this part of the ToolResult
        const fileName = path.basename(params.file_path);
        const fileDiff = Diff.createPatch(
          fileName,
          editData.currentContent ?? '', // Should not be null here 

## File Path: packages/core/src/tools/edit.test.ts

ensureCorrectEdit for diff generation', async () => {
      const originalContent = 'This is the original string to be replaced.';
      const originalOldString = 'original string';
      const originalNewString = 'new string';

      const correctedOldString = 'original string to be replaced'; // More specific
      const correctedNewString = 'completely new string'; // Different replacement
      const expectedFinalContent = 'This is the completely new string.';

      fs.writeFileSync(filePath, originalContent);
      const params: EditToolParams = {
        file_path: filePath,
        old_string: originalOldString,
        new_string: originalNewString,
      };

      // The main beforeEach already calls mockEnsureCorrectEdit.mockReset()
      // Set a specific mock for this test case
      let mockCalled = false;
      mockEnsureCorrectEdit.mockImplementationOnce(
        async (_, content, p, client) => {
          mockCalled = true;
          expect(content).toBe(originalContent);
          expect(p).toBe(params);
          expect(client).toBe(geminiClient);
          return {
            params: {
              file_path: filePath,
              old_string: correctedOldString,
              new_string: correctedNewString,
            },
            occurrences: 1,
          };
        },
    

file/);
      expect(fs.readFileSync(filePath, 'utf8')).toBe(
        'new text new text new text',
      );
      const display = result.returnDisplay as FileDiff;
      expect(display.fileDiff).toMatch(/old text old text old text/);
      expect(display.fileDiff).toMatch(/new text new text new text/);
      expect(display.fileName).toBe(testFile);
    });

    it('should return error if expected_replacements does not match actual occurrences', async () => {
      fs.writeFileSync(filePath, 'old text old text', 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
        expected_replacements: 3, // Expecting 3 but only 2 exist
      };
      const result = await tool.execute(params, new AbortController().signal);
      expect(result.llmContent).toMatch(
        /Expected 3 occurrences but found 2 for old_string in file/,
      );
      expect(result.returnDisplay).toMatch(
        /Failed to edit, expected 3 occurrences but found 2/,
      );
    });

    it('should return error if trying to create a file that already exists (empty old_string)', async () => {
      fs.writeFileSync(filePath, 'Existing content', 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: '',
        new_string: 'new content',
      };
      const result = await tool.execute(params, new AbortController().signal);
      expect(result.llmContent).toMatch(/File already exists, cannot create/);
      expect(result.returnDisplay).toMatch(
        /Attempted to create a file that already exists/,
    

 } else if (params.old_string === '') {
          occurrences = 0;
        }
        return { params, occurrences };
      });
    });

    it('should return error if params are invalid', async () => {
      const params: EditToolParams = {
        file_path: 'relative.txt',
        old_string: 'old',
        new_string: 'new',
      };
      const result = await tool.execute(params, new AbortController().signal);
      expect(result.llmContent).toMatch(/Error: Invalid parameters provided/);
      expect(result.returnDisplay).toMatch(/Error: File path must be absolute/);
    });

    it('should edit an existing file and return diff with fileName', async () => {
      const initialContent = 'This is some old text.';
      const newContent = 'This is some new text.'; // old -> new
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
      };

      // Specific mock for this test's execution path in calculateEdit
      // ensureCorrectEdit is NOT called by calculateEdit, only by shouldConfirmExecute
      // So, the default mockEnsureCorrectEdit should correctly return 1 occurrence for 'old' in initialContent

      // Simulate confirmation by setting shouldAlwaysEdit
      (tool as any).shouldAlwaysEdit = true;

      const result = await tool.execute(params, new AbortController().signal);

      (tool as any).shouldAlwaysEdit = false; // Reset for other tests

      expect(result.llmContent).toMatch(/Successfully modified file/);
     

   );
    });

    it('should not include modification message when proposed content is not modified', async () => {
      const initialContent = 'This is some old text.';
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
        modified_by_user: false,
      };

      (mockConfig.getApprovalMode as Mock).mockReturnValueOnce(
        ApprovalMode.AUTO_EDIT,
      );
      const result = await tool.execute(params, new AbortController().signal);

      expect(result.llmContent).not.toMatch(
        /User modified the `new_string` content/,
      );
    });

    it('should not include modification message when modified_by_user is not provided', async () => {
      const initialContent = 'This is some old text.';
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
      };

      (mockConfig.getApprovalMode as Mock).mockReturnValueOnce(
        ApprovalMode.AUTO_EDIT,
      );
      const result = await tool.execute(params, new AbortController().signal);

      expect(result.llmContent).not.toMatch(
        /User modified the `new_string` content/,
      );
    });
  });

  describe('getDescription', () => {
    it('should return "No file changes to..." if old_string and new_string are the same', () => {
      const testFileName = 'test.txt';
      const params: EditToolParams = {
  

       ApprovalMode.AUTO_EDIT,
      );
      const result = await tool.execute(params, new AbortController().signal);

      expect(result.llmContent).not.toMatch(
        /User modified the `new_string` content/,
      );
    });
  });

  describe('getDescription', () => {
    it('should return "No file changes to..." if old_string and new_string are the same', () => {
      const testFileName = 'test.txt';
      const params: EditToolParams = {
        file_path: path.join(rootDir, testFileName),
        old_string: 'identical_string',
        new_string: 'identical_string',
      };
      // shortenPath will be called internally, resulting in just the file name
      expect(tool.getDescription(params)).toBe(
        `No file changes to ${testFileName}`,
      );
    });

    it('should return a snippet of old and new strings if they are different', () => {
      const testFileName = 'test.txt';
      const params: EditToolParams = {
        file_path: path.join(rootDir, testFileName),
        old_string: 'this is the old string value',
        new_string: 'this is the new string value',
      };
      // shortenPath will be called internally, resulting in just the file name
      // The snippets are truncated at 30 chars + '...'
      expect(tool.getDescription(params)).toBe(
        `${testFileName}: this is the old string value => this is the new string value`,
      );
    });

    it('should handle very short strings correctly in the description', () => {
      const 

{
      fs.writeFileSync(filePath, 'Existing content', 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: '',
        new_string: 'new content',
      };
      const result = await tool.execute(params, new AbortController().signal);
      expect(result.llmContent).toMatch(/File already exists, cannot create/);
      expect(result.returnDisplay).toMatch(
        /Attempted to create a file that already exists/,
      );
    });

    it('should include modification message when proposed content is modified', async () => {
      const initialContent = 'This is some old text.';
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
        modified_by_user: true,
      };

      (mockConfig.getApprovalMode as Mock).mockReturnValueOnce(
        ApprovalMode.AUTO_EDIT,
      );
      const result = await tool.execute(params, new AbortController().signal);

      expect(result.llmContent).toMatch(
        /User modified the `new_string` content/,
      );
    });

    it('should not include modification message when proposed content is not modified', async () => {
      const initialContent = 'This is some old text.';
      fs.writeFileSync(filePath, initialContent, 'utf8');
      const params: EditToolParams = {
        file_path: filePath,
        old_string: 'old',
        new_string: 'new',
        modified_by_user: false,
      };

  

## File Path: packages/core/src/tools/modifiable-tool.test.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  vi,
  describe,
  it,
  expect,
  beforeEach,
  afterEach,
  type Mock,
} from 'vitest';
import {
  modifyWithEditor,
  ModifyContext,
  ModifiableTool,
  isModifiableTool,
} from './modifiable-tool.js';
import { EditorType } from '../utils/editor.js';
import fs from 'fs';
import os from 'os';
import * as path from 'path';

// Mock dependencies
const mockOpenDiff = vi.hoisted(() => vi.fn());
const mockCreatePatch = vi.hoisted(() => vi.fn());

vi.mock('../utils/editor.js', () => ({
  openDiff: mockOpenDiff,
}));

vi.mock('diff', () => ({
  createPatch: mockCreatePatch,
}));

vi.mock('fs');
vi.mock('os');

interface TestParams {
  filePath: string;
  someOtherParam: string;
  modifiedContent?: string;
}

describe('modifyWithEditor', () => {
  let tempDir: string;
  let mockModifyContext: ModifyContext<TestParams>;
  let mockParams: TestParams;
  let currentContent: string;
  let proposedContent: string;
  let modifiedContent: string;
  let abortSignal: AbortSignal;

  beforeEach(() => {
    vi.resetAllMocks();

    tempDir = '/tmp/test-dir';
    abortSignal = new AbortController().signal;

    currentContent = 'original content\nline 2\nline 3';
    proposedContent = 'modified content\nline 2\nline 3';
    modifiedContent = 'user modified content\nline 2\nline 3\nnew line';
    mockParams = {
      filePath: path.join(tempDir, 'test.txt'),
      someOtherParam: 'value',
    };

    mockModifyContext = {
      getFilePath: vi.fn().mockReturnValue(mockParams.filePath),
      getCurrentContent: vi.fn().mockResolvedValue(currentContent),
      getProposedContent: vi.fn().mockResolvedValue(proposedContent),
      createUpdatedParams: vi
        .fn()
        .mockImplementation((oldContent, modifiedContent, originalParams) => ({
          ...originalParams,
          modifiedContent,
          oldContent,
        })),
    };

    (os.tmpdir as Mock).mockReturnValue(tempDir);

    (fs.existsSync as Mock).mockReturnValue(true);
    (fs.mkdirSync as Mock).mockImplementation(() => undefined);
    (fs.writeFileSync as Mock).mockImplementation(() => {});
    (fs.unlinkSync as Mock).mockImplementation(() => {});

    (fs.readFileSync as Mock).mockImplementation((filePath: string) => {
      if (filePath.includes('-new-')) {
    

   mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    expect(consoleErrorSpy).toHaveBeenCalledTimes(2);
    expect(consoleErrorSpy).toHaveBeenCalledWith(
      expect.stringContaining('Error deleting temp diff file:'),
    );

    consoleErrorSpy.mockRestore();
  });

  it('should create temp files with correct naming with extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file.txt');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    const writeFileCalls = (fs.writeFileSync as Mock).mock.calls;
    expect(writeFileCalls).toHaveLength(2);

    const oldFilePath = writeFileCalls[0][0];
    const newFilePath = writeFileCalls[1][0];

    expect(oldFilePath).toMatch(/gemini-cli-modify-test-file-old-\d+\.txt$/);
    expect(newFilePath).toMatch(/gemini-cli-modify-test-file-new-\d+\.txt$/);
    expect(oldFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
    expect(newFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
  });

  it('should create temp files with correct naming without extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    const writeFileCalls = (fs.writeFileSync as Mock).mock.calls;
    expect(writeFileCalls).toHaveLength(2);

    const oldFilePath = writeFileCalls[0][0];
    const newFilePath = writeFileCalls[1][0];

    expect(oldFilePath).toMatch(/gemini-cli-modify-test-file-old-\d+$/);
    expect(newFilePath).toMatch(/gemini-cli-modify-test-file-new-\d+$/);
    expect(oldFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
    expect(newFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
  });
});

describe('isModifiableTool', () => {
  it('should return true for objects with getModifyContext method', () => {
    const mockTool = {
      name: 'test-tool',
      getModifyContext: vi.fn(),
    } as unknown as ModifiableTool<TestParams>;

    expect(isModifiableTool(mockTool)).toBe(true);
  });

  it('should return false for objects without getModifyContext method', () => {
    const mockTool = {
      name: 'test-tool',
 

    );
      expect(mockModifyContext.getFilePath).toHaveBeenCalledWith(mockParams);

      expect(fs.writeFileSync).toHaveBeenCalledTimes(2);
      expect(fs.writeFileSync).toHaveBeenNthCalledWith(
        1,
        expect.stringContaining(
          path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        ),
        currentContent,
        'utf8',
      );
      expect(fs.writeFileSync).toHaveBeenNthCalledWith(
        2,
        expect.stringContaining(
          path.join(tempDir, 'gemini-cli-tool-modify-diffs'),
        ),
        proposedContent,
        'utf8',
      );

      expect(mockOpenDiff).toHaveBeenCalledWith(
        expect.stringContaining('-old-'),
        expect.stringContaining('-new-'),
        'vscode',
      );

      expect(fs.readFileSync).toHaveBeenCalledWith(
        expect.stringContaining('-old-'),
        'utf8',
      );
      expect(fs.readFileSync).toHaveBeenCalledWith(
        expect.stringContaining('-new-'),
        'utf8',
      );

      expect(mockModifyContext.createUpdatedParams).toHaveBeenCalledWith(
        currentContent,
        modifiedContent,
        mockParams,
      );

      expect(mockCreatePatch).toHaveBeenCalledWith(
        path.basename(mockParams.filePath),
        currentContent,
        modifiedContent,
        'Current',
        'Proposed',
        expect.objectContaining({
          context: 3,
          ignoreWhitespace: true,
    

## File Path: packages/core/src/tools/write-file.test.ts

     mockGeminiClientInstance,
        abortSignal,
      );
      expect(confirmation).toEqual(
        expect.objectContaining({
          title: `Confirm Write: ${path.basename(filePath)}`,
          fileName: 'confirm_new_file.txt',
          fileDiff: expect.stringContaining(correctedContent),
        }),
      );
      expect(confirmation.fileDiff).toMatch(
        /--- confirm_new_file.txt\tCurrent/,
      );
      expect(confirmation.fileDiff).toMatch(
        /\+\+\+ confirm_new_file.txt\tProposed/,
      );
    });

    it('should request confirmation with diff for an existing file (with corrected content)', async () => {
      const filePath = path.join(rootDir, 'confirm_existing_file.txt');
      const originalContent = 'Original content for confirmation.';
      const proposedContent = 'Proposed replacement for confirmation.';
      const correctedProposedContent =
        'Corrected replacement for confirmation.';
      fs.writeFileSync(filePath, originalContent, 'utf8');

      mockEnsureCorrectEdit.mockResolvedValue({
        params: {
          file_path: filePath,
          old_string: originalContent,
          new_string: correctedProposedContent,
        },
        occurrences: 1,
      });

      const params = { file_path: filePath, content: proposedContent };
      const confirmation = (await tool.shouldConfirmExecute(
        params,
        abortSignal,
      )) as ToolEditConfirmationDetails;

      expect(mockEnsureCorrectEdit).toHaveBeenCalledWith(
        filePath,
        originalContent,
 

## File Path: packages/core/src/core/coreToolScheduler.ts

    // If the client provided new content, apply it before scheduling.
      if (payload?.newContent && toolCall) {
        await this._applyInlineModify(
          toolCall as WaitingToolCall,
          payload,
          signal,
        );
      }
      this.setStatusInternal(callId, 'scheduled');
    }
    this.attemptExecutionOfScheduledCalls(signal);
  }

  /**
   * Applies user-provided content changes to a tool call that is awaiting confirmation.
   * This method updates the tool's arguments and refreshes the confirmation prompt with a new diff
   * before the tool is scheduled for execution.
   * @private
   */
  private async _applyInlineModify(
    toolCall: WaitingToolCall,
    payload: ToolConfirmationPayload,
    signal: AbortSignal,
  ): Promise<void> {
    if (
      toolCall.confirmationDetails.type !== 'edit' ||
      !isModifiableTool(toolCall.tool)
    ) {
      return;
    }

    const modifyContext = toolCall.tool.getModifyContext(signal);
    const currentContent = await modifyContext.getCurrentContent(
      toolCall.request.args,
    );

    const updatedParams = modifyContext.createUpdatedParams(
      currentContent,
      payload.newContent,
      toolCall.request.args,
    );
    const updatedDiff = Diff.createPatch(
      modifyContext.getFilePath(toolCall.request.args),
      currentContent,
      payload.newContent,
      'Current',
      'Proposed',
    );

    this.setArgsInternal(toolCall.request.callId, updatedParams);
    this.setStatusInternal(toolCall.request.callId, 'awaiting_approval', {
      ...toolCall.confirmationDetails,
      fileDiff: updatedDiff,
    });
  }

  private attemptExecutionOfScheduledCalls(signal: AbortSignal): void {
    const allCallsFinalOrScheduled = this.toolCalls.every(
      

## File Path: packages/core/src/core/prompts.test.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { describe, it, expect, vi } from 'vitest';
import { getCoreSystemPrompt } from './prompts.js';
import { isGitRepository } from '../utils/gitUtils.js';

// Mock tool names if they are dynamically generated or complex
vi.mock('../tools/ls', () => ({ LSTool: { Name: 'list_directory' } }));
vi.mock('../tools/edit', () => ({ EditTool: { Name: 'replace' } }));
vi.mock('../tools/glob', () => ({ GlobTool: { Name: 'glob' } }));
vi.mock('../tools/grep', () => ({ GrepTool: { Name: 'search_file_content' } }));
vi.mock('../tools/read-file', () => ({ ReadFileTool: { Name: 'read_file' } }));
vi.mock('../tools/read-many-files', () => ({
  ReadManyFilesTool: { Name: 'read_many_files' },
}));
vi.mock('../tools/shell', () => ({
  ShellTool: { Name: 'run_shell_command' },
}));
vi.mock('../tools/write-file', () => ({
  WriteFileTool: { Name: 'write_file' },
}));
vi.mock('../utils/gitUtils', () => ({
  isGitRepository: vi.fn(),
}));

describe('Core System Prompt (prompts.ts)', () => {
  it('should return the base prompt when no userMemory is provided', () => {
    vi.stubEnv('SANDBOX', undefined);
    const prompt = getCoreSystemPrompt();
    expect(prompt).not.toContain('---\n\n'); // Separator should not be present
    expect(prompt).toContain('You are an interactive CLI agent'); // Check for core content
    expect(prompt).toMatchSnapshot(); // Use snapshot for base prompt structure
  });

  it('should return the base prompt when userMemory is empty string', () => {
    vi.stubEnv('SANDBOX', undefined);
    const prompt = getCoreSystemPrompt('');
    expect(prompt).not.toContain('---\n\n');
    expect(prompt).toContain('You are an interactive CLI agent');
    expect(prompt).toMatchSnapshot();
  });

  it('should return the base prompt when userMemory is whitespace only', () => {
    vi.stubEnv('SANDBOX', undefined);
    const prompt = getCoreSystemPrompt('   \n  \t ');
    expect(prompt).not.toContain('---\n\n');
    expect(prompt).toContain('You are an interactive CLI agent');
    expect(prompt).toMatchSnapshot();
  });

  it('should append userMemory with separator when provided', () => {
    vi.stubEnv('SANDBOX', undefined);
    const memory = 'This is custom user memory.\nBe extra polite.';
    const expectedSuffix = `\n\n---\n\n${memory}`;
    const prompt = getCoreSystemPrompt(memory);

    expect(prompt.endsWith(expectedSuffix)).toBe(true);
    expect(prompt).toContain('You are an interactive CLI agent'); // Ensure base prompt follows
    expect(prompt).toMatchSnapshot(); // Snapshot 
<END_OF_CONTEXT>

<query>
既然是 diff-based edit，那么提供的prompt 是什么
</query>

Assistant: 
2025-07-10 03:24:01,736 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 03:24:01,764 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 03:24:03,503 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:24:34,741 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
在 `gemini-cli` 项目中，diff-based edit 的实现涉及多个工具和函数。根据提供的代码分析，以下与 diff-based edit 相关的 prompt 或描述信息可以总结如下：

1. **EditTool 的 `getDescription` 方法**  
   该方法返回用于描述编辑操作的简短文本，通常用于用户界面或日志记录。它会根据 `old_string` 和 `new_string` 是否相同，返回不同的描述：

   ```ts
   getDescription(params: EditToolParams): string {
     if (!params.file_path || !params.old_string || !params.new_string) {
       return `Model did not provide valid parameters for edit tool`;
     }
     const relativePath = makeRelative(params.file_path, this.rootDirectory);
     if (params.old_string === '') {
       return `Create ${shortenPath(relativePath)}`;
     }

     const oldStringSnippet =
       params.old_string.split('\n')[0].substring(0, 30) +
       (params.old_string.length > 30 ? '...' : '');
     const newStringSnippet =
       params.new_string.split('\n')[0].substring(0, 30) +
       (params.new_string.length > 30 ? '...' : '');

     if (params.old_string === params.new_string) {
       return `No file changes to ${shortenPath(relativePath)}`;
     }
     return `${shortenPath(relativePath)}: ${oldStringSnippet} => ${newStringSnippet}`;
   }
   ```

   - 如果 `old_string` 为空，则表示创建新文件。
   - 如果 `old_string` 和 `new_string` 相同，则表示没有实际修改。
   - 否则，显示文件名和内容片段的前后对比。

2. **执行编辑时的提示信息（prompt）**  
   在 `execute` 方法中，成功执行编辑后会生成一条消息，可能用于 LLM 返回的响应中，例如：

   ```ts
   const llmSuccessMessageParts = [
     editData.isNewFile
       ? `Created new file: ${params.file_path} with provided content.`
       : `Successfully modified file: ${params.file_path} (${editData.occurrences} replacements).`,
   ];
   if (params.modified_by_user) {
     llmSuccessMessageParts.push(
       `User modified the \`new_string\` content to be: ${params.new_string}.`,
     );
   }
   ```

   - 如果是新建文件，提示信息为：`Created new file: {file_path} with provided content.`
   - 如果是修改现有文件，提示信息为：`Successfully modified file: {file_path} ({number of replacements} replacements).`
   - 如果用户修改了 `new_string`，会附加说明：`User modified the new_string content to be: {new_string}`.

3. **确认编辑操作的提示（Confirmation Prompt）**  
   在 `shouldConfirmExecute` 方法中，如果配置要求确认编辑操作，则会生成一个确认提示，例如：

   ```ts
   const confirmationDetails: ToolEditConfirmationDetails = {
     type: 'edit',
     title: `Confirm Edit: ${shortenPath(makeRelative(params.file_path, this.rootDirectory))}`,
     fileName,
     fileDiff,
     onConfirm: async (outcome: ToolConfirmationOutcome) => {
       if (outcome === ToolConfirmationOutcome.ProceedAlways) {
         this.config.setApprovalMode(ApprovalMode.AUTO_EDIT);
       }
     },
   };
   ```

   - 提示标题为：`Confirm Edit: {file_path}`。
   - 显示文件名和生成的 diff 内容。
   - 用户可以选择是否确认执行编辑。

4. **ModifiableTool 中的修改提示**  
   在 `modifyWithEditor` 函数中，当用户使用编辑器修改文件时，会生成 diff 并返回更新后的参数和 diff 信息。虽然不直接包含 prompt，但 diff 会被传递给确认流程，用于用户确认。

5. **测试用例中的提示样例**  
   在测试文件 `edit.test.ts` 中，有多个测试用例验证了提示信息的正确性，例如：

   ```ts
   expect(result.llmContent).toMatch(/Successfully modified file/);
   expect(result.llmContent).toMatch(/User modified the `new_string` content/);
   ```

   这些测试表明，LLM 返回的提示信息会明确说明编辑操作是否成功，以及是否涉及用户修改的内容。

### 总结
在 `gemini-cli` 中，diff-based edit 的 prompt 主要包括以下几种类型：

- **描述性 prompt**：用于展示编辑操作的简要描述，如“修改了文件”或“创建了新文件”。
- **成功提示**：LLM 返回的提示，说明编辑操作的结果，如“成功修改了文件”或“用户修改了内容”。
- **确认提示**：在用户需要确认编辑操作时，会显示文件名和 diff，供用户确认。
- **错误提示**：如果编辑操作失败，会返回错误信息，如“文件已存在，无法创建”或“预期替换次数与实际不匹配”。

这些 prompt 有助于用户理解当前的编辑操作，并在需要时进行确认或调整。
2025-07-10 03:25:17,213 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 22 tokens
2025-07-10 03:25:17,242 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:25:17,244 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:25:17,244 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:25:17,244 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:25:17,277 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:25:17,284 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['input_str', 'conversation_history', 'contexts', 'output_format_str', 'system_prompt']
2025-07-10 03:25:17,284 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 03:25:17,285 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/google-gemini/gemini-cli...
2025-07-10 03:25:17,285 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: google-gemini_gemini-cli
2025-07-10 03:25:17,285 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/google-gemini_gemini-cli. Using existing repository.
2025-07-10 03:25:17,285 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/google-gemini_gemini-cli', 'save_db_file': '/root/.adalflow/databases/google-gemini_gemini-cli.pkl'}
2025-07-10 03:25:17,285 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 03:25:17,556 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 03:25:17,557 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:25:17,557 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:25:17,557 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:25:17,613 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:25:17,613 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:25:17,613 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:25:17,671 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 1962 documents from existing database
2025-07-10 03:25:17,671 - INFO - api.rag - rag.py:421 - Loaded 1962 documents for retrieval
2025-07-10 03:25:17,672 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 1962 documents)
2025-07-10 03:25:17,673 - INFO - api.rag - rag.py:384 - Embedding validation complete: 1962/1962 documents have valid embeddings
2025-07-10 03:25:17,673 - INFO - api.rag - rag.py:429 - Using 1962 documents with valid embeddings for retrieval
2025-07-10 03:25:17,674 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:25:17,694 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 1962 chunks
2025-07-10 03:25:17,694 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 03:25:17,694 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/google-gemini/gemini-cli
2025-07-10 03:25:17,694 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['给gemini 提供了什么提示词，让他进行基于 diff-based edit']}
2025-07-10 03:25:18,112 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 03:25:18,114 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 03:25:18,115 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:25:18,115 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:25:18,115 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/google-gemini/gemini-cli (gemini-cli).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: packages/core/src/utils/editCorrector.test.ts

  old_string: 'find me',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find me');
        expect(result.occurrences).toBe(1);
      });
      it('Test 1.3: old_string (with literal \\), new_string (escaped by Gemini) -> new_string unchanged (still escaped)', async () => {
        const currentContent = 'This is a test string to find\\me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find\\me',
          new_string: 'replace with \\"this\\"',
        };
        mockResponses.push({
          corrected_new_string_escaping: 'replace with "this"',
        });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
  

    originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find\\me');
        expect(result.occurrences).toBe(1);
      });
    });

    describe('Scenario Group 2: originalParams.old_string does NOT match, but unescapeStringForGeminiBug(originalParams.old_string) DOES match', () => {
      it('Test 2.1: old_string (over-escaped, no intended literal \\), new_string (escaped by Gemini) -> new_string unescaped', async () => {
        const currentContent = 'This is a test string to find "me".';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find \\"me\\"',
          new_string: 'replace with \\"this\\"',
        };
        mockResponses.push({ corrected_new_string: 'replace with "this"' });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find "me"');
        expect(result.occurrences).toBe(1);
      });
      it('Test 2.2: old_string (over-escaped, no intended literal \\), new_string (correctly formatted) -> new_string unescaped (harmlessly)', async () => {
        const currentContent = 'This 

     mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find "me"');
        expect(result.occurrences).toBe(1);
      });
      it('Test 2.2: old_string (over-escaped, no intended literal \\), new_string (correctly formatted) -> new_string unescaped (harmlessly)', async () => {
        const currentContent = 'This is a test string to find "me".';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find \\"me\\"',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find "me"');
        expect(result.occurrences).toBe(1);
      });
      it('Test 2.3: old_string (over-escaped, with intended literal \\), new_string (simple) -> new_string corrected', async () => {
        const currentContent = 'This is a test string to find \\me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find \\\\me',
    

       file_path: '/test/file.txt',
          old_string: 'find me',
          new_string: 'replace with \\"this\\"',
        };
        mockResponses.push({
          corrected_new_string_escaping: 'replace with "this"',
        });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find me');
        expect(result.occurrences).toBe(1);
      });
      it('Test 1.2: old_string (no literal \\), new_string (correctly formatted) -> new_string unchanged', async () => {
        const currentContent = 'This is a test string to find me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find me',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
     

'replace with "this"',
        });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find\\me');
        expect(result.occurrences).toBe(1);
      });
      it('Test 1.4: old_string (with literal \\), new_string (correctly formatted) -> new_string unchanged', async () => {
        const currentContent = 'This is a test string to find\\me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find\\me',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find\\me');
        expect(result.occurrences).toBe(1);
      });
    });

    describe('Scenario Group 2: originalParams.old_string does NOT match, but unescapeStringForGeminiBug(originalParams.old_string) DOES match', () => {
      it('Test 

= {
          file_path: '/test/file.txt',
          old_string: 'find "me"',
          new_string: 'some new string',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params).toEqual(originalParams);
        expect(result.occurrences).toBe(2);
      });
    });

    describe('Scenario Group 5: Specific unescapeStringForGeminiBug checks (integrated into ensureCorrectEdit)', () => {
      it('Test 5.1: old_string needs LLM to become currentContent, new_string also needs correction', async () => {
        const currentContent = 'const x = "a\nbc\\"def\\"';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'const x = \\"a\\nbc\\\\"def\\\\"',
          new_string: 'const y = \\"new\\nval\\\\"content\\\\"',
        };
        const expectedFinalNewString = 'const y = "new\nval\\"content\\"';
        mockResponses.push({ corrected_target_snippet: currentContent });
        mockResponses.push({ corrected_new_string: expectedFinalNewString });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
     

       // Check if the signal is aborted. If so, throw an error or return a specific response.
          if (signal && signal.aborted) {
            return Promise.reject(new Error('Aborted')); // Or some other specific error/response
          }
          const response = mockResponses[callCount];
          callCount++;
          if (response === undefined) return Promise.resolve({});
          return Promise.resolve(response);
        });
      mockStartChat = vi.fn();
      mockSendMessageStream = vi.fn();

      mockGeminiClientInstance = new GeminiClient(
        mockConfigInstance,
      ) as Mocked<GeminiClient>;
      mockGeminiClientInstance.getHistory = vi.fn().mockResolvedValue([]);
      resetEditCorrectorCaches_TEST_ONLY();
    });

    describe('Scenario Group 1: originalParams.old_string matches currentContent directly', () => {
      it('Test 1.1: old_string (no literal \\), new_string (escaped by Gemini) -> new_string unescaped', async () => {
        const currentContent = 'This is a test string to find me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find me',
          new_string: 'replace with \\"this\\"',
        };
        mockResponses.push({
          corrected_new_string_escaping: 'replace with "this"',
        });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
        

## File Path: packages/cli/src/ui/components/AuthDialog.tsx

     </Box>
      <Box marginTop={1}>
        <Text>Terms of Services and Privacy Notice for Gemini CLI</Text>
      </Box>
      <Box marginTop={1}>
        <Text color={Colors.AccentBlue}>
          {
            'https://github.com/google-gemini/gemini-cli/blob/main/docs/tos-privacy.md'
          }
        </Text>
      </Box>
    </Box>
  );
}


## File Path: packages/cli/src/ui/commands/helpCommand.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { OpenDialogActionReturn, SlashCommand } from './types.js';

export const helpCommand: SlashCommand = {
  name: 'help',
  altName: '?',
  description: 'for help on gemini-cli',
  action: (_context, _args): OpenDialogActionReturn => {
    console.debug('Opening help UI ...');
    return {
      type: 'dialog',
      dialog: 'help',
    };
  },
};


## File Path: packages/cli/src/ui/App.tsx

 openEditorDialog();
      return;
    }
    return editorType as EditorType;
  }, [settings, openEditorDialog]);

  const onAuthError = useCallback(() => {
    setAuthError('reauth required');
    openAuthDialog();
  }, [openAuthDialog, setAuthError]);

  const {
    streamingState,
    submitQuery,
    initError,
    pendingHistoryItems: pendingGeminiHistoryItems,
    thought,
  } = useGeminiStream(
    config.getGeminiClient(),
    history,
    addItem,
    setShowHelp,
    config,
    setDebugMessage,
    handleSlashCommand,
    shellModeActive,
    getPreferredEditor,
    onAuthError,
    performMemoryRefresh,
  );
  pendingHistoryItems.push(...pendingGeminiHistoryItems);
  const { elapsedTime, currentLoadingPhrase } =
    useLoadingIndicator(streamingState);
  const showAutoAcceptIndicator = useAutoAcceptIndicator({ config });

  const handleFinalSubmit = useCallback(
    (submittedValue: string) => {
      const trimmedValue = submittedValue.trim();
      if (trimmedValue.length > 0) {
        submitQuery(trimmedValue);
      }
    },
    [submitQuery],
  );

  const logger = useLogger();
  const [userMessages, setUserMessages] = useState<string[]>([]);

  useEffect(() => {
    const fetchUserMessages = async () => {
      const pastMessagesRaw = (await logger?.getPreviousUserMessages()) || []; // Newest first

      const currentSessionUserMessages = history
        .filter(
          (item): item is HistoryItem & { type: 'user'; text: string } =>
            item.type === 'user' &&
            typeof item.text === 'string' &&
            item.text.trim() !== '',
        )
        .map((item) => item.text)
        .reverse(); // Newest first, to match pastMessagesRaw sorting

   

## File Path: packages/core/src/tools/edit.test.ts

ensureCorrectEdit for diff generation', async () => {
      const originalContent = 'This is the original string to be replaced.';
      const originalOldString = 'original string';
      const originalNewString = 'new string';

      const correctedOldString = 'original string to be replaced'; // More specific
      const correctedNewString = 'completely new string'; // Different replacement
      const expectedFinalContent = 'This is the completely new string.';

      fs.writeFileSync(filePath, originalContent);
      const params: EditToolParams = {
        file_path: filePath,
        old_string: originalOldString,
        new_string: originalNewString,
      };

      // The main beforeEach already calls mockEnsureCorrectEdit.mockReset()
      // Set a specific mock for this test case
      let mockCalled = false;
      mockEnsureCorrectEdit.mockImplementationOnce(
        async (_, content, p, client) => {
          mockCalled = true;
          expect(content).toBe(originalContent);
          expect(p).toBe(params);
          expect(client).toBe(geminiClient);
          return {
            params: {
              file_path: filePath,
              old_string: correctedOldString,
              new_string: correctedNewString,
            },
            occurrences: 1,
          };
        },
    

## File Path: packages/cli/src/ui/privacy/CloudFreePrivacyNotice.tsx

Gemini Code Assist for Individuals Privacy Notice
      </Text>
      <Newline />
      <Text>
        This notice and our Privacy Policy
        <Text color={Colors.AccentBlue}>[1]</Text> describe how Gemini Code
        Assist handles your data. Please read them carefully.
      </Text>
      <Newline />
      <Text>
        When you use Gemini Code Assist for individuals with Gemini CLI, Google
        collects your prompts, related code, generated output, code edits,
        related feature usage information, and your feedback to provide,
        improve, and develop Google products and services and machine learning
        technologies.
      </Text>
      <Newline />
      <Text>
        To help with quality and improve our products (such as generative
        machine-learning models), human reviewers may read, annotate, and
        process the data collected above. We take steps to protect your privacy
        as part of this process. This includes disconnecting the data from your
        Google Account before reviewers see or annotate it, and storing those
        disconnected copies for up to 18 months. Please don&apos;t submit
        confidential information or any data you wouldn&apos;t want a reviewer
        to see or Google to use to improve our products, services and
        machine-learning technologies.
      </Text>
      <Newline />
      

## File Path: packages/cli/src/ui/components/Tips.tsx

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import React from 'react';
import { Box, Text } from 'ink';
import { Colors } from '../colors.js';
import { type Config } from '@google/gemini-cli-core';

interface TipsProps {
  config: Config;
}

export const Tips: React.FC<TipsProps> = ({ config }) => {
  const geminiMdFileCount = config.getGeminiMdFileCount();
  return (
    <Box flexDirection="column" marginBottom={1}>
      <Text color={Colors.Foreground}>Tips for getting started:</Text>
      <Text color={Colors.Foreground}>
        1. Ask questions, edit files, or run commands.
      </Text>
      <Text color={Colors.Foreground}>
        2. Be specific for the best results.
      </Text>
      {geminiMdFileCount === 0 && (
        <Text color={Colors.Foreground}>
          3. Create{' '}
          <Text bold color={Colors.AccentPurple}>
            GEMINI.md
          </Text>{' '}
          files to customize your interactions with Gemini.
        </Text>
      )}
      <Text color={Colors.Foreground}>
        {geminiMdFileCount === 0 ? '4.' : '3.'}{' '}
        <Text bold color={Colors.AccentPurple}>
          /help
        </Text>{' '}
        for more information.
      </Text>
    </Box>
  );
};


## File Path: packages/core/src/utils/editCorrector.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  Content,
  GenerateContentConfig,
  SchemaUnion,
  Type,
} from '@google/genai';
import { GeminiClient } from '../core/client.js';
import { EditToolParams, EditTool } from '../tools/edit.js';
import { WriteFileTool } from '../tools/write-file.js';
import { ReadFileTool } from '../tools/read-file.js';
import { ReadManyFilesTool } from '../tools/read-many-files.js';
import { GrepTool } from '../tools/grep.js';
import { LruCache } from './LruCache.js';
import { DEFAULT_GEMINI_FLASH_MODEL } from '../config/models.js';
import {
  isFunctionResponse,
  isFunctionCall,
} from '../utils/messageInspectors.js';
import * as fs from 'fs';

const EditModel = DEFAULT_GEMINI_FLASH_MODEL;
const EditConfig: GenerateContentConfig = {
  thinkingConfig: {
    thinkingBudget: 0,
  },
};

const MAX_CACHE_SIZE = 50;

// Cache for ensureCorrectEdit results
const editCorrectionCache = new LruCache<string, CorrectedEditResult>(
  MAX_CACHE_SIZE,
);

// Cache for ensureCorrectFileContent results
const fileContentCorrectionCache = new LruCache<string, string>(MAX_CACHE_SIZE);

/**
 * Defines the structure of the parameters within CorrectedEditResult
 */
interface CorrectedEditParams {
  file_path: string;
  old_string: string;
  new_string: string;
}

/**
 * Defines the result structure for ensureCorrectEdit.
 */
export interface CorrectedEditResult {
  params: CorrectedEditParams;
  occurrences: number;
}

/**
 * Extracts the timestamp from the .id value, which is in format
 * <tool.name>-<timestamp>-<uuid>
 * @param fcnId the ID value of a functionCall or functionResponse object
 * @returns -1 if the timestamp could not be extracted, else the timestamp (as a number)
 */
function getTimestampFromFunctionId(fcnId: string): number {
  const idParts = fcnId.split('-');
  if (idParts.length > 2) {
    const timestamp = parseInt(idParts[1], 10);
    if (!isNaN(timestamp)) {
      return timestamp;
    }
  }
  return -1;
}

/**
 * Will look through the gemini client history and determine when the most recent
 * edit to a target file occured. If no edit happened, it will return -1
 * @param filePath the path to the file
 * @param client the geminiClient, so that we can get the history
 * @returns a DateTime (as a number) of when the last edit occured, or -1 if no edit was found.
 */
async function findLastEditTimestamp(
  filePath: string,
  client: GeminiClient,
): Promise<number> {
  const history = (await client.getHistory()) ?? [];

  // Tools that may reference the file path in their FunctionResponse `output`.
  const toolsInResp = new Set([
  

and determine when the most recent
 * edit to a target file occured. If no edit happened, it will return -1
 * @param filePath the path to the file
 * @param client the geminiClient, so that we can get the history
 * @returns a DateTime (as a number) of when the last edit occured, or -1 if no edit was found.
 */
async function findLastEditTimestamp(
  filePath: string,
  client: GeminiClient,
): Promise<number> {
  const history = (await client.getHistory()) ?? [];

  // Tools that may reference the file path in their FunctionResponse `output`.
  const toolsInResp = new Set([
    WriteFileTool.Name,
    EditTool.Name,
    ReadManyFilesTool.Name,
    GrepTool.Name,
  ]);
  // Tools that may reference the file path in their FunctionCall `args`.
  const toolsInCall = new Set([...toolsInResp, ReadFileTool.Name]);

  // Iterate backwards to find the most recent relevant action.
  for (const entry of history.slice().reverse()) {
    if (!entry.parts) continue;

    for (const part of entry.parts) {
      let id: string | undefined;
      let content: unknown;

      // Check for a relevant FunctionCall with the file path in its arguments.
      if (
        isFunctionCall(entry) &&
        part.functionCall?.name &&
        toolsInCall.has(part.functionCall.name)
      ) {
        id = part.functionCall.id;
        content = part.functionCall.args;
      }
      // Check for a relevant FunctionResponse with the file path in its output.
      else if (
        isFunctionResponse(entry) &&
        part.functionResponse?.name &&
        toolsInResp.has(part.functionResponse.name)
      ) {
        const { response } = part.functionResponse;
       

## File Path: packages/cli/src/ui/hooks/useGeminiStream.ts

    if (turnCancelledRef.current) {
        // Prevents additional output after a user initiated cancel.
        return '';
      }
      let newGeminiMessageBuffer = currentGeminiMessageBuffer + eventValue;
      if (
        pendingHistoryItemRef.current?.type !== 'gemini' &&
        pendingHistoryItemRef.current?.type !== 'gemini_content'
      ) {
        if (pendingHistoryItemRef.current) {
          addItem(pendingHistoryItemRef.current, userMessageTimestamp);
        }
        setPendingHistoryItem({ type: 'gemini', text: '' });
        newGeminiMessageBuffer = eventValue;
      }
      // Split large messages for better rendering performance. Ideally,
      // we should maximize the amount of output sent to <Static />.
      const splitPoint = findLastSafeSplitPoint(newGeminiMessageBuffer);
      if (splitPoint === newGeminiMessageBuffer.length) {
        // Update the existing message with accumulated content
        setPendingHistoryItem((item) => ({
          type: item?.type as 'gemini' | 'gemini_content',
          text: newGeminiMessageBuffer,
        }));
      } else {
        // This indicates that we need to split up this Gemini Message.
        // Splitting a message is primarily a performance consideration. There is a
        // <Static> component at the root of App.tsx which takes care of rendering
        // content statically or dynamically. Everything but the last message is
        // treated as static in order to prevent 

## File Path: packages/cli/index.ts

#!/usr/bin/env node

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import './src/gemini.js';
import { main } from './src/gemini.js';

// --- Global Entry Point ---
main().catch((error) => {
  console.error('An unexpected critical error occurred:');
  if (error instanceof Error) {
    console.error(error.stack);
  } else {
    console.error(String(error));
  }
  process.exit(1);
});


## File Path: packages/core/src/tools/modifiable-tool.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { EditorType, openDiff } from '../utils/editor.js';
import os from 'os';
import path from 'path';
import fs from 'fs';
import * as Diff from 'diff';
import { DEFAULT_DIFF_OPTIONS } from './diffOptions.js';
import { isNodeError } from '../utils/errors.js';
import { Tool } from './tools.js';

/**
 * A tool that supports a modify operation.
 */
export interface ModifiableTool<ToolParams> extends Tool<ToolParams> {
  getModifyContext(abortSignal: AbortSignal): ModifyContext<ToolParams>;
}

export interface ModifyContext<ToolParams> {
  getFilePath: (params: ToolParams) => string;

  getCurrentContent: (params: ToolParams) => Promise<string>;

  getProposedContent: (params: ToolParams) => Promise<string>;

  createUpdatedParams: (
    oldContent: string,
    modifiedProposedContent: string,
    originalParams: ToolParams,
  ) => ToolParams;
}

export interface ModifyResult<ToolParams> {
  updatedParams: ToolParams;
  updatedDiff: string;
}

export function isModifiableTool<TParams>(
  tool: Tool<TParams>,
): tool is ModifiableTool<TParams> {
  return 'getModifyContext' in tool;
}

function createTempFilesForModify(
  currentContent: string,
  proposedContent: string,
  file_path: string,
): { oldPath: string; newPath: string } {
  const tempDir = os.tmpdir();
  const diffDir = path.join(tempDir, 'gemini-cli-tool-modify-diffs');

  if (!fs.existsSync(diffDir)) {
    fs.mkdirSync(diffDir, { recursive: true });
  }

  const ext = path.extname(file_path);
  const fileName = path.basename(file_path, ext);
  const timestamp = Date.now();
  const tempOldPath = path.join(
    diffDir,
    `gemini-cli-modify-${fileName}-old-${timestamp}${ext}`,
  );
  const tempNewPath = path.join(
    diffDir,
    `gemini-cli-modify-${fileName}-new-${timestamp}${ext}`,
  );

  fs.writeFileSync(tempOldPath, currentContent, 'utf8');
  fs.writeFileSync(tempNewPath, proposedContent, 'utf8');

  return { oldPath: tempOldPath, newPath: tempNewPath };
}

function getUpdatedParams<ToolParams>(
  tmpOldPath: string,
  tempNewPath: string,
  originalParams: ToolParams,
  modifyContext: ModifyContext<ToolParams>,
): { updatedParams: ToolParams; updatedDiff: string } {
  let oldContent = '';
  let newContent = '';

  try {
    oldContent = fs.readFileSync(tmpOldPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    oldContent = '';
  }

  try {
    newContent = fs.readFileSync(tempNewPath, 'utf8');
  } catch (err) {
    if (!isNodeError(err) || err.code !== 'ENOENT') throw err;
    newContent = '';
  }

  const updatedParams = modifyContext.createUpdatedParams(
    oldContent,
    newContent,
    originalParams,
 

## File Path: packages/cli/src/ui/components/StatsDisplay.test.tsx

      tokens: {
            prompt: 100,
            candidates: 100,
            total: 250,
            cached: 50,
            thoughts: 0,
            tool: 0,
          },
        },
      },
      tools: {
        totalCalls: 2,
        totalSuccess: 1,
        totalFail: 1,
        totalDurationMs: 123,
        totalDecisions: { accept: 1, reject: 0, modify: 0 },
        byName: {
          'test-tool': {
            count: 2,
            success: 1,
            fail: 1,
            durationMs: 123,
            decisions: { accept: 1, reject: 0, modify: 0 },
          },
        },
      },
    };

    const { lastFrame } = renderWithMockedStats(metrics);
    const output = lastFrame();

    expect(output).toContain('Performance');
    expect(output).toContain('Interaction Summary');
    expect(output).toContain('User Agreement');
    expect(output).toContain('Savings Highlight');
    expect(output).toContain('gemini-2.5-pro');
    expect(output).toMatchSnapshot();
  });

  describe('Conditional Rendering Tests', () => {
    it('hides User Agreement when no decisions 

## File Path: packages/core/src/tools/modifiable-tool.test.ts

   mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    expect(consoleErrorSpy).toHaveBeenCalledTimes(2);
    expect(consoleErrorSpy).toHaveBeenCalledWith(
      expect.stringContaining('Error deleting temp diff file:'),
    );

    consoleErrorSpy.mockRestore();
  });

  it('should create temp files with correct naming with extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file.txt');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    const writeFileCalls = (fs.writeFileSync as Mock).mock.calls;
    expect(writeFileCalls).toHaveLength(2);

    const oldFilePath = writeFileCalls[0][0];
    const newFilePath = writeFileCalls[1][0];

    expect(oldFilePath).toMatch(/gemini-cli-modify-test-file-old-\d+\.txt$/);
    expect(newFilePath).toMatch(/gemini-cli-modify-test-file-new-\d+\.txt$/);
    expect(oldFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
    expect(newFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
  });

  it('should create temp files with correct naming without extension', async () => {
    const testFilePath = path.join(tempDir, 'subfolder', 'test-file');
    mockModifyContext.getFilePath = vi.fn().mockReturnValue(testFilePath);

    await modifyWithEditor(
      mockParams,
      mockModifyContext,
      'vscode' as EditorType,
      abortSignal,
    );

    const writeFileCalls = (fs.writeFileSync as Mock).mock.calls;
    expect(writeFileCalls).toHaveLength(2);

    const oldFilePath = writeFileCalls[0][0];
    const newFilePath = writeFileCalls[1][0];

    expect(oldFilePath).toMatch(/gemini-cli-modify-test-file-old-\d+$/);
    expect(newFilePath).toMatch(/gemini-cli-modify-test-file-new-\d+$/);
    expect(oldFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
    expect(newFilePath).toContain(`${tempDir}/gemini-cli-tool-modify-diffs/`);
  });
});

describe('isModifiableTool', () => {
  it('should return true for objects with getModifyContext method', () => {
    const mockTool = {
      name: 'test-tool',
      getModifyContext: vi.fn(),
    } as unknown as ModifiableTool<TestParams>;

    expect(isModifiableTool(mockTool)).toBe(true);
  });

  it('should return false for objects without getModifyContext method', () => {
    const mockTool = {
      name: 'test-tool',
 
<END_OF_CONTEXT>

<query>
给gemini 提供了什么提示词，让他进行基于 diff-based edit
</query>

Assistant: 
2025-07-10 03:25:18,115 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 03:25:18,129 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 03:25:19,477 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:25:31,576 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
Gemini 会根据以下提示词进行基于 diff-based edit 的操作：

1. **编辑上下文（Modify Context）**  
   Gemini 提供的提示词通常包含当前文件内容和拟议修改内容。这包括：
   - `currentContent`：文件的原始内容。
   - `proposedContent`：建议修改后的内容。

2. **创建差异（Create Diff）**  
   Gemini 会通过比较 `currentContent` 和 `proposedContent` 来生成 diff，提示词通常会要求它识别两个版本之间的具体差异。

3. **修改参数（Modify Parameters）**  
   Gemini 会根据修改后的内容重新构造编辑参数（`params`），例如：
   - `old_string`：要替换的原始字符串。
   - `new_string`：替换后的新字符串。

4. **工具接口提示（Tool Interface Prompts）**  
   在 `packages/core/src/tools/edit.test.ts` 中，`ensureCorrectEdit` 用于测试 diff 生成。Gemini 会根据以下逻辑进行处理：
   ```ts
   const correctedOldString = 'original string to be replaced'; // 更具体的旧字符串
   const correctedNewString = 'completely new string'; // 不同的替换字符串
   const expectedFinalContent = 'This is the completely new string.';
   ```
   这表明 Gemini 会根据提供的上下文自动调整 `old_string` 和 `new_string`，以确保生成的 diff 正确无误。

5. **缓存与性能优化**  
   Gemini 会利用缓存（如 `editCorrectionCache` 和 `fileContentCorrectionCache`）来避免重复计算，提高性能。这表明它会基于历史记录进行智能决策。

6. **编辑器集成（Editor Integration）**  
   在 `packages/core/src/utils/editor.ts` 中，Gemini 会通过调用编辑器（如 `vscode`）来让用户审查 diff 并确认修改。这表明 Gemini 的提示词会包含有关如何使用编辑器进行交互的说明。

综上，Gemini 会基于当前文件内容、修改建议、缓存记录和用户交互来生成和应用 diff-based edit。
2025-07-10 03:26:41,059 - INFO - api.websocket_wiki - websocket_wiki.py:69 - Request size: 18 tokens
2025-07-10 03:26:41,095 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:26:41,096 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['schema']
2025-07-10 03:26:41,096 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:26:41,096 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:26:41,127 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:26:41,141 - INFO - adalflow.core.prompt_builder - prompt_builder.py:74 - Prompt has variables: ['input_str', 'conversation_history', 'contexts', 'output_format_str', 'system_prompt']
2025-07-10 03:26:41,142 - INFO - adalflow.core.generator - generator.py:180 - Generator Generator initialized.
2025-07-10 03:26:41,142 - INFO - api.data_pipeline - data_pipeline.py:879 - Preparing repo storage for https://github.com/google-gemini/gemini-cli...
2025-07-10 03:26:41,143 - INFO - api.data_pipeline - data_pipeline.py:893 - Extracted repo name: google-gemini_gemini-cli
2025-07-10 03:26:41,143 - INFO - api.data_pipeline - data_pipeline.py:904 - Repository already exists at /root/.adalflow/repos/google-gemini_gemini-cli. Using existing repository.
2025-07-10 03:26:41,143 - INFO - api.data_pipeline - data_pipeline.py:920 - Repo paths: {'save_repo_dir': '/root/.adalflow/repos/google-gemini_gemini-cli', 'save_db_file': '/root/.adalflow/databases/google-gemini_gemini-cli.pkl'}
2025-07-10 03:26:41,143 - INFO - api.data_pipeline - data_pipeline.py:950 - Loading existing database...
2025-07-10 03:26:41,181 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict TextSplitter, {'type': 'TextSplitter', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'TextSplitter', '_init_args': {'split_by': 'word', 'chunk_size': 800, 'chunk_overlap': 200, 'batch_size': 1000, 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}}, 'split_by': 'word', 'separators': {'page': '\x0c', 'passage': '\n\n', 'word': ' ', 'sentence': '.', 'token': ''}, 'chunk_size': 350, 'chunk_overlap': 100, 'batch_size': 1000}}
2025-07-10 03:26:41,182 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict ToEmbeddings, {'type': 'ToEmbeddings', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}), ('batch_embedder', {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'ToEmbeddings', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:26:41,182 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:26:41,182 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:26:41,241 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict BatchEmbedder, {'type': 'BatchEmbedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('embedder', {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'BatchEmbedder', '_init_args': {'embedder': None, 'batch_size': 10}, 'batch_size': 10}}
2025-07-10 03:26:41,241 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict Embedder, {'type': 'Embedder', 'data': {'_components': {'_ordered_dict': True, 'data': [('model_client', {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}})]}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'Embedder', '_init_args': {'model_client': None, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}, 'model_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float'}, 'output_processors': None}}
2025-07-10 03:26:41,241 - INFO - adalflow.core.component - component.py:335 - Restoring class using from_dict OpenAIClient, {'type': 'OpenAIClient', 'data': {'_components': {'_ordered_dict': True, 'data': []}, '_parameters': {'_ordered_dict': True, 'data': []}, 'training': False, 'teacher_mode': False, 'tracing': False, 'name': 'OpenAIClient', '_init_args': {'api_key': None, 'chat_completion_parser': None, 'input_type': 'text', 'base_url': None, 'env_base_url_name': 'OPENAI_BASE_URL', 'env_api_key_name': 'OPENAI_API_KEY'}, '_api_key': 'sk-db2ad92210a348fd884c3b94655095c5', '_env_api_key_name': 'OPENAI_API_KEY', '_env_base_url_name': 'OPENAI_BASE_URL', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/', 'chat_completion_parser': <function get_first_message_content at 0x7fb954b320c0>, '_input_type': 'text', '_api_kwargs': {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['"yargs": "^17.7.2"\n  },\n  "devDependencies": {\n    "@babel/runtime": "^7.27.6",\n    "@testing-library/react": "^16.3.0",\n    "@types/command-exists": "^1.2.3",\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/node": "^20.11.24",\n    "@types/react": "^19.1.8",\n    "@types/react-dom": "^19.1.6",\n    "@types/semver": "^7.7.0",\n    "@types/shell-quote": "^1.7.5",\n    "@types/yargs": "^17.0.32",\n    "ink-testing-library": "^4.0.0",\n    "jsdom": "^26.1.0",\n    "pretty-format": "^30.0.2",\n    "react-dom": "^19.1.0",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n', '{\n  "name": "@google/gemini-cli-core",\n  "version": "0.1.9",\n  "description": "Gemini CLI Core",\n  "repository": {\n    "type": "git",\n    "url": "git+https://github.com/google-gemini/gemini-cli.git"\n  },\n  "type": "module",\n  "main": "dist/index.js",\n  "scripts": {\n    "build": "node ../../scripts/build_package.js",\n    "lint": "eslint . --ext .ts,.tsx",\n    "format": "prettier --write .",\n    "test": "vitest run",\n    "test:ci": "vitest run --coverage",\n    "typecheck": "tsc --noEmit"\n  },\n  "files": [\n    "dist"\n  ],\n  "dependencies": {\n    "@google/genai": "^1.8.0",\n    "@modelcontextprotocol/sdk": "^1.11.0",\n    "@opentelemetry/api": "^1.9.0",\n    "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",\n    "@opentelemetry/exporter-trace-otlp-grpc": "^0.52.0",\n    "@opentelemetry/instrumentation-http": "^0.52.0",\n    "@opentelemetry/sdk-node": "^0.52.0",\n    "@types/glob": "^8.1.0",\n    "@types/html-to-text": "^9.0.4",\n    "ajv": "^8.17.1",\n    "diff": "^7.0.0",\n    "dotenv": "^16.6.1",\n    "gaxios": "^6.1.1",\n    "glob": "^10.4.5",\n    "google-auth-library": "^9.11.0",\n    "html-to-text": "^9.0.5",\n    "ignore": "^7.0.0",\n    "micromatch": "^4.0.8",\n    "open": "^10.1.2",\n    "shell-quote": "^1.8.3",\n    "simple-git": "^3.28.0",\n    "strip-ansi": "^7.1.0",\n    "undici": "^7.10.0",\n    "ws": "^8.18.0"\n  },\n  "devDependencies": {\n    "@types/diff": "^7.0.2",\n    "@types/dotenv": "^6.1.1",\n    "@types/micromatch": "^4.0.8",\n    "@types/minimatch": "^5.1.2",\n    "@types/ws": "^8.5.10",\n    "typescript": "^5.3.3",\n    "vitest": "^3.1.1"\n  },\n  "engines": {\n    "node": ">=20"\n  }\n}\n']}}}
2025-07-10 03:26:41,300 - INFO - api.data_pipeline - data_pipeline.py:955 - Loaded 1962 documents from existing database
2025-07-10 03:26:41,300 - INFO - api.rag - rag.py:421 - Loaded 1962 documents for retrieval
2025-07-10 03:26:41,301 - INFO - api.rag - rag.py:350 - Target embedding size: 256 (found in 1962 documents)
2025-07-10 03:26:41,301 - INFO - api.rag - rag.py:384 - Embedding validation complete: 1962/1962 documents have valid embeddings
2025-07-10 03:26:41,301 - INFO - api.rag - rag.py:429 - Using 1962 documents with valid embeddings for retrieval
2025-07-10 03:26:41,301 - INFO - adalflow.optim.grad_component - grad_component.py:79 - EvalFnToTextLoss: No backward engine provided. Creating one using model_client and model_kwargs.
2025-07-10 03:26:41,321 - INFO - adalflow.components.retriever.faiss_retriever - faiss_retriever.py:190 - Index built with 1962 chunks
2025-07-10 03:26:41,322 - INFO - api.rag - rag.py:440 - FAISS retriever created successfully
2025-07-10 03:26:41,322 - INFO - api.websocket_wiki - websocket_wiki.py:103 - Retriever prepared for https://github.com/google-gemini/gemini-cli
2025-07-10 03:26:41,322 - INFO - api.openai_client - openai_client.py:480 - api_kwargs: {'model': 'text-embedding-v4', 'dimensions': 256, 'encoding_format': 'float', 'input': ['在让gemini进行代码编辑的业务逻辑在哪里']}
2025-07-10 03:26:41,539 - INFO - httpx - _client.py:1025 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings "HTTP/1.1 200 OK"
2025-07-10 03:26:41,542 - INFO - api.websocket_wiki - websocket_wiki.py:216 - Retrieved 20 documents
2025-07-10 03:26:41,542 - INFO - api.rag - rag.py:73 - Dialog turns list exists but is empty
2025-07-10 03:26:41,542 - INFO - api.rag - rag.py:87 - Returning 0 dialog turns from memory
2025-07-10 03:26:41,542 - INFO - api.websocket_wiki - websocket_wiki.py:449 - 449- Prompt:
/no_think <role>
You are an expert code analyst examining the github repository: https://github.com/google-gemini/gemini-cli (gemini-cli).
You provide direct, concise, and accurate information about code repositories.
You NEVER start responses with markdown headers or code fences.
IMPORTANT:You MUST respond in Mandarin Chinese (中文) language.
</role>

<guidelines>
- Answer the user's question directly without ANY preamble or filler phrases
- DO NOT include any rationale, explanation, or extra comments.
- Strictly base answers ONLY on existing code or documents
- DO NOT speculate or invent citations.
- DO NOT start with preambles like "Okay, here's a breakdown" or "Here's an explanation"
- DO NOT start with markdown headers like "## Analysis of..." or any file path references
- DO NOT start with ```markdown code fences
- DO NOT end your response with ``` closing fences
- DO NOT start by repeating or acknowledging the question
- JUST START with the direct answer to the question

<example_of_what_not_to_do>
```markdown
## Analysis of `adalflow/adalflow/datasets/gsm8k.py`

This file contains...
```
</example_of_what_not_to_do>

- Format your response with proper markdown including headings, lists, and code blocks WITHIN your answer
- For code analysis, organize your response with clear sections
- Think step by step and structure your answer logically
- Start with the most relevant information that directly addresses the user's query
- Be precise and technical when discussing code
- Your response language should be in the same language as the user's query
</guidelines>

<style>
- Use concise, direct language
- Prioritize accuracy over verbosity
- When showing code, include line numbers and file paths when relevant
- Use markdown formatting to improve readability
</style>

<START_OF_CONTEXT>


----------## File Path: packages/cli/src/ui/privacy/CloudFreePrivacyNotice.tsx

Gemini Code Assist for Individuals Privacy Notice
      </Text>
      <Newline />
      <Text>
        This notice and our Privacy Policy
        <Text color={Colors.AccentBlue}>[1]</Text> describe how Gemini Code
        Assist handles your data. Please read them carefully.
      </Text>
      <Newline />
      <Text>
        When you use Gemini Code Assist for individuals with Gemini CLI, Google
        collects your prompts, related code, generated output, code edits,
        related feature usage information, and your feedback to provide,
        improve, and develop Google products and services and machine learning
        technologies.
      </Text>
      <Newline />
      <Text>
        To help with quality and improve our products (such as generative
        machine-learning models), human reviewers may read, annotate, and
        process the data collected above. We take steps to protect your privacy
        as part of this process. This includes disconnecting the data from your
        Google Account before reviewers see or annotate it, and storing those
        disconnected copies for up to 18 months. Please don&apos;t submit
        confidential information or any data you wouldn&apos;t want a reviewer
        to see or Google to use to improve our products, services and
        machine-learning technologies.
      </Text>
      <Newline />
      

## File Path: packages/core/src/utils/editCorrector.test.ts

    originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find\\me');
        expect(result.occurrences).toBe(1);
      });
    });

    describe('Scenario Group 2: originalParams.old_string does NOT match, but unescapeStringForGeminiBug(originalParams.old_string) DOES match', () => {
      it('Test 2.1: old_string (over-escaped, no intended literal \\), new_string (escaped by Gemini) -> new_string unescaped', async () => {
        const currentContent = 'This is a test string to find "me".';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find \\"me\\"',
          new_string: 'replace with \\"this\\"',
        };
        mockResponses.push({ corrected_new_string: 'replace with "this"' });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find "me"');
        expect(result.occurrences).toBe(1);
      });
      it('Test 2.2: old_string (over-escaped, no intended literal \\), new_string (correctly formatted) -> new_string unescaped (harmlessly)', async () => {
        const currentContent = 'This 

  old_string: 'find me',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find me');
        expect(result.occurrences).toBe(1);
      });
      it('Test 1.3: old_string (with literal \\), new_string (escaped by Gemini) -> new_string unchanged (still escaped)', async () => {
        const currentContent = 'This is a test string to find\\me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find\\me',
          new_string: 'replace with \\"this\\"',
        };
        mockResponses.push({
          corrected_new_string_escaping: 'replace with "this"',
        });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
  

= {
          file_path: '/test/file.txt',
          old_string: 'find "me"',
          new_string: 'some new string',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params).toEqual(originalParams);
        expect(result.occurrences).toBe(2);
      });
    });

    describe('Scenario Group 5: Specific unescapeStringForGeminiBug checks (integrated into ensureCorrectEdit)', () => {
      it('Test 5.1: old_string needs LLM to become currentContent, new_string also needs correction', async () => {
        const currentContent = 'const x = "a\nbc\\"def\\"';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'const x = \\"a\\nbc\\\\"def\\\\"',
          new_string: 'const y = \\"new\\nval\\\\"content\\\\"',
        };
        const expectedFinalNewString = 'const y = "new\nval\\"content\\"';
        mockResponses.push({ corrected_target_snippet: currentContent });
        mockResponses.push({ corrected_new_string: expectedFinalNewString });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
     

     mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find "me"');
        expect(result.occurrences).toBe(1);
      });
      it('Test 2.2: old_string (over-escaped, no intended literal \\), new_string (correctly formatted) -> new_string unescaped (harmlessly)', async () => {
        const currentContent = 'This is a test string to find "me".';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find \\"me\\"',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find "me"');
        expect(result.occurrences).toBe(1);
      });
      it('Test 2.3: old_string (over-escaped, with intended literal \\), new_string (simple) -> new_string corrected', async () => {
        const currentContent = 'This is a test string to find \\me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find \\\\me',
    

mixed slashes and characters', () => {
      expect(
        unescapeStringForGeminiBug('\\\\\\\nLine1\\\nLine2\\tTab\\\\`Tick\\"'),
      ).toBe('\nLine1\nLine2\tTab`Tick"');
    });
    it('should handle escaped backslashes', () => {
      expect(unescapeStringForGeminiBug('\\\\')).toBe('\\');
      expect(unescapeStringForGeminiBug('C:\\\\Users')).toBe('C:\\Users');
      expect(unescapeStringForGeminiBug('path\\\\to\\\\file')).toBe(
        'path\to\\file',
      );
    });
    it('should handle escaped backslashes mixed with other escapes (aggressive unescaping)', () => {
      expect(unescapeStringForGeminiBug('line1\\\\\\nline2')).toBe(
        'line1\nline2',
      );
      expect(unescapeStringForGeminiBug('quote\\\\"text\\\\nline')).toBe(
        'quote"text\nline',
      );
    });
  });

  describe('ensureCorrectEdit', () => {
    let mockGeminiClientInstance: Mocked<GeminiClient>;
    let mockToolRegistry: Mocked<ToolRegistry>;
    let mockConfigInstance: Config;
    const abortSignal = new AbortController().signal;

    beforeEach(() => {
      mockToolRegistry = new ToolRegistry({} as Config) as Mocked<ToolRegistry>;
      const configParams = {
        apiKey: 'test-api-key',
        model: 'test-model',
        sandbox: false as boolean | string,
        targetDir: '/test',
        debugMode: false,
        question: undefined as string | undefined,
        fullContext: false,
        coreTools: undefined as string[] | undefined,
        toolDiscoveryCommand: undefined as string | undefined,
        toolCallCommand: undefined as string | undefined,
        mcpServerCommand: undefined as string | undefined,
        mcpServers: undefined as Record<string, any> | undefined,
        userAgent: 'test-agent',
       

'replace with "this"',
        });
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(1);
        expect(result.params.new_string).toBe('replace with "this"');
        expect(result.params.old_string).toBe('find\\me');
        expect(result.occurrences).toBe(1);
      });
      it('Test 1.4: old_string (with literal \\), new_string (correctly formatted) -> new_string unchanged', async () => {
        const currentContent = 'This is a test string to find\\me.';
        const originalParams = {
          file_path: '/test/file.txt',
          old_string: 'find\\me',
          new_string: 'replace with this',
        };
        const result = await ensureCorrectEdit(
          '/test/file.txt',
          currentContent,
          originalParams,
          mockGeminiClientInstance,
          abortSignal,
        );
        expect(mockGenerateJson).toHaveBeenCalledTimes(0);
        expect(result.params.new_string).toBe('replace with this');
        expect(result.params.old_string).toBe('find\\me');
        expect(result.occurrences).toBe(1);
      });
    });

    describe('Scenario Group 2: originalParams.old_string does NOT match, but unescapeStringForGeminiBug(originalParams.old_string) DOES match', () => {
      it('Test 

## File Path: packages/cli/src/ui/hooks/usePrivacySettings.ts

       error: e instanceof Error ? e.message : String(e),
        });
      }
    },
    [config],
  );

  return {
    privacyState,
    updateDataCollectionOptIn,
  };
};

function getCodeAssistServer(config: Config): CodeAssistServer {
  const server = config.getGeminiClient().getContentGenerator();
  // Neither of these cases should ever happen.
  if (!(server instanceof CodeAssistServer)) {
    throw new Error('Oauth not being used');
  } else if (!server.projectId) {
    throw new Error('Oauth not being used');
  }
  return server;
}

async function getTier(server: CodeAssistServer): Promise<UserTierId> {
  const loadRes = await server.loadCodeAssist({
    cloudaicompanionProject: server.projectId,
    metadata: {
      ideType: 'IDE_UNSPECIFIED',
      platform: 'PLATFORM_UNSPECIFIED',
      pluginType: 'GEMINI',
      duetProject: server.projectId,
    },
  });
  if (!loadRes.currentTier) {
    throw new Error('User does not have a current tier');
  }
  return loadRes.currentTier.id;
}

async function getRemoteDataCollectionOptIn(
  server: CodeAssistServer,
): Promise<boolean> {
  try {
    const resp = await server.getCodeAssistGlobalUserSetting();
    return resp.freeTierDataCollectionOptin;
  } catch (e) {
    if (e instanceof GaxiosError) {
      if (e.response?.status === 404) {
        return true;
      }
    }
    throw e;
  }
}

async function setRemoteDataCollectionOptIn(
  server: CodeAssistServer,
  optIn: boolean,
): Promise<boolean> {
  const resp = await server.setCodeAssistGlobalUserSetting({
    cloudaicompanionProject: server.projectId,
    freeTierDataCollectionOptin: optIn,
  });
  return resp.freeTierDataCollectionOptin;
}


   const optIn = await getRemoteDataCollectionOptIn(server);
        setPrivacyState({
          isLoading: false,
          isFreeTier: true,
          dataCollectionOptIn: optIn,
        });
      } catch (e) {
        setPrivacyState({
          isLoading: false,
          error: e instanceof Error ? e.message : String(e),
        });
      }
    };
    fetchInitialState();
  }, [config]);

  const updateDataCollectionOptIn = useCallback(
    async (optIn: boolean) => {
      try {
        const server = getCodeAssistServer(config);
        const updatedOptIn = await setRemoteDataCollectionOptIn(server, optIn);
        setPrivacyState({
          isLoading: false,
          isFreeTier: true,
          dataCollectionOptIn: updatedOptIn,
        });
      } catch (e) {
        setPrivacyState({
          isLoading: false,
          error: e instanceof Error ? e.message : String(e),
        });
      }
    },
    [config],
  );

  return {
    privacyState,
    updateDataCollectionOptIn,
  };
};

function getCodeAssistServer(config: Config): CodeAssistServer {
  const server = config.getGeminiClient().getContentGenerator();
  // Neither of these cases should ever happen.
  if (!(server instanceof CodeAssistServer)) {
    throw new Error('Oauth not being used');
  } else if (!server.projectId) {
    throw new Error('Oauth not being used');
 

## File Path: packages/core/src/code_assist/setup.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  ClientMetadata,
  GeminiUserTier,
  LoadCodeAssistResponse,
  OnboardUserRequest,
  UserTierId,
} from './types.js';
import { CodeAssistServer } from './server.js';
import { OAuth2Client } from 'google-auth-library';

export class ProjectIdRequiredError extends Error {
  constructor() {
    super(
      'This account requires setting the GOOGLE_CLOUD_PROJECT env var. See https://goo.gle/gemini-cli-auth-docs#workspace-gca',
    );
  }
}

/**
 *
 * @param projectId the user's project id, if any
 * @returns the user's actual project id
 */
export async function setupUser(client: OAuth2Client): Promise<string> {
  let projectId = process.env.GOOGLE_CLOUD_PROJECT || undefined;
  const caServer = new CodeAssistServer(client, projectId);

  const clientMetadata: ClientMetadata = {
    ideType: 'IDE_UNSPECIFIED',
    platform: 'PLATFORM_UNSPECIFIED',
    pluginType: 'GEMINI',
    duetProject: projectId,
  };

  const loadRes = await caServer.loadCodeAssist({
    cloudaicompanionProject: projectId,
    metadata: clientMetadata,
  });

  if (!projectId && loadRes.cloudaicompanionProject) {
    projectId = loadRes.cloudaicompanionProject;
  }

  const tier = getOnboardTier(loadRes);
  if (tier.userDefinedCloudaicompanionProject && !projectId) {
    throw new ProjectIdRequiredError();
  }

  const onboardReq: OnboardUserRequest = {
    tierId: tier.id,
    cloudaicompanionProject: projectId,
    metadata: clientMetadata,
  };

  // Poll onboardUser until long running operation is complete.
  let lroRes = await caServer.onboardUser(onboardReq);
  while (!lroRes.done) {
    await new Promise((f) => setTimeout(f, 5000));
    lroRes = await caServer.onboardUser(onboardReq);
  }
  return lroRes.response?.cloudaicompanionProject?.id || '';
}

function getOnboardTier(res: LoadCodeAssistResponse): GeminiUserTier {
  if (res.currentTier) {
    return res.currentTier;
  }
  for (const tier of res.allowedTiers || []) {
    if (tier.isDefault) {
      return tier;
    }
  }
  return {
    name: '',
    description: '',
    id: UserTierId.LEGACY,
    userDefinedCloudaicompanionProject: true,
  };
}


## File Path: packages/core/src/core/turn.ts

GeminiEventType.ToolCallRequest;
  value: ToolCallRequestInfo;
};

export type ServerGeminiToolCallResponseEvent = {
  type: GeminiEventType.ToolCallResponse;
  value: ToolCallResponseInfo;
};

export type ServerGeminiToolCallConfirmationEvent = {
  type: GeminiEventType.ToolCallConfirmation;
  value: ServerToolCallConfirmationDetails;
};

export type ServerGeminiUserCancelledEvent = {
  type: GeminiEventType.UserCancelled;
};

export type ServerGeminiErrorEvent = {
  type: GeminiEventType.Error;
  value: GeminiErrorEventValue;
};

export interface ChatCompressionInfo {
  originalTokenCount: number;
  newTokenCount: number;
}

export type ServerGeminiChatCompressedEvent = {
  type: GeminiEventType.ChatCompressed;
  value: ChatCompressionInfo | null;
};

// The original union type, now composed of the individual types
export type ServerGeminiStreamEvent =
  | ServerGeminiContentEvent
  | ServerGeminiToolCallRequestEvent
  | ServerGeminiToolCallResponseEvent
  | ServerGeminiToolCallConfirmationEvent
  | ServerGeminiUserCancelledEvent
  | ServerGeminiErrorEvent
  | ServerGeminiChatCompressedEvent
  | ServerGeminiThoughtEvent;

// A turn manages the agentic loop turn within the server context.
export class Turn {
  readonly pendingToolCalls: ToolCallRequestInfo[];
  private debugResponses: GenerateContentResponse[];

  constructor(private readonly chat: GeminiChat) {
    this.pendingToolCalls = [];
    this.debugResponses = [];
  }
  // The run method yields simpler events suitable for server logic
  async *run(
    req: PartListUnion,
    signal: AbortSignal,
  ): AsyncGenerator<ServerGeminiStreamEvent> {
    try {
      const responseStream = await this.chat.sendMessageStream({
        message: req,
        config: {
          abortSignal: signal,
        },
      });

      for await (const resp of responseStream) {
        if (signal?.aborted) {
          yield { type: GeminiEventType.UserCancelled };
          // Do not add resp to debugResponses if aborted before processing
          return;
        }
        this.debugResponses.push(resp);

        const thoughtPart = resp.candidates?.[0]?.content?.parts?.[0];
        if (thoughtPart?.thought) {
          // Thought always has a bold "subject" part enclosed in 

## File Path: packages/core/src/core/geminiRequest.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { type PartListUnion, type Part } from '@google/genai';

/**
 * Represents a request to be sent to the Gemini API.
 * For now, it's an alias to PartListUnion as the primary content.
 * This can be expanded later to include other request parameters.
 */
export type GeminiCodeRequest = PartListUnion;

export function partListUnionToString(value: PartListUnion): string {
  if (typeof value === 'string') {
    return value;
  }

  if (Array.isArray(value)) {
    return value.map(partListUnionToString).join('');
  }

  // Cast to Part, assuming it might contain project-specific fields
  const part = value as Part & {
    videoMetadata?: unknown;
    thought?: string;
    codeExecutionResult?: unknown;
    executableCode?: unknown;
  };

  if (part.videoMetadata !== undefined) {
    return `[Video Metadata]`;
  }

  if (part.thought !== undefined) {
    return `[Thought: ${part.thought}]`;
  }

  if (part.codeExecutionResult !== undefined) {
    return `[Code Execution Result]`;
  }

  if (part.executableCode !== undefined) {
    return `[Executable Code]`;
  }

  // Standard Part fields
  if (part.fileData !== undefined) {
    return `[File Data]`;
  }

  if (part.functionCall !== undefined) {
    return `[Function Call: ${part.functionCall.name}]`;
  }

  if (part.functionResponse !== undefined) {
    return `[Function Response: ${part.functionResponse.name}]`;
  }

  if (part.inlineData !== undefined) {
    return `<${part.inlineData.mimeType}>`;
  }

  if (part.text !== undefined) {
    return part.text;
  }

  return '';
}


## File Path: packages/core/src/code_assist/converter.test.ts

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import { describe, it, expect } from 'vitest';
import {
  toGenerateContentRequest,
  fromGenerateContentResponse,
  CaGenerateContentResponse,
} from './converter.js';
import {
  GenerateContentParameters,
  GenerateContentResponse,
  FinishReason,
  BlockedReason,
} from '@google/genai';

describe('converter', () => {
  describe('toCodeAssistRequest', () => {
    it('should convert a simple request with project', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
      };
      const codeAssistReq = toGenerateContentRequest(genaiReq, 'my-project');
      expect(codeAssistReq).toEqual({
        model: 'gemini-pro',
        project: 'my-project',
        request: {
          contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
          systemInstruction: undefined,
          cachedContent: undefined,
          tools: undefined,
          toolConfig: undefined,
          labels: undefined,
          safetySettings: undefined,
          generationConfig: undefined,
          session_id: undefined,
        },
      });
    });

    it('should convert a request without a project', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
      };
      const codeAssistReq = toGenerateContentRequest(genaiReq);
      expect(codeAssistReq).toEqual({
        model: 'gemini-pro',
  

() => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: 'Hello',
      };
      const codeAssistReq = toGenerateContentRequest(genaiReq);
      expect(codeAssistReq.request.contents).toEqual([
        { role: 'user', parts: [{ text: 'Hello' }] },
      ]);
    });

    it('should handle Part[] content', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: [{ text: 'Hello' }, { text: 'World' }],
      };
      const codeAssistReq = toGenerateContentRequest(genaiReq);
      expect(codeAssistReq.request.contents).toEqual([
        { role: 'user', parts: [{ text: 'Hello' }] },
        { role: 'user', parts: [{ text: 'World' }] },
      ]);
    });

    it('should handle system instructions', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: 'Hello',
        config: {
          systemInstruction: 'You are a helpful assistant.',
        },
      };
      const codeAssistReq = toGenerateContentRequest(genaiReq);
      expect(codeAssistReq.request.systemInstruction).toEqual({
        role: 'user',
        parts: [{ text: 'You are a helpful assistant.' }],
      });
    });

    it('should handle generation config', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: 'Hello',
 

session_id: undefined,
        },
      });
    });

    it('should convert a request without a project', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
      };
      const codeAssistReq = toGenerateContentRequest(genaiReq);
      expect(codeAssistReq).toEqual({
        model: 'gemini-pro',
        project: undefined,
        request: {
          contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
          systemInstruction: undefined,
          cachedContent: undefined,
          tools: undefined,
          toolConfig: undefined,
          labels: undefined,
          safetySettings: undefined,
          generationConfig: undefined,
          session_id: undefined,
        },
      });
    });

    it('should convert a request with sessionId', () => {
      const genaiReq: GenerateContentParameters = {
        model: 'gemini-pro',
        contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
      };
      const codeAssistReq = toGenerateContentRequest(
        genaiReq,
        'my-project',
        'session-123',
      );
      expect(codeAssistReq).toEqual({
        model: 'gemini-pro',
 

## File Path: CONTRIBUTING.md

"
# Run npm build and check for errors
if ! npm run preflight; then
  echo "npm build failed. Commit aborted."
  exit 1
fi
" > .git/hooks/pre-commit && chmod +x .git/hooks/pre-commit
```

#### Formatting

To separately format the code in this project by running the following command from the root directory:

```bash
npm run format
```

This command uses Prettier to format the code according to the project's style guidelines.

#### Linting

To separately lint the code in this project, run the following command from the root directory:

```bash
npm run lint
```

### Coding Conventions

- Please adhere to the coding style, patterns, and conventions used throughout the existing codebase.
- Consult [GEMINI.md](https://github.com/google-gemini/gemini-cli/blob/main/GEMINI.md) (typically found in the project root) for specific instructions related to AI-assisted development, including conventions for React, comments, and Git usage.
- **Imports:** Pay special attention to import paths. The project uses `eslint-rules/no-relative-cross-package-imports.js` to enforce restrictions on relative imports between packages.

### Project Structure

- `packages/`: Contains the individual sub-packages of the project.
  - `cli/`: The command-line interface.
  - `core/`: The core backend logic for the Gemini CLI.
- `docs/`: Contains all project documentation.
- `scripts/`: Utility scripts for building, testing, and development tasks.

For more detailed architecture, see `docs/architecture.md`.

## Debugging

### VS Code:

0.  Run the CLI to interactively debug in VS Code with `F5`
1.  Start the CLI in debug mode from the root directory:
    ```bash
    npm run debug
    ```
    This command runs `node --inspect-brk dist/gemini.js` within the `packages/cli` directory, pausing execution until a debugger attaches. You can then open `chrome://inspect` in your Chrome browser to connect to the debugger.
2.  In VS Code, use the "Attach" launch configuration (found in `.vscode/launch.json`).

Alternatively, you can use the "Launch Program" configuration in VS Code if you prefer to launch the currently open file directly, but 'F5' is generally recommended.

To hit a breakpoint inside the sandbox container run:

```bash
DEBUG=1 gemini
```

### React DevTools

To debug the CLI's React-based UI, you can use React DevTools. Ink, the library used for the CLI's interface, is compatible with React DevTools version 4.x.

1.  **Start the Gemini CLI in development mode:**

    ```bash
    DEV=true npm start
    

## File Path: packages/cli/index.ts

#!/usr/bin/env node

/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import './src/gemini.js';
import { main } from './src/gemini.js';

// --- Global Entry Point ---
main().catch((error) => {
  console.error('An unexpected critical error occurred:');
  if (error instanceof Error) {
    console.error(error.stack);
  } else {
    console.error(String(error));
  }
  process.exit(1);
});


## File Path: packages/core/src/utils/editCorrector.ts

     abortSignal,
      EditModel,
      EditConfig,
    );

    if (
      result &&
      typeof result.corrected_new_string === 'string' &&
      result.corrected_new_string.length > 0
    ) {
      return result.corrected_new_string;
    } else {
      return originalNewString;
    }
  } catch (error) {
    if (abortSignal.aborted) {
      throw error;
    }

    console.error('Error during LLM call for new_string correction:', error);
    return originalNewString;
  }
}

const CORRECT_NEW_STRING_ESCAPING_SCHEMA: SchemaUnion = {
  type: Type.OBJECT,
  properties: {
    corrected_new_string_escaping: {
      type: Type.STRING,
      description:
        'The new_string with corrected escaping, ensuring it is a proper replacement for the old_string, especially considering potential over-escaping issues from previous LLM generations.',
    },
  },
  required: ['corrected_new_string_escaping'],
};

export async function correctNewStringEscaping(
  geminiClient: GeminiClient,
  oldString: string,
  potentiallyProblematicNewString: string,
  abortSignal: AbortSignal,
): Promise<string> {
  const prompt = `
Context: A text replacement operation is planned. The text to be replaced (old_string) has been correctly identified in the file. However, the replacement text (new_string) might have been improperly escaped by a previous LLM generation (e.g. too many backslashes for newlines like \\n instead of \n, or unnecessarily quotes like \\"Hello\\" instead of "Hello").

old_string (this is the exact text that will be replaced):
\`\`\`
${oldString}
\`\`\`

potentially_problematic_new_string (this is the text that should replace old_string, but MIGHT have bad escaping, or might be entirely correct):
\`\`\`
${potentiallyProblematicNewString}
\`\`\`

Task: Analyze the potentially_problematic_new_string. If it's syntactically invalid due to incorrect escaping (e.g., "\n", "\t", "\\", "\\'", "\\""), correct the invalid syntax. The goal is to ensure the new_string, when inserted into the code, will be a valid and correctly interpreted.

For example, if old_string is "foo" and potentially_problematic_new_string is "bar\\nbaz", the corrected_new_string_escaping should be "bar\nbaz".
If potentially_problematic_new_string is console.log(\\"Hello World\\"), it should 

## File Path: packages/core/src/telemetry/clearcut-logger/event-metadata-key.ts

==========================================================================
  // Tool Call Event Keys
  // ===========================================================================

  // Logs the function name.
  GEMINI_CLI_TOOL_CALL_NAME = 14,

  // Logs the user's decision about how to handle the tool call.
  GEMINI_CLI_TOOL_CALL_DECISION = 15,

  // Logs whether the tool call succeeded.
  GEMINI_CLI_TOOL_CALL_SUCCESS = 16,

  // Logs the tool call duration in milliseconds.
  GEMINI_CLI_TOOL_CALL_DURATION_MS = 17,

  // Logs the tool call error message, if any.
  GEMINI_CLI_TOOL_ERROR_MESSAGE = 18,

  // Logs the tool call error type, if any.
  GEMINI_CLI_TOOL_CALL_ERROR_TYPE = 19,

  // ==========================================================================
  // GenAI API Request Event Keys
  // ===========================================================================

  // Logs the model id of the request.
  GEMINI_CLI_API_REQUEST_MODEL = 20,

  // ==========================================================================
  // GenAI API Response Event Keys
  // ===========================================================================

  // Logs the model id of the API call.
  GEMINI_CLI_API_RESPONSE_MODEL = 21,

  // Logs the status code of the response.
  GEMINI_CLI_API_RESPONSE_STATUS_CODE = 22,

  // Logs the duration of the API call in milliseconds.
  GEMINI_CLI_API_RESPONSE_DURATION_MS = 23,

  // Logs the error message of the API call, if any.
  GEMINI_CLI_API_ERROR_MESSAGE = 24,

  // Logs the input token count of the API call.
  GEMINI_CLI_API_RESPONSE_INPUT_TOKEN_COUNT = 25,

  // Logs the output token count of the API call.
  GEMINI_CLI_API_RESPONSE_OUTPUT_TOKEN_COUNT = 26,

  // Logs the cached token count of the API call.
  GEMINI_CLI_API_RESPONSE_CACHED_TOKEN_COUNT = 27,

  // Logs the thinking token count of the API call.
  GEMINI_CLI_API_RESPONSE_THINKING_TOKEN_COUNT = 28,

  // Logs the tool use token count of the API call.
  GEMINI_CLI_API_RESPONSE_TOOL_TOKEN_COUNT = 29,

  // ==========================================================================
  // GenAI API Error Event Keys
  // ===========================================================================

  // Logs the model id of the API call.
  GEMINI_CLI_API_ERROR_MODEL = 30,

  // Logs the error type.
  GEMINI_CLI_API_ERROR_TYPE = 31,

  // Logs the status code of the error response.
  GEMINI_CLI_API_ERROR_STATUS_CODE = 32,

  // Logs the duration of the API call in milliseconds.
  GEMINI_CLI_API_ERROR_DURATION_MS = 33,

  // ==========================================================================
  // End Session Event Keys
  // ===========================================================================

  // Logs the end of a session.
  

## File Path: packages/cli/src/ui/hooks/useGeminiStream.test.tsx

= new MockedGeminiClientClass(mockConfig);
      return clientInstance;
    });

    mockConfig = {
      apiKey: 'test-api-key',
      model: 'gemini-pro',
      sandbox: false,
      targetDir: '/test/dir',
      debugMode: false,
      question: undefined,
      fullContext: false,
      coreTools: [],
      toolDiscoveryCommand: undefined,
      toolCallCommand: undefined,
      mcpServerCommand: undefined,
      mcpServers: undefined,
      userAgent: 'test-agent',
      userMemory: '',
      geminiMdFileCount: 0,
      alwaysSkipModificationConfirmation: false,
      vertexai: false,
      showMemoryUsage: false,
      contextFileName: undefined,
      getToolRegistry: vi.fn(
        () => ({ getToolSchemaList: vi.fn(() => []) }) as any,
      ),
      getProjectRoot: vi.fn(() => '/test/dir'),
      getCheckpointingEnabled: vi.fn(() => false),
      getGeminiClient: mockGetGeminiClient,
      getUsageStatisticsEnabled: () => true,
      getDebugMode: () => false,
      addHistory: vi.fn(),
    } as unknown as Config;
    mockOnDebugMessage = vi.fn();
    mockHandleSlashCommand = vi.fn().mockResolvedValue(false);

    // Mock return value for useReactToolScheduler
    mockScheduleToolCalls = vi.fn();
    mockCancelAllToolCalls = vi.fn();
    mockMarkToolsAsSubmitted = vi.fn();

    // Default mock for useReactToolScheduler to prevent toolCalls being undefined initially
    mockUseReactToolScheduler.mockReturnValue([
      [], // Default to empty array for toolCalls
      mockScheduleToolCalls,
      mockCancelAllToolCalls,
      mockMarkToolsAsSubmitted,
    ]);

    // Reset mocks for GeminiClient instance methods (startChat and sendMessageStream)
    // 
<END_OF_CONTEXT>

<query>
在让gemini进行代码编辑的业务逻辑在哪里
</query>

Assistant: 
2025-07-10 03:26:41,542 - INFO - api.websocket_wiki - websocket_wiki.py:506 - Using Openai protocol with model: qwen3-32b
2025-07-10 03:26:41,576 - INFO - api.websocket_wiki - websocket_wiki.py:592 - Making Openai API call
2025-07-10 03:26:43,219 - INFO - httpx - _client.py:1740 - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:26:52,026 - INFO - api.websocket_wiki - websocket_wiki.py:606 - 602- Openai API response:
在 `packages/core/src/utils/editCorrector.ts` 和 `packages/core/src/utils/editCorrector.test.ts` 中可以找到与 Gemini 进行代码编辑相关的业务逻辑。具体包括以下内容：

- **函数 `ensureCorrectEdit`**  
  该函数负责确保生成的编辑参数（如 `old_string` 和 `new_string`）是正确且与文件当前内容匹配的。它会调用 Gemini 客户端来校正可能的转义问题，例如多余的反斜杠或引号。

- **函数 `unescapeStringForGeminiBug`**  
  用于处理 Gemini 生成的字符串中可能存在的错误转义。例如，将 `\"` 转换为 `"`, 将 `\\n` 转换为 `\n`。这部分逻辑通过多个测试用例验证了各种转义情况的处理。

- **测试逻辑**  
  在 `editCorrector.test.ts` 中，有多个测试用例覆盖了不同场景下的代码编辑逻辑，例如：
  - `old_string` 与当前内容匹配，但 `new_string` 需要修正转义。
  - `old_string` 被过度转义，但解码后能匹配当前内容。
  - 检查 `unescapeStringForGeminiBug` 是否能正确处理混合的转义字符。

这些逻辑共同构成了 Gemini CLI 在执行代码编辑时的核心业务逻辑，确保生成的编辑操作在语法和语义上都是正确的。
